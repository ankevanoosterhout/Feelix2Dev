{"ast":null,"code":"/**\n * @license\n * Copyright 2017 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport * as util from './util';\n/**\n * Computes a list of TapeNodes that connect x to y, filtering everything else\n * out and preserving the order of the original tape elements.\n *\n * @param tape The tape elements to filter.\n * @param xs The input Tensors.\n * @param y The output Tensor.\n */\n\nexport function getFilteredNodesXToY(tape, xs, y) {\n  // Forward pass to compute all the nodes and Tensors that are transitively a\n  // function of x.\n  const tensorsFromX = {};\n  const nodesFromX = {};\n\n  for (let i = 0; i < xs.length; i++) {\n    tensorsFromX[xs[i].id] = true;\n  }\n\n  for (let i = 0; i < tape.length; i++) {\n    const node = tape[i];\n    const nodeInputs = node.inputs;\n\n    for (const inputName in nodeInputs) {\n      const input = nodeInputs[inputName];\n      let anyInputFromX = false;\n\n      for (let j = 0; j < xs.length; j++) {\n        if (tensorsFromX[input.id]) {\n          node.outputs.forEach(output => tensorsFromX[output.id] = true);\n          anyInputFromX = true;\n          nodesFromX[node.id] = true;\n          break;\n        }\n      }\n\n      if (anyInputFromX) {\n        break;\n      }\n    }\n  } // Backward pass to find all of the nodes and Tensors that lead to y.\n\n\n  const tensorsLeadToY = {};\n  tensorsLeadToY[y.id] = true;\n  const nodesToY = {};\n\n  for (let i = tape.length - 1; i >= 0; i--) {\n    const node = tape[i];\n    const nodeInputs = node.inputs; // If any of the outputs lead to y, mark all of the inputs as leading to y.\n\n    for (let j = 0; j < node.outputs.length; j++) {\n      if (tensorsLeadToY[node.outputs[j].id]) {\n        for (const inputName in nodeInputs) {\n          tensorsLeadToY[nodeInputs[inputName].id] = true;\n          nodesToY[node.id] = true;\n        }\n\n        break;\n      }\n    }\n  } // Return the paths that come from x and lead to y.\n\n\n  const filteredTape = [];\n\n  for (let i = 0; i < tape.length; i++) {\n    const node = tape[i];\n\n    if (nodesFromX[node.id] && nodesToY[node.id]) {\n      // Prune the inputs from the node that aren't a function of x.\n      const prunedInputs = {};\n\n      for (const inputName in node.inputs) {\n        const nodeInput = node.inputs[inputName];\n\n        if (tensorsFromX[nodeInput.id]) {\n          prunedInputs[inputName] = nodeInput;\n        }\n      } // Copy the node and overwrite inputsAndArgs to the pruned version.\n\n\n      const prunedNode = Object.assign({}, node);\n      prunedNode.inputs = prunedInputs;\n      prunedNode.outputs = node.outputs;\n      filteredTape.push(prunedNode);\n    }\n  }\n\n  return filteredTape;\n}\n/**\n * Backpropagate gradients through the filtered TapeNodes.\n *\n * @param tensorAccumulatedGradientMap A map of Tensor to its gradient. This map\n * is mutated by this method.\n * @param filteredTape The filtered TapeNodes to backprop through.\n */\n\nexport function backpropagateGradients(tensorAccumulatedGradientMap, filteredTape, tidy, add) {\n  // Walk the tape backward and keep a map of Tensor to its gradient.\n  for (let i = filteredTape.length - 1; i >= 0; i--) {\n    const node = filteredTape[i];\n    const dys = [];\n    node.outputs.forEach(o => {\n      const gradTensor = tensorAccumulatedGradientMap[o.id];\n\n      if (gradTensor != null) {\n        dys.push(gradTensor);\n      } else {\n        // This particular output is not in the back-propagation subgraph, so it\n        // does not affect the final output, thus we put null for its dy.\n        dys.push(null);\n      }\n    });\n\n    if (node.gradient == null) {\n      throw new Error(`Cannot compute gradient: gradient function not found ` + `for ${node.kernelName}.`);\n    } // Backprop dy through this node and accumulate gradients over the inputs.\n\n\n    const inputGradients = node.gradient(dys);\n\n    for (const inputName in node.inputs) {\n      if (!(inputName in inputGradients)) {\n        throw new Error(`Cannot backprop through input ${inputName}. ` + `Available gradients found: ${Object.keys(inputGradients)}.`);\n      } // Call the gradient function.\n\n\n      const dx = tidy(() => inputGradients[inputName]());\n\n      if (dx.dtype !== 'float32') {\n        throw new Error(`Error in gradient for op ${node.kernelName}. The gradient of input ` + `${inputName} must have 'float32' dtype, but has '${dx.dtype}'`);\n      }\n\n      const x = node.inputs[inputName];\n\n      if (!util.arraysEqual(dx.shape, x.shape)) {\n        throw new Error(`Error in gradient for op ${node.kernelName}. The gradient of input ` + `'${inputName}' has shape '${dx.shape}', which does not match ` + `the shape of the input '${x.shape}'`);\n      }\n\n      if (tensorAccumulatedGradientMap[x.id] == null) {\n        tensorAccumulatedGradientMap[x.id] = dx;\n      } else {\n        const curGradient = tensorAccumulatedGradientMap[x.id];\n        tensorAccumulatedGradientMap[x.id] = add(curGradient, dx);\n        curGradient.dispose();\n      }\n    }\n  }\n}","map":{"version":3,"names":["util","getFilteredNodesXToY","tape","xs","y","tensorsFromX","nodesFromX","i","length","id","node","nodeInputs","inputs","inputName","input","anyInputFromX","j","outputs","forEach","output","tensorsLeadToY","nodesToY","filteredTape","prunedInputs","nodeInput","prunedNode","Object","assign","push","backpropagateGradients","tensorAccumulatedGradientMap","tidy","add","dys","o","gradTensor","gradient","Error","kernelName","inputGradients","keys","dx","dtype","x","arraysEqual","shape","curGradient","dispose"],"sources":["C:/Users/Anke/Documents/Feelix documents/Feelix2.0-dev/Feelix v2/node_modules/@tensorflow/tfjs-core/dist/tape.js"],"sourcesContent":["/**\n * @license\n * Copyright 2017 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport * as util from './util';\n/**\n * Computes a list of TapeNodes that connect x to y, filtering everything else\n * out and preserving the order of the original tape elements.\n *\n * @param tape The tape elements to filter.\n * @param xs The input Tensors.\n * @param y The output Tensor.\n */\nexport function getFilteredNodesXToY(tape, xs, y) {\n    // Forward pass to compute all the nodes and Tensors that are transitively a\n    // function of x.\n    const tensorsFromX = {};\n    const nodesFromX = {};\n    for (let i = 0; i < xs.length; i++) {\n        tensorsFromX[xs[i].id] = true;\n    }\n    for (let i = 0; i < tape.length; i++) {\n        const node = tape[i];\n        const nodeInputs = node.inputs;\n        for (const inputName in nodeInputs) {\n            const input = nodeInputs[inputName];\n            let anyInputFromX = false;\n            for (let j = 0; j < xs.length; j++) {\n                if (tensorsFromX[input.id]) {\n                    node.outputs.forEach(output => tensorsFromX[output.id] = true);\n                    anyInputFromX = true;\n                    nodesFromX[node.id] = true;\n                    break;\n                }\n            }\n            if (anyInputFromX) {\n                break;\n            }\n        }\n    }\n    // Backward pass to find all of the nodes and Tensors that lead to y.\n    const tensorsLeadToY = {};\n    tensorsLeadToY[y.id] = true;\n    const nodesToY = {};\n    for (let i = tape.length - 1; i >= 0; i--) {\n        const node = tape[i];\n        const nodeInputs = node.inputs;\n        // If any of the outputs lead to y, mark all of the inputs as leading to y.\n        for (let j = 0; j < node.outputs.length; j++) {\n            if (tensorsLeadToY[node.outputs[j].id]) {\n                for (const inputName in nodeInputs) {\n                    tensorsLeadToY[nodeInputs[inputName].id] = true;\n                    nodesToY[node.id] = true;\n                }\n                break;\n            }\n        }\n    }\n    // Return the paths that come from x and lead to y.\n    const filteredTape = [];\n    for (let i = 0; i < tape.length; i++) {\n        const node = tape[i];\n        if (nodesFromX[node.id] && nodesToY[node.id]) {\n            // Prune the inputs from the node that aren't a function of x.\n            const prunedInputs = {};\n            for (const inputName in node.inputs) {\n                const nodeInput = node.inputs[inputName];\n                if (tensorsFromX[nodeInput.id]) {\n                    prunedInputs[inputName] = nodeInput;\n                }\n            }\n            // Copy the node and overwrite inputsAndArgs to the pruned version.\n            const prunedNode = Object.assign({}, node);\n            prunedNode.inputs = prunedInputs;\n            prunedNode.outputs = node.outputs;\n            filteredTape.push(prunedNode);\n        }\n    }\n    return filteredTape;\n}\n/**\n * Backpropagate gradients through the filtered TapeNodes.\n *\n * @param tensorAccumulatedGradientMap A map of Tensor to its gradient. This map\n * is mutated by this method.\n * @param filteredTape The filtered TapeNodes to backprop through.\n */\nexport function backpropagateGradients(tensorAccumulatedGradientMap, filteredTape, tidy, add) {\n    // Walk the tape backward and keep a map of Tensor to its gradient.\n    for (let i = filteredTape.length - 1; i >= 0; i--) {\n        const node = filteredTape[i];\n        const dys = [];\n        node.outputs.forEach(o => {\n            const gradTensor = tensorAccumulatedGradientMap[o.id];\n            if (gradTensor != null) {\n                dys.push(gradTensor);\n            }\n            else {\n                // This particular output is not in the back-propagation subgraph, so it\n                // does not affect the final output, thus we put null for its dy.\n                dys.push(null);\n            }\n        });\n        if (node.gradient == null) {\n            throw new Error(`Cannot compute gradient: gradient function not found ` +\n                `for ${node.kernelName}.`);\n        }\n        // Backprop dy through this node and accumulate gradients over the inputs.\n        const inputGradients = node.gradient(dys);\n        for (const inputName in node.inputs) {\n            if (!(inputName in inputGradients)) {\n                throw new Error(`Cannot backprop through input ${inputName}. ` +\n                    `Available gradients found: ${Object.keys(inputGradients)}.`);\n            }\n            // Call the gradient function.\n            const dx = tidy(() => inputGradients[inputName]());\n            if (dx.dtype !== 'float32') {\n                throw new Error(`Error in gradient for op ${node.kernelName}. The gradient of input ` +\n                    `${inputName} must have 'float32' dtype, but has '${dx.dtype}'`);\n            }\n            const x = node.inputs[inputName];\n            if (!util.arraysEqual(dx.shape, x.shape)) {\n                throw new Error(`Error in gradient for op ${node.kernelName}. The gradient of input ` +\n                    `'${inputName}' has shape '${dx.shape}', which does not match ` +\n                    `the shape of the input '${x.shape}'`);\n            }\n            if (tensorAccumulatedGradientMap[x.id] == null) {\n                tensorAccumulatedGradientMap[x.id] = dx;\n            }\n            else {\n                const curGradient = tensorAccumulatedGradientMap[x.id];\n                tensorAccumulatedGradientMap[x.id] = add(curGradient, dx);\n                curGradient.dispose();\n            }\n        }\n    }\n}\n"],"mappings":"AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,KAAKA,IAAZ,MAAsB,QAAtB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AACA,OAAO,SAASC,oBAAT,CAA8BC,IAA9B,EAAoCC,EAApC,EAAwCC,CAAxC,EAA2C;EAC9C;EACA;EACA,MAAMC,YAAY,GAAG,EAArB;EACA,MAAMC,UAAU,GAAG,EAAnB;;EACA,KAAK,IAAIC,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGJ,EAAE,CAACK,MAAvB,EAA+BD,CAAC,EAAhC,EAAoC;IAChCF,YAAY,CAACF,EAAE,CAACI,CAAD,CAAF,CAAME,EAAP,CAAZ,GAAyB,IAAzB;EACH;;EACD,KAAK,IAAIF,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGL,IAAI,CAACM,MAAzB,EAAiCD,CAAC,EAAlC,EAAsC;IAClC,MAAMG,IAAI,GAAGR,IAAI,CAACK,CAAD,CAAjB;IACA,MAAMI,UAAU,GAAGD,IAAI,CAACE,MAAxB;;IACA,KAAK,MAAMC,SAAX,IAAwBF,UAAxB,EAAoC;MAChC,MAAMG,KAAK,GAAGH,UAAU,CAACE,SAAD,CAAxB;MACA,IAAIE,aAAa,GAAG,KAApB;;MACA,KAAK,IAAIC,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGb,EAAE,CAACK,MAAvB,EAA+BQ,CAAC,EAAhC,EAAoC;QAChC,IAAIX,YAAY,CAACS,KAAK,CAACL,EAAP,CAAhB,EAA4B;UACxBC,IAAI,CAACO,OAAL,CAAaC,OAAb,CAAqBC,MAAM,IAAId,YAAY,CAACc,MAAM,CAACV,EAAR,CAAZ,GAA0B,IAAzD;UACAM,aAAa,GAAG,IAAhB;UACAT,UAAU,CAACI,IAAI,CAACD,EAAN,CAAV,GAAsB,IAAtB;UACA;QACH;MACJ;;MACD,IAAIM,aAAJ,EAAmB;QACf;MACH;IACJ;EACJ,CA1B6C,CA2B9C;;;EACA,MAAMK,cAAc,GAAG,EAAvB;EACAA,cAAc,CAAChB,CAAC,CAACK,EAAH,CAAd,GAAuB,IAAvB;EACA,MAAMY,QAAQ,GAAG,EAAjB;;EACA,KAAK,IAAId,CAAC,GAAGL,IAAI,CAACM,MAAL,GAAc,CAA3B,EAA8BD,CAAC,IAAI,CAAnC,EAAsCA,CAAC,EAAvC,EAA2C;IACvC,MAAMG,IAAI,GAAGR,IAAI,CAACK,CAAD,CAAjB;IACA,MAAMI,UAAU,GAAGD,IAAI,CAACE,MAAxB,CAFuC,CAGvC;;IACA,KAAK,IAAII,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGN,IAAI,CAACO,OAAL,CAAaT,MAAjC,EAAyCQ,CAAC,EAA1C,EAA8C;MAC1C,IAAII,cAAc,CAACV,IAAI,CAACO,OAAL,CAAaD,CAAb,EAAgBP,EAAjB,CAAlB,EAAwC;QACpC,KAAK,MAAMI,SAAX,IAAwBF,UAAxB,EAAoC;UAChCS,cAAc,CAACT,UAAU,CAACE,SAAD,CAAV,CAAsBJ,EAAvB,CAAd,GAA2C,IAA3C;UACAY,QAAQ,CAACX,IAAI,CAACD,EAAN,CAAR,GAAoB,IAApB;QACH;;QACD;MACH;IACJ;EACJ,CA5C6C,CA6C9C;;;EACA,MAAMa,YAAY,GAAG,EAArB;;EACA,KAAK,IAAIf,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGL,IAAI,CAACM,MAAzB,EAAiCD,CAAC,EAAlC,EAAsC;IAClC,MAAMG,IAAI,GAAGR,IAAI,CAACK,CAAD,CAAjB;;IACA,IAAID,UAAU,CAACI,IAAI,CAACD,EAAN,CAAV,IAAuBY,QAAQ,CAACX,IAAI,CAACD,EAAN,CAAnC,EAA8C;MAC1C;MACA,MAAMc,YAAY,GAAG,EAArB;;MACA,KAAK,MAAMV,SAAX,IAAwBH,IAAI,CAACE,MAA7B,EAAqC;QACjC,MAAMY,SAAS,GAAGd,IAAI,CAACE,MAAL,CAAYC,SAAZ,CAAlB;;QACA,IAAIR,YAAY,CAACmB,SAAS,CAACf,EAAX,CAAhB,EAAgC;UAC5Bc,YAAY,CAACV,SAAD,CAAZ,GAA0BW,SAA1B;QACH;MACJ,CARyC,CAS1C;;;MACA,MAAMC,UAAU,GAAGC,MAAM,CAACC,MAAP,CAAc,EAAd,EAAkBjB,IAAlB,CAAnB;MACAe,UAAU,CAACb,MAAX,GAAoBW,YAApB;MACAE,UAAU,CAACR,OAAX,GAAqBP,IAAI,CAACO,OAA1B;MACAK,YAAY,CAACM,IAAb,CAAkBH,UAAlB;IACH;EACJ;;EACD,OAAOH,YAAP;AACH;AACD;AACA;AACA;AACA;AACA;AACA;AACA;;AACA,OAAO,SAASO,sBAAT,CAAgCC,4BAAhC,EAA8DR,YAA9D,EAA4ES,IAA5E,EAAkFC,GAAlF,EAAuF;EAC1F;EACA,KAAK,IAAIzB,CAAC,GAAGe,YAAY,CAACd,MAAb,GAAsB,CAAnC,EAAsCD,CAAC,IAAI,CAA3C,EAA8CA,CAAC,EAA/C,EAAmD;IAC/C,MAAMG,IAAI,GAAGY,YAAY,CAACf,CAAD,CAAzB;IACA,MAAM0B,GAAG,GAAG,EAAZ;IACAvB,IAAI,CAACO,OAAL,CAAaC,OAAb,CAAqBgB,CAAC,IAAI;MACtB,MAAMC,UAAU,GAAGL,4BAA4B,CAACI,CAAC,CAACzB,EAAH,CAA/C;;MACA,IAAI0B,UAAU,IAAI,IAAlB,EAAwB;QACpBF,GAAG,CAACL,IAAJ,CAASO,UAAT;MACH,CAFD,MAGK;QACD;QACA;QACAF,GAAG,CAACL,IAAJ,CAAS,IAAT;MACH;IACJ,CAVD;;IAWA,IAAIlB,IAAI,CAAC0B,QAAL,IAAiB,IAArB,EAA2B;MACvB,MAAM,IAAIC,KAAJ,CAAW,uDAAD,GACX,OAAM3B,IAAI,CAAC4B,UAAW,GADrB,CAAN;IAEH,CAjB8C,CAkB/C;;;IACA,MAAMC,cAAc,GAAG7B,IAAI,CAAC0B,QAAL,CAAcH,GAAd,CAAvB;;IACA,KAAK,MAAMpB,SAAX,IAAwBH,IAAI,CAACE,MAA7B,EAAqC;MACjC,IAAI,EAAEC,SAAS,IAAI0B,cAAf,CAAJ,EAAoC;QAChC,MAAM,IAAIF,KAAJ,CAAW,iCAAgCxB,SAAU,IAA3C,GACX,8BAA6Ba,MAAM,CAACc,IAAP,CAAYD,cAAZ,CAA4B,GADxD,CAAN;MAEH,CAJgC,CAKjC;;;MACA,MAAME,EAAE,GAAGV,IAAI,CAAC,MAAMQ,cAAc,CAAC1B,SAAD,CAAd,EAAP,CAAf;;MACA,IAAI4B,EAAE,CAACC,KAAH,KAAa,SAAjB,EAA4B;QACxB,MAAM,IAAIL,KAAJ,CAAW,4BAA2B3B,IAAI,CAAC4B,UAAW,0BAA5C,GACX,GAAEzB,SAAU,wCAAuC4B,EAAE,CAACC,KAAM,GAD3D,CAAN;MAEH;;MACD,MAAMC,CAAC,GAAGjC,IAAI,CAACE,MAAL,CAAYC,SAAZ,CAAV;;MACA,IAAI,CAACb,IAAI,CAAC4C,WAAL,CAAiBH,EAAE,CAACI,KAApB,EAA2BF,CAAC,CAACE,KAA7B,CAAL,EAA0C;QACtC,MAAM,IAAIR,KAAJ,CAAW,4BAA2B3B,IAAI,CAAC4B,UAAW,0BAA5C,GACX,IAAGzB,SAAU,gBAAe4B,EAAE,CAACI,KAAM,0BAD1B,GAEX,2BAA0BF,CAAC,CAACE,KAAM,GAFjC,CAAN;MAGH;;MACD,IAAIf,4BAA4B,CAACa,CAAC,CAAClC,EAAH,CAA5B,IAAsC,IAA1C,EAAgD;QAC5CqB,4BAA4B,CAACa,CAAC,CAAClC,EAAH,CAA5B,GAAqCgC,EAArC;MACH,CAFD,MAGK;QACD,MAAMK,WAAW,GAAGhB,4BAA4B,CAACa,CAAC,CAAClC,EAAH,CAAhD;QACAqB,4BAA4B,CAACa,CAAC,CAAClC,EAAH,CAA5B,GAAqCuB,GAAG,CAACc,WAAD,EAAcL,EAAd,CAAxC;QACAK,WAAW,CAACC,OAAZ;MACH;IACJ;EACJ;AACJ"},"metadata":{},"sourceType":"module"}