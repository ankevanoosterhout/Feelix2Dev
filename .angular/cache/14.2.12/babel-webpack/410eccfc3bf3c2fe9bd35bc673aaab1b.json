{"ast":null,"code":"/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { customGrad } from '../../gradients';\nimport { convertToTensor } from '../../tensor_util_env';\nimport { assertShapesMatch } from '../../util';\nimport { add } from '../add';\nimport { expandShapeToKeepDim } from '../axis_util';\nimport { cast } from '../cast';\nimport { div } from '../div';\nimport { exp } from '../exp';\nimport { logSumExp } from '../log_sum_exp';\nimport { Reduction } from '../loss_ops_utils';\nimport { mul } from '../mul';\nimport { neg } from '../neg';\nimport { op } from '../operation';\nimport { reshape } from '../reshape';\nimport { scalar } from '../scalar';\nimport { sub } from '../sub';\nimport { sum } from '../sum';\nimport { computeWeightedLoss } from './compute_weighted_loss';\n/**\n * Computes softmax cross entropy between logits and labels.\n *\n * Measures the probability error in discrete classification tasks in which\n * the classes are mutually exclusive (each entry is in exactly one class).\n * For example, each CIFAR-10 image is labeled with one and only one label: an\n * image can be a dog or a truck, but not both.\n *\n * `NOTE`: While the classes are mutually exclusive, their probabilities need\n * not be. All that is required is that each row of labels is a valid\n * probability distribution. If they are not, the computation of the gradient\n * will be incorrect.\n *\n * `WARNING`: This op expects unscaled logits, since it performs a softmax on\n * logits internally for efficiency. Do not call this op with the output of\n * softmax, as it will produce incorrect results.\n *\n * logits and labels must have the same shape, e.g. [batch_size, num_classes]\n * and the same dtype.\n * @param labels The labels array.\n * @param logits The logits array.\n * @param dim The dimension softmax would be performed on. Defaults to `-1`\n *     which indicates the last dimension.\n */\n\nfunction softmaxCrossEntropyWithLogits_(labels, logits, dim = -1) {\n  if (dim === -1) {\n    dim = logits.rank - 1;\n  }\n\n  if (dim !== logits.rank - 1) {\n    throw Error(`Softmax cross entropy along a non-last dimension is not yet ` + `supported. Labels / logits was rank ${logits.rank} ` + `and dim was ${dim}`);\n  } // Use a custom gradient for numerical stability.\n\n\n  const customOp = customGrad((labels, logits, save) => {\n    // Reference:\n    //   1. http://cs231n.github.io/linear-classify/#softmax\n    //   2. https://blog.feedly.com/tricks-of-the-trade-logsumexp/\n    const keepDims = true;\n    const lse = logSumExp(logits, [dim], keepDims);\n    const logResult = sub(cast(logits, 'float32'), lse);\n    save([labels, logResult]);\n    const costVector = neg(mul(logResult, labels));\n    const value = sum(costVector, [dim]);\n\n    const gradFunc = (dy, saved) => {\n      const [labels, logResult] = saved;\n      const dyShape = expandShapeToKeepDim(dy.shape, [dim]);\n      return [mul(reshape(dy, dyShape), sub(cast(labels, 'float32'), exp(logResult))), mul(reshape(dy, dyShape), sub(exp(logResult), cast(labels, 'float32')))];\n    };\n\n    return {\n      value,\n      gradFunc\n    };\n  });\n  return customOp(labels, logits);\n}\n/**\n * Computes the softmax cross entropy loss between two tensors.\n *\n * If labelSmoothing is nonzero, smooth the labels towards 1/2:\n *\n *   newOnehotLabels = onehotLabels * (1 - labelSmoothing)\n *                         + labelSmoothing / numClasses\n *\n * @param onehotLabels One hot encoded labels\n *    [batch_size, num_classes], same dimensions as 'predictions'.\n * @param logits The predicted outputs.\n * @param weights Tensor whose rank is either 0, or 1, and must be\n *    broadcastable to `loss`  of shape [batch_size]\n * @param labelSmoothing If greater than 0, then smooth the labels.\n * @param reduction Type of reduction to apply to loss. Should be of type\n *    `Reduction`\n *\n * @doc { heading: 'Training', subheading: 'Losses', namespace: 'losses' }\n */\n\n\nfunction softmaxCrossEntropy_(onehotLabels, logits, weights, labelSmoothing = 0, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\n  let $onehotLabels = convertToTensor(onehotLabels, 'onehotLabels', 'softmaxCrossEntropy');\n  const $logits = convertToTensor(logits, 'logits', 'softmaxCrossEntropy');\n  let $weights = null;\n\n  if (weights != null) {\n    $weights = convertToTensor(weights, 'weights', 'softmaxCrossEntropy');\n  }\n\n  assertShapesMatch($onehotLabels.shape, $logits.shape, 'Error in softmaxCrossEntropy: ');\n\n  if (labelSmoothing > 0) {\n    const labelSmoothingScalar = scalar(labelSmoothing);\n    const one = scalar(1);\n    const numClasses = scalar($onehotLabels.shape[1]);\n    $onehotLabels = add(mul($onehotLabels, sub(one, labelSmoothingScalar)), div(labelSmoothingScalar, numClasses));\n  }\n\n  const losses = softmaxCrossEntropyWithLogits_($onehotLabels, $logits);\n  return computeWeightedLoss(losses, $weights, reduction);\n}\n\nexport const softmaxCrossEntropy = op({\n  softmaxCrossEntropy_\n});","map":{"version":3,"names":["customGrad","convertToTensor","assertShapesMatch","add","expandShapeToKeepDim","cast","div","exp","logSumExp","Reduction","mul","neg","op","reshape","scalar","sub","sum","computeWeightedLoss","softmaxCrossEntropyWithLogits_","labels","logits","dim","rank","Error","customOp","save","keepDims","lse","logResult","costVector","value","gradFunc","dy","saved","dyShape","shape","softmaxCrossEntropy_","onehotLabels","weights","labelSmoothing","reduction","SUM_BY_NONZERO_WEIGHTS","$onehotLabels","$logits","$weights","labelSmoothingScalar","one","numClasses","losses","softmaxCrossEntropy"],"sources":["C:/Users/Anke/Documents/Feelix documents/Feelix2.0-dev/Feelix v2/node_modules/@tensorflow/tfjs-core/dist/ops/losses/softmax_cross_entropy.js"],"sourcesContent":["/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { customGrad } from '../../gradients';\nimport { convertToTensor } from '../../tensor_util_env';\nimport { assertShapesMatch } from '../../util';\nimport { add } from '../add';\nimport { expandShapeToKeepDim } from '../axis_util';\nimport { cast } from '../cast';\nimport { div } from '../div';\nimport { exp } from '../exp';\nimport { logSumExp } from '../log_sum_exp';\nimport { Reduction } from '../loss_ops_utils';\nimport { mul } from '../mul';\nimport { neg } from '../neg';\nimport { op } from '../operation';\nimport { reshape } from '../reshape';\nimport { scalar } from '../scalar';\nimport { sub } from '../sub';\nimport { sum } from '../sum';\nimport { computeWeightedLoss } from './compute_weighted_loss';\n/**\n * Computes softmax cross entropy between logits and labels.\n *\n * Measures the probability error in discrete classification tasks in which\n * the classes are mutually exclusive (each entry is in exactly one class).\n * For example, each CIFAR-10 image is labeled with one and only one label: an\n * image can be a dog or a truck, but not both.\n *\n * `NOTE`: While the classes are mutually exclusive, their probabilities need\n * not be. All that is required is that each row of labels is a valid\n * probability distribution. If they are not, the computation of the gradient\n * will be incorrect.\n *\n * `WARNING`: This op expects unscaled logits, since it performs a softmax on\n * logits internally for efficiency. Do not call this op with the output of\n * softmax, as it will produce incorrect results.\n *\n * logits and labels must have the same shape, e.g. [batch_size, num_classes]\n * and the same dtype.\n * @param labels The labels array.\n * @param logits The logits array.\n * @param dim The dimension softmax would be performed on. Defaults to `-1`\n *     which indicates the last dimension.\n */\nfunction softmaxCrossEntropyWithLogits_(labels, logits, dim = -1) {\n    if (dim === -1) {\n        dim = logits.rank - 1;\n    }\n    if (dim !== logits.rank - 1) {\n        throw Error(`Softmax cross entropy along a non-last dimension is not yet ` +\n            `supported. Labels / logits was rank ${logits.rank} ` +\n            `and dim was ${dim}`);\n    }\n    // Use a custom gradient for numerical stability.\n    const customOp = customGrad((labels, logits, save) => {\n        // Reference:\n        //   1. http://cs231n.github.io/linear-classify/#softmax\n        //   2. https://blog.feedly.com/tricks-of-the-trade-logsumexp/\n        const keepDims = true;\n        const lse = logSumExp(logits, [dim], keepDims);\n        const logResult = sub(cast(logits, 'float32'), lse);\n        save([labels, logResult]);\n        const costVector = neg(mul(logResult, labels));\n        const value = sum(costVector, [dim]);\n        const gradFunc = (dy, saved) => {\n            const [labels, logResult] = saved;\n            const dyShape = expandShapeToKeepDim(dy.shape, [dim]);\n            return [\n                mul(reshape(dy, dyShape), sub(cast(labels, 'float32'), exp(logResult))),\n                mul(reshape(dy, dyShape), sub(exp(logResult), cast(labels, 'float32'))),\n            ];\n        };\n        return { value, gradFunc };\n    });\n    return customOp(labels, logits);\n}\n/**\n * Computes the softmax cross entropy loss between two tensors.\n *\n * If labelSmoothing is nonzero, smooth the labels towards 1/2:\n *\n *   newOnehotLabels = onehotLabels * (1 - labelSmoothing)\n *                         + labelSmoothing / numClasses\n *\n * @param onehotLabels One hot encoded labels\n *    [batch_size, num_classes], same dimensions as 'predictions'.\n * @param logits The predicted outputs.\n * @param weights Tensor whose rank is either 0, or 1, and must be\n *    broadcastable to `loss`  of shape [batch_size]\n * @param labelSmoothing If greater than 0, then smooth the labels.\n * @param reduction Type of reduction to apply to loss. Should be of type\n *    `Reduction`\n *\n * @doc { heading: 'Training', subheading: 'Losses', namespace: 'losses' }\n */\nfunction softmaxCrossEntropy_(onehotLabels, logits, weights, labelSmoothing = 0, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\n    let $onehotLabels = convertToTensor(onehotLabels, 'onehotLabels', 'softmaxCrossEntropy');\n    const $logits = convertToTensor(logits, 'logits', 'softmaxCrossEntropy');\n    let $weights = null;\n    if (weights != null) {\n        $weights = convertToTensor(weights, 'weights', 'softmaxCrossEntropy');\n    }\n    assertShapesMatch($onehotLabels.shape, $logits.shape, 'Error in softmaxCrossEntropy: ');\n    if (labelSmoothing > 0) {\n        const labelSmoothingScalar = scalar(labelSmoothing);\n        const one = scalar(1);\n        const numClasses = scalar($onehotLabels.shape[1]);\n        $onehotLabels =\n            add(mul($onehotLabels, sub(one, labelSmoothingScalar)), div(labelSmoothingScalar, numClasses));\n    }\n    const losses = softmaxCrossEntropyWithLogits_($onehotLabels, $logits);\n    return computeWeightedLoss(losses, $weights, reduction);\n}\nexport const softmaxCrossEntropy = op({ softmaxCrossEntropy_ });\n"],"mappings":"AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAASA,UAAT,QAA2B,iBAA3B;AACA,SAASC,eAAT,QAAgC,uBAAhC;AACA,SAASC,iBAAT,QAAkC,YAAlC;AACA,SAASC,GAAT,QAAoB,QAApB;AACA,SAASC,oBAAT,QAAqC,cAArC;AACA,SAASC,IAAT,QAAqB,SAArB;AACA,SAASC,GAAT,QAAoB,QAApB;AACA,SAASC,GAAT,QAAoB,QAApB;AACA,SAASC,SAAT,QAA0B,gBAA1B;AACA,SAASC,SAAT,QAA0B,mBAA1B;AACA,SAASC,GAAT,QAAoB,QAApB;AACA,SAASC,GAAT,QAAoB,QAApB;AACA,SAASC,EAAT,QAAmB,cAAnB;AACA,SAASC,OAAT,QAAwB,YAAxB;AACA,SAASC,MAAT,QAAuB,WAAvB;AACA,SAASC,GAAT,QAAoB,QAApB;AACA,SAASC,GAAT,QAAoB,QAApB;AACA,SAASC,mBAAT,QAAoC,yBAApC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AACA,SAASC,8BAAT,CAAwCC,MAAxC,EAAgDC,MAAhD,EAAwDC,GAAG,GAAG,CAAC,CAA/D,EAAkE;EAC9D,IAAIA,GAAG,KAAK,CAAC,CAAb,EAAgB;IACZA,GAAG,GAAGD,MAAM,CAACE,IAAP,GAAc,CAApB;EACH;;EACD,IAAID,GAAG,KAAKD,MAAM,CAACE,IAAP,GAAc,CAA1B,EAA6B;IACzB,MAAMC,KAAK,CAAE,8DAAD,GACP,uCAAsCH,MAAM,CAACE,IAAK,GAD3C,GAEP,eAAcD,GAAI,EAFZ,CAAX;EAGH,CAR6D,CAS9D;;;EACA,MAAMG,QAAQ,GAAGxB,UAAU,CAAC,CAACmB,MAAD,EAASC,MAAT,EAAiBK,IAAjB,KAA0B;IAClD;IACA;IACA;IACA,MAAMC,QAAQ,GAAG,IAAjB;IACA,MAAMC,GAAG,GAAGnB,SAAS,CAACY,MAAD,EAAS,CAACC,GAAD,CAAT,EAAgBK,QAAhB,CAArB;IACA,MAAME,SAAS,GAAGb,GAAG,CAACV,IAAI,CAACe,MAAD,EAAS,SAAT,CAAL,EAA0BO,GAA1B,CAArB;IACAF,IAAI,CAAC,CAACN,MAAD,EAASS,SAAT,CAAD,CAAJ;IACA,MAAMC,UAAU,GAAGlB,GAAG,CAACD,GAAG,CAACkB,SAAD,EAAYT,MAAZ,CAAJ,CAAtB;IACA,MAAMW,KAAK,GAAGd,GAAG,CAACa,UAAD,EAAa,CAACR,GAAD,CAAb,CAAjB;;IACA,MAAMU,QAAQ,GAAG,CAACC,EAAD,EAAKC,KAAL,KAAe;MAC5B,MAAM,CAACd,MAAD,EAASS,SAAT,IAAsBK,KAA5B;MACA,MAAMC,OAAO,GAAG9B,oBAAoB,CAAC4B,EAAE,CAACG,KAAJ,EAAW,CAACd,GAAD,CAAX,CAApC;MACA,OAAO,CACHX,GAAG,CAACG,OAAO,CAACmB,EAAD,EAAKE,OAAL,CAAR,EAAuBnB,GAAG,CAACV,IAAI,CAACc,MAAD,EAAS,SAAT,CAAL,EAA0BZ,GAAG,CAACqB,SAAD,CAA7B,CAA1B,CADA,EAEHlB,GAAG,CAACG,OAAO,CAACmB,EAAD,EAAKE,OAAL,CAAR,EAAuBnB,GAAG,CAACR,GAAG,CAACqB,SAAD,CAAJ,EAAiBvB,IAAI,CAACc,MAAD,EAAS,SAAT,CAArB,CAA1B,CAFA,CAAP;IAIH,CAPD;;IAQA,OAAO;MAAEW,KAAF;MAASC;IAAT,CAAP;EACH,CAnB0B,CAA3B;EAoBA,OAAOP,QAAQ,CAACL,MAAD,EAASC,MAAT,CAAf;AACH;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;AACA,SAASgB,oBAAT,CAA8BC,YAA9B,EAA4CjB,MAA5C,EAAoDkB,OAApD,EAA6DC,cAAc,GAAG,CAA9E,EAAiFC,SAAS,GAAG/B,SAAS,CAACgC,sBAAvG,EAA+H;EAC3H,IAAIC,aAAa,GAAGzC,eAAe,CAACoC,YAAD,EAAe,cAAf,EAA+B,qBAA/B,CAAnC;EACA,MAAMM,OAAO,GAAG1C,eAAe,CAACmB,MAAD,EAAS,QAAT,EAAmB,qBAAnB,CAA/B;EACA,IAAIwB,QAAQ,GAAG,IAAf;;EACA,IAAIN,OAAO,IAAI,IAAf,EAAqB;IACjBM,QAAQ,GAAG3C,eAAe,CAACqC,OAAD,EAAU,SAAV,EAAqB,qBAArB,CAA1B;EACH;;EACDpC,iBAAiB,CAACwC,aAAa,CAACP,KAAf,EAAsBQ,OAAO,CAACR,KAA9B,EAAqC,gCAArC,CAAjB;;EACA,IAAII,cAAc,GAAG,CAArB,EAAwB;IACpB,MAAMM,oBAAoB,GAAG/B,MAAM,CAACyB,cAAD,CAAnC;IACA,MAAMO,GAAG,GAAGhC,MAAM,CAAC,CAAD,CAAlB;IACA,MAAMiC,UAAU,GAAGjC,MAAM,CAAC4B,aAAa,CAACP,KAAd,CAAoB,CAApB,CAAD,CAAzB;IACAO,aAAa,GACTvC,GAAG,CAACO,GAAG,CAACgC,aAAD,EAAgB3B,GAAG,CAAC+B,GAAD,EAAMD,oBAAN,CAAnB,CAAJ,EAAqDvC,GAAG,CAACuC,oBAAD,EAAuBE,UAAvB,CAAxD,CADP;EAEH;;EACD,MAAMC,MAAM,GAAG9B,8BAA8B,CAACwB,aAAD,EAAgBC,OAAhB,CAA7C;EACA,OAAO1B,mBAAmB,CAAC+B,MAAD,EAASJ,QAAT,EAAmBJ,SAAnB,CAA1B;AACH;;AACD,OAAO,MAAMS,mBAAmB,GAAGrC,EAAE,CAAC;EAAEwB;AAAF,CAAD,CAA9B"},"metadata":{},"sourceType":"module"}