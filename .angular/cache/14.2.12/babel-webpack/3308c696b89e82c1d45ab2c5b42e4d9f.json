{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * TensorFlow.js Layers: Recurrent Neural Network Layers.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { serialization, tidy, util } from '@tensorflow/tfjs-core';\nimport { getActivation, serializeActivation } from '../activations';\nimport * as K from '../backend/tfjs_backend';\nimport { nameScope } from '../common';\nimport { getConstraint, serializeConstraint } from '../constraints';\nimport { InputSpec, SymbolicTensor } from '../engine/topology';\nimport { Layer } from '../engine/topology';\nimport { AttributeError, NotImplementedError, ValueError } from '../errors';\nimport { getInitializer, Initializer, Ones, serializeInitializer } from '../initializers';\nimport { getRegularizer, serializeRegularizer } from '../regularizers';\nimport { assertPositiveInteger } from '../utils/generic_utils';\nimport * as math_utils from '../utils/math_utils';\nimport { getExactlyOneShape, getExactlyOneTensor, isArrayOfShapes } from '../utils/types_utils';\nimport { batchGetValue, batchSetValue } from '../variables';\nimport { deserialize } from './serialization';\n/**\n * Standardize `apply()` args to a single list of tensor inputs.\n *\n * When running a model loaded from file, the input tensors `initialState` and\n * `constants` are passed to `RNN.apply()` as part of `inputs` instead of the\n * dedicated kwargs fields. `inputs` consists of\n * `[inputs, initialState0, initialState1, ..., constant0, constant1]` in this\n * case.\n * This method makes sure that arguments are\n * separated and that `initialState` and `constants` are `Array`s of tensors\n * (or None).\n *\n * @param inputs Tensor or `Array` of  tensors.\n * @param initialState Tensor or `Array` of tensors or `null`/`undefined`.\n * @param constants Tensor or `Array` of tensors or `null`/`undefined`.\n * @returns An object consisting of\n *   inputs: A tensor.\n *   initialState: `Array` of tensors or `null`.\n *   constants: `Array` of tensors or `null`.\n * @throws ValueError, if `inputs` is an `Array` but either `initialState` or\n *   `constants` is provided.\n */\n\nexport function standardizeArgs(inputs, initialState, constants, numConstants) {\n  if (Array.isArray(inputs)) {\n    if (initialState != null || constants != null) {\n      throw new ValueError('When inputs is an array, neither initialState or constants ' + 'should be provided');\n    }\n\n    if (numConstants != null) {\n      constants = inputs.slice(inputs.length - numConstants, inputs.length);\n      inputs = inputs.slice(0, inputs.length - numConstants);\n    }\n\n    if (inputs.length > 1) {\n      initialState = inputs.slice(1, inputs.length);\n    }\n\n    inputs = inputs[0];\n  }\n\n  function toListOrNull(x) {\n    if (x == null || Array.isArray(x)) {\n      return x;\n    } else {\n      return [x];\n    }\n  }\n\n  initialState = toListOrNull(initialState);\n  constants = toListOrNull(constants);\n  return {\n    inputs,\n    initialState,\n    constants\n  };\n}\n/**\n * Iterates over the time dimension of a tensor.\n *\n * @param stepFunction RNN step function.\n *   Parameters:\n *     inputs: tensor with shape `[samples, ...]` (no time dimension),\n *       representing input for the batch of samples at a certain time step.\n *     states: an Array of tensors.\n *   Returns:\n *     outputs: tensor with shape `[samples, outputDim]` (no time dimension).\n *     newStates: list of tensors, same length and shapes as `states`. The first\n *       state in the list must be the output tensor at the previous timestep.\n * @param inputs Tensor of temporal data of shape `[samples, time, ...]` (at\n *   least 3D).\n * @param initialStates Tensor with shape `[samples, outputDim]` (no time\n *   dimension), containing the initial values of the states used in the step\n *   function.\n * @param goBackwards If `true`, do the iteration over the time dimension in\n *   reverse order and return the reversed sequence.\n * @param mask Binary tensor with shape `[sample, time, 1]`, with a zero for\n *   every element that is masked.\n * @param constants An Array of constant values passed at each step.\n * @param unroll Whether to unroll the RNN or to use a symbolic loop. *Not*\n *   applicable to this imperative deeplearn.js backend. Its value is ignored.\n * @param needPerStepOutputs Whether the per-step outputs are to be\n *   concatenated into a single tensor and returned (as the second return\n *   value). Default: `false`. This arg is included so that the relatively\n *   expensive concatenation of the stepwise outputs can be omitted unless\n *   the stepwise outputs need to be kept (e.g., for an LSTM layer of which\n *   `returnSequence` is `true`.)\n * @returns An Array: `[lastOutput, outputs, newStates]`.\n *   lastOutput: the lastest output of the RNN, of shape `[samples, ...]`.\n *   outputs: tensor with shape `[samples, time, ...]` where each entry\n *     `output[s, t]` is the output of the step function at time `t` for sample\n *     `s`. This return value is provided if and only if the\n *     `needPerStepOutputs` is set as `true`. If it is set as `false`, this\n *     return value will be `undefined`.\n *   newStates: Array of tensors, latest states returned by the step function,\n *      of shape `(samples, ...)`.\n * @throws ValueError If input dimension is less than 3.\n *\n * TODO(nielsene): This needs to be tidy-ed.\n */\n\nexport function rnn(stepFunction, inputs, initialStates, goBackwards = false, mask, constants, unroll = false, needPerStepOutputs = false) {\n  return tfc.tidy(() => {\n    const ndim = inputs.shape.length;\n\n    if (ndim < 3) {\n      throw new ValueError(`Input should be at least 3D, but is ${ndim}D.`);\n    } // Transpose to time-major, i.e., from [batch, time, ...] to [time, batch,\n    // ...].\n\n\n    const axes = [1, 0].concat(math_utils.range(2, ndim));\n    inputs = tfc.transpose(inputs, axes);\n\n    if (constants != null) {\n      throw new NotImplementedError('The rnn() functoin of the deeplearn.js backend does not support ' + 'constants yet.');\n    } // Porting Note: the unroll option is ignored by the imperative backend.\n\n\n    if (unroll) {\n      console.warn('Backend rnn(): the unroll = true option is not applicable to the ' + 'imperative deeplearn.js backend.');\n    }\n\n    if (mask != null) {\n      mask = tfc.cast(tfc.cast(mask, 'bool'), 'float32');\n\n      if (mask.rank === ndim - 1) {\n        mask = tfc.expandDims(mask, -1);\n      }\n\n      mask = tfc.transpose(mask, axes);\n    }\n\n    if (goBackwards) {\n      inputs = tfc.reverse(inputs, 0);\n\n      if (mask != null) {\n        mask = tfc.reverse(mask, 0);\n      }\n    } // Porting Note: PyKeras with TensorFlow backend uses a symbolic loop\n    //   (tf.while_loop). But for the imperative deeplearn.js backend, we just\n    //   use the usual TypeScript control flow to iterate over the time steps in\n    //   the inputs.\n    // Porting Note: PyKeras patches a \"_use_learning_phase\" attribute to\n    // outputs.\n    //   This is not idiomatic in TypeScript. The info regarding whether we are\n    //   in a learning (i.e., training) phase for RNN is passed in a different\n    //   way.\n\n\n    const perStepOutputs = [];\n    let lastOutput;\n    let states = initialStates;\n    const timeSteps = inputs.shape[0];\n    const perStepInputs = tfc.unstack(inputs);\n    let perStepMasks;\n\n    if (mask != null) {\n      perStepMasks = tfc.unstack(mask);\n    }\n\n    for (let t = 0; t < timeSteps; ++t) {\n      const currentInput = perStepInputs[t];\n      const stepOutputs = tfc.tidy(() => stepFunction(currentInput, states));\n\n      if (mask == null) {\n        lastOutput = stepOutputs[0];\n        states = stepOutputs[1];\n      } else {\n        const maskedOutputs = tfc.tidy(() => {\n          const stepMask = perStepMasks[t];\n          const negStepMask = tfc.sub(tfc.onesLike(stepMask), stepMask); // TODO(cais): Would tfc.where() be better for performance?\n\n          const output = tfc.add(tfc.mul(stepOutputs[0], stepMask), tfc.mul(states[0], negStepMask));\n          const newStates = states.map((state, i) => {\n            return tfc.add(tfc.mul(stepOutputs[1][i], stepMask), tfc.mul(state, negStepMask));\n          });\n          return {\n            output,\n            newStates\n          };\n        });\n        lastOutput = maskedOutputs.output;\n        states = maskedOutputs.newStates;\n      }\n\n      if (needPerStepOutputs) {\n        perStepOutputs.push(lastOutput);\n      }\n    }\n\n    let outputs;\n\n    if (needPerStepOutputs) {\n      const axis = 1;\n      outputs = tfc.stack(perStepOutputs, axis);\n    }\n\n    return [lastOutput, outputs, states];\n  });\n}\nexport class RNN extends Layer {\n  constructor(args) {\n    super(args);\n    let cell;\n\n    if (args.cell == null) {\n      throw new ValueError('cell property is missing for the constructor of RNN.');\n    } else if (Array.isArray(args.cell)) {\n      cell = new StackedRNNCells({\n        cells: args.cell\n      });\n    } else {\n      cell = args.cell;\n    }\n\n    if (cell.stateSize == null) {\n      throw new ValueError('The RNN cell should have an attribute `stateSize` (tuple of ' + 'integers, one integer per RNN state).');\n    }\n\n    this.cell = cell;\n    this.returnSequences = args.returnSequences == null ? false : args.returnSequences;\n    this.returnState = args.returnState == null ? false : args.returnState;\n    this.goBackwards = args.goBackwards == null ? false : args.goBackwards;\n    this._stateful = args.stateful == null ? false : args.stateful;\n    this.unroll = args.unroll == null ? false : args.unroll;\n    this.supportsMasking = true;\n    this.inputSpec = [new InputSpec({\n      ndim: 3\n    })];\n    this.stateSpec = null;\n    this.states_ = null; // TODO(cais): Add constantsSpec and numConstants.\n\n    this.numConstants = null; // TODO(cais): Look into the use of initial_state in the kwargs of the\n    //   constructor.\n\n    this.keptStates = [];\n  } // Porting Note: This is the equivalent of `RNN.states` property getter in\n  //   PyKeras.\n\n\n  getStates() {\n    if (this.states_ == null) {\n      const numStates = Array.isArray(this.cell.stateSize) ? this.cell.stateSize.length : 1;\n      return math_utils.range(0, numStates).map(x => null);\n    } else {\n      return this.states_;\n    }\n  } // Porting Note: This is the equivalent of the `RNN.states` property setter in\n  //   PyKeras.\n\n\n  setStates(states) {\n    this.states_ = states;\n  }\n\n  computeOutputShape(inputShape) {\n    if (isArrayOfShapes(inputShape)) {\n      inputShape = inputShape[0];\n    }\n\n    inputShape = inputShape; // TODO(cais): Remove the casting once stacked RNN cells become supported.\n\n    let stateSize = this.cell.stateSize;\n\n    if (!Array.isArray(stateSize)) {\n      stateSize = [stateSize];\n    }\n\n    const outputDim = stateSize[0];\n    let outputShape;\n\n    if (this.returnSequences) {\n      outputShape = [inputShape[0], inputShape[1], outputDim];\n    } else {\n      outputShape = [inputShape[0], outputDim];\n    }\n\n    if (this.returnState) {\n      const stateShape = [];\n\n      for (const dim of stateSize) {\n        stateShape.push([inputShape[0], dim]);\n      }\n\n      return [outputShape].concat(stateShape);\n    } else {\n      return outputShape;\n    }\n  }\n\n  computeMask(inputs, mask) {\n    return tfc.tidy(() => {\n      if (Array.isArray(mask)) {\n        mask = mask[0];\n      }\n\n      const outputMask = this.returnSequences ? mask : null;\n\n      if (this.returnState) {\n        const stateMask = this.states.map(s => null);\n        return [outputMask].concat(stateMask);\n      } else {\n        return outputMask;\n      }\n    });\n  }\n  /**\n   * Get the current state tensors of the RNN.\n   *\n   * If the state hasn't been set, return an array of `null`s of the correct\n   * length.\n   */\n\n\n  get states() {\n    if (this.states_ == null) {\n      const numStates = Array.isArray(this.cell.stateSize) ? this.cell.stateSize.length : 1;\n      const output = [];\n\n      for (let i = 0; i < numStates; ++i) {\n        output.push(null);\n      }\n\n      return output;\n    } else {\n      return this.states_;\n    }\n  }\n\n  set states(s) {\n    this.states_ = s;\n  }\n\n  build(inputShape) {\n    // Note inputShape will be an Array of Shapes of initial states and\n    // constants if these are passed in apply().\n    const constantShape = null;\n\n    if (this.numConstants != null) {\n      throw new NotImplementedError('Constants support is not implemented in RNN yet.');\n    }\n\n    if (isArrayOfShapes(inputShape)) {\n      inputShape = inputShape[0];\n    }\n\n    inputShape = inputShape;\n    const batchSize = this.stateful ? inputShape[0] : null;\n    const inputDim = inputShape.slice(2);\n    this.inputSpec[0] = new InputSpec({\n      shape: [batchSize, null, ...inputDim]\n    }); // Allow cell (if RNNCell Layer) to build before we set or validate\n    // stateSpec.\n\n    const stepInputShape = [inputShape[0]].concat(inputShape.slice(2));\n\n    if (constantShape != null) {\n      throw new NotImplementedError('Constants support is not implemented in RNN yet.');\n    } else {\n      this.cell.build(stepInputShape);\n    } // Set or validate stateSpec.\n\n\n    let stateSize;\n\n    if (Array.isArray(this.cell.stateSize)) {\n      stateSize = this.cell.stateSize;\n    } else {\n      stateSize = [this.cell.stateSize];\n    }\n\n    if (this.stateSpec != null) {\n      if (!util.arraysEqual(this.stateSpec.map(spec => spec.shape[spec.shape.length - 1]), stateSize)) {\n        throw new ValueError(`An initialState was passed that is not compatible with ` + `cell.stateSize. Received stateSpec=${this.stateSpec}; ` + `However cell.stateSize is ${this.cell.stateSize}`);\n      }\n    } else {\n      this.stateSpec = stateSize.map(dim => new InputSpec({\n        shape: [null, dim]\n      }));\n    }\n\n    if (this.stateful) {\n      this.resetStates();\n    }\n  }\n  /**\n   * Reset the state tensors of the RNN.\n   *\n   * If the `states` argument is `undefined` or `null`, will set the\n   * state tensor(s) of the RNN to all-zero tensors of the appropriate\n   * shape(s).\n   *\n   * If `states` is provided, will set the state tensors of the RNN to its\n   * value.\n   *\n   * @param states Optional externally-provided initial states.\n   * @param training Whether this call is done during training. For stateful\n   *   RNNs, this affects whether the old states are kept or discarded. In\n   *   particular, if `training` is `true`, the old states will be kept so\n   *   that subsequent backpropgataion through time (BPTT) may work properly.\n   *   Else, the old states will be discarded.\n   */\n\n\n  resetStates(states, training = false) {\n    tidy(() => {\n      if (!this.stateful) {\n        throw new AttributeError('Cannot call resetStates() on an RNN Layer that is not stateful.');\n      }\n\n      const batchSize = this.inputSpec[0].shape[0];\n\n      if (batchSize == null) {\n        throw new ValueError('If an RNN is stateful, it needs to know its batch size. Specify ' + 'the batch size of your input tensors: \\n' + '- If using a Sequential model, specify the batch size by ' + 'passing a `batchInputShape` option to your first layer.\\n' + '- If using the functional API, specify the batch size by ' + 'passing a `batchShape` option to your Input layer.');\n      } // Initialize state if null.\n\n\n      if (this.states_ == null) {\n        if (Array.isArray(this.cell.stateSize)) {\n          this.states_ = this.cell.stateSize.map(dim => tfc.zeros([batchSize, dim]));\n        } else {\n          this.states_ = [tfc.zeros([batchSize, this.cell.stateSize])];\n        }\n      } else if (states == null) {\n        // Dispose old state tensors.\n        tfc.dispose(this.states_); // For stateful RNNs, fully dispose kept old states.\n\n        if (this.keptStates != null) {\n          tfc.dispose(this.keptStates);\n          this.keptStates = [];\n        }\n\n        if (Array.isArray(this.cell.stateSize)) {\n          this.states_ = this.cell.stateSize.map(dim => tfc.zeros([batchSize, dim]));\n        } else {\n          this.states_[0] = tfc.zeros([batchSize, this.cell.stateSize]);\n        }\n      } else {\n        if (!Array.isArray(states)) {\n          states = [states];\n        }\n\n        if (states.length !== this.states_.length) {\n          throw new ValueError(`Layer ${this.name} expects ${this.states_.length} state(s), ` + `but it received ${states.length} state value(s). Input ` + `received: ${states}`);\n        }\n\n        if (training === true) {\n          // Store old state tensors for complete disposal later, i.e., during\n          // the next no-arg call to this method. We do not dispose the old\n          // states immediately because that BPTT (among other things) require\n          // them.\n          this.keptStates.push(this.states_.slice());\n        } else {\n          tfc.dispose(this.states_);\n        }\n\n        for (let index = 0; index < this.states_.length; ++index) {\n          const value = states[index];\n          const dim = Array.isArray(this.cell.stateSize) ? this.cell.stateSize[index] : this.cell.stateSize;\n          const expectedShape = [batchSize, dim];\n\n          if (!util.arraysEqual(value.shape, expectedShape)) {\n            throw new ValueError(`State ${index} is incompatible with layer ${this.name}: ` + `expected shape=${expectedShape}, received shape=${value.shape}`);\n          }\n\n          this.states_[index] = value;\n        }\n      }\n\n      this.states_ = this.states_.map(state => tfc.keep(state.clone()));\n    });\n  }\n\n  apply(inputs, kwargs) {\n    // TODO(cais): Figure out whether initialState is in kwargs or inputs.\n    let initialState = kwargs == null ? null : kwargs['initialState'];\n    let constants = kwargs == null ? null : kwargs['constants'];\n\n    if (kwargs == null) {\n      kwargs = {};\n    }\n\n    const standardized = standardizeArgs(inputs, initialState, constants, this.numConstants);\n    inputs = standardized.inputs;\n    initialState = standardized.initialState;\n    constants = standardized.constants; // If any of `initial_state` or `constants` are specified and are\n    // `tf.SymbolicTensor`s, then add them to the inputs and temporarily modify\n    // the input_spec to include them.\n\n    let additionalInputs = [];\n    let additionalSpecs = [];\n\n    if (initialState != null) {\n      kwargs['initialState'] = initialState;\n      additionalInputs = additionalInputs.concat(initialState);\n      this.stateSpec = [];\n\n      for (const state of initialState) {\n        this.stateSpec.push(new InputSpec({\n          shape: state.shape\n        }));\n      } // TODO(cais): Use the following instead.\n      // this.stateSpec = initialState.map(state => new InputSpec({shape:\n      // state.shape}));\n\n\n      additionalSpecs = additionalSpecs.concat(this.stateSpec);\n    }\n\n    if (constants != null) {\n      kwargs['constants'] = constants;\n      additionalInputs = additionalInputs.concat(constants); // TODO(cais): Add this.constantsSpec.\n\n      this.numConstants = constants.length;\n    }\n\n    const isTensor = additionalInputs[0] instanceof SymbolicTensor;\n\n    if (isTensor) {\n      // Compute full input spec, including state and constants.\n      const fullInput = [inputs].concat(additionalInputs);\n      const fullInputSpec = this.inputSpec.concat(additionalSpecs); // Perform the call with temporarily replaced inputSpec.\n\n      const originalInputSpec = this.inputSpec;\n      this.inputSpec = fullInputSpec;\n      const output = super.apply(fullInput, kwargs);\n      this.inputSpec = originalInputSpec;\n      return output;\n    } else {\n      return super.apply(inputs, kwargs);\n    }\n  } // tslint:disable-next-line:no-any\n\n\n  call(inputs, kwargs) {\n    // Input shape: `[samples, time (padded with zeros), input_dim]`.\n    // Note that the .build() method of subclasses **must** define\n    // this.inputSpec and this.stateSpec owith complete input shapes.\n    return tidy(() => {\n      const mask = kwargs == null ? null : kwargs['mask'];\n      const training = kwargs == null ? null : kwargs['training'];\n      let initialState = kwargs == null ? null : kwargs['initialState'];\n      inputs = getExactlyOneTensor(inputs);\n\n      if (initialState == null) {\n        if (this.stateful) {\n          initialState = this.states_;\n        } else {\n          initialState = this.getInitialState(inputs);\n        }\n      }\n\n      const numStates = Array.isArray(this.cell.stateSize) ? this.cell.stateSize.length : 1;\n\n      if (initialState.length !== numStates) {\n        throw new ValueError(`RNN Layer has ${numStates} state(s) but was passed ` + `${initialState.length} initial state(s).`);\n      }\n\n      if (this.unroll) {\n        console.warn('Ignoring unroll = true for RNN layer, due to imperative backend.');\n      }\n\n      const cellCallKwargs = {\n        training\n      }; // TODO(cais): Add support for constants.\n\n      const step = (inputs, states) => {\n        // `inputs` and `states` are concatenated to form a single `Array` of\n        // `tf.Tensor`s as the input to `cell.call()`.\n        const outputs = this.cell.call([inputs].concat(states), cellCallKwargs); // Marshall the return value into output and new states.\n\n        return [outputs[0], outputs.slice(1)];\n      }; // TODO(cais): Add support for constants.\n\n\n      const rnnOutputs = rnn(step, inputs, initialState, this.goBackwards, mask, null, this.unroll, this.returnSequences);\n      const lastOutput = rnnOutputs[0];\n      const outputs = rnnOutputs[1];\n      const states = rnnOutputs[2];\n\n      if (this.stateful) {\n        this.resetStates(states, training);\n      }\n\n      const output = this.returnSequences ? outputs : lastOutput; // TODO(cais): Porperty set learning phase flag.\n\n      if (this.returnState) {\n        return [output].concat(states);\n      } else {\n        return output;\n      }\n    });\n  }\n\n  getInitialState(inputs) {\n    return tidy(() => {\n      // Build an all-zero tensor of shape [samples, outputDim].\n      // [Samples, timeSteps, inputDim].\n      let initialState = tfc.zeros(inputs.shape); // [Samples].\n\n      initialState = tfc.sum(initialState, [1, 2]);\n      initialState = K.expandDims(initialState); // [Samples, 1].\n\n      if (Array.isArray(this.cell.stateSize)) {\n        return this.cell.stateSize.map(dim => dim > 1 ? K.tile(initialState, [1, dim]) : initialState);\n      } else {\n        return this.cell.stateSize > 1 ? [K.tile(initialState, [1, this.cell.stateSize])] : [initialState];\n      }\n    });\n  }\n\n  get trainableWeights() {\n    if (!this.trainable) {\n      return [];\n    } // Porting Note: In TypeScript, `this` is always an instance of `Layer`.\n\n\n    return this.cell.trainableWeights;\n  }\n\n  get nonTrainableWeights() {\n    // Porting Note: In TypeScript, `this` is always an instance of `Layer`.\n    if (!this.trainable) {\n      return this.cell.weights;\n    }\n\n    return this.cell.nonTrainableWeights;\n  }\n\n  setFastWeightInitDuringBuild(value) {\n    super.setFastWeightInitDuringBuild(value);\n\n    if (this.cell != null) {\n      this.cell.setFastWeightInitDuringBuild(value);\n    }\n  }\n\n  getConfig() {\n    const baseConfig = super.getConfig();\n    const config = {\n      returnSequences: this.returnSequences,\n      returnState: this.returnState,\n      goBackwards: this.goBackwards,\n      stateful: this.stateful,\n      unroll: this.unroll\n    };\n\n    if (this.numConstants != null) {\n      config['numConstants'] = this.numConstants;\n    }\n\n    const cellConfig = this.cell.getConfig();\n\n    if (this.getClassName() === RNN.className) {\n      config['cell'] = {\n        'className': this.cell.getClassName(),\n        'config': cellConfig\n      };\n    } // this order is necessary, to prevent cell name from replacing layer name\n\n\n    return Object.assign({}, cellConfig, baseConfig, config);\n  }\n  /** @nocollapse */\n\n\n  static fromConfig(cls, config, customObjects = {}) {\n    const cellConfig = config['cell'];\n    const cell = deserialize(cellConfig, customObjects);\n    return new cls(Object.assign(config, {\n      cell\n    }));\n  }\n\n}\n/** @nocollapse */\n\nRNN.className = 'RNN';\nserialization.registerClass(RNN); // Porting Note: This is a common parent class for RNN cells. There is no\n// equivalent of this in PyKeras. Having a common parent class forgoes the\n//  need for `has_attr(cell, ...)` checks or its TypeScript equivalent.\n\n/**\n * An RNNCell layer.\n *\n * @doc {heading: 'Layers', subheading: 'Classes'}\n */\n\nexport class RNNCell extends Layer {}\nexport class SimpleRNNCell extends RNNCell {\n  constructor(args) {\n    super(args);\n    this.DEFAULT_ACTIVATION = 'tanh';\n    this.DEFAULT_KERNEL_INITIALIZER = 'glorotNormal';\n    this.DEFAULT_RECURRENT_INITIALIZER = 'orthogonal';\n    this.DEFAULT_BIAS_INITIALIZER = 'zeros';\n    this.units = args.units;\n    assertPositiveInteger(this.units, `units`);\n    this.activation = getActivation(args.activation == null ? this.DEFAULT_ACTIVATION : args.activation);\n    this.useBias = args.useBias == null ? true : args.useBias;\n    this.kernelInitializer = getInitializer(args.kernelInitializer || this.DEFAULT_KERNEL_INITIALIZER);\n    this.recurrentInitializer = getInitializer(args.recurrentInitializer || this.DEFAULT_RECURRENT_INITIALIZER);\n    this.biasInitializer = getInitializer(args.biasInitializer || this.DEFAULT_BIAS_INITIALIZER);\n    this.kernelRegularizer = getRegularizer(args.kernelRegularizer);\n    this.recurrentRegularizer = getRegularizer(args.recurrentRegularizer);\n    this.biasRegularizer = getRegularizer(args.biasRegularizer);\n    this.kernelConstraint = getConstraint(args.kernelConstraint);\n    this.recurrentConstraint = getConstraint(args.recurrentConstraint);\n    this.biasConstraint = getConstraint(args.biasConstraint);\n    this.dropout = math_utils.min([1, math_utils.max([0, args.dropout == null ? 0 : args.dropout])]);\n    this.recurrentDropout = math_utils.min([1, math_utils.max([0, args.recurrentDropout == null ? 0 : args.recurrentDropout])]);\n    this.dropoutFunc = args.dropoutFunc;\n    this.stateSize = this.units;\n    this.dropoutMask = null;\n    this.recurrentDropoutMask = null;\n  }\n\n  build(inputShape) {\n    inputShape = getExactlyOneShape(inputShape); // TODO(cais): Use regularizer.\n\n    this.kernel = this.addWeight('kernel', [inputShape[inputShape.length - 1], this.units], null, this.kernelInitializer, this.kernelRegularizer, true, this.kernelConstraint);\n    this.recurrentKernel = this.addWeight('recurrent_kernel', [this.units, this.units], null, this.recurrentInitializer, this.recurrentRegularizer, true, this.recurrentConstraint);\n\n    if (this.useBias) {\n      this.bias = this.addWeight('bias', [this.units], null, this.biasInitializer, this.biasRegularizer, true, this.biasConstraint);\n    } else {\n      this.bias = null;\n    }\n\n    this.built = true;\n  } // Porting Note: PyKeras' equivalent of this method takes two tensor inputs:\n  //   `inputs` and `states`. Here, the two tensors are combined into an\n  //   `Tensor[]` Array as the first input argument.\n  //   Similarly, PyKeras' equivalent of this method returns two values:\n  //    `output` and `[output]`. Here the two are combined into one length-2\n  //    `Tensor[]`, consisting of `output` repeated.\n\n\n  call(inputs, kwargs) {\n    return tidy(() => {\n      inputs = inputs;\n\n      if (inputs.length !== 2) {\n        throw new ValueError(`SimpleRNNCell expects 2 input Tensors, got ${inputs.length}.`);\n      }\n\n      let prevOutput = inputs[1];\n      inputs = inputs[0];\n      const training = kwargs['training'] == null ? false : kwargs['training'];\n\n      if (0 < this.dropout && this.dropout < 1 && this.dropoutMask == null) {\n        this.dropoutMask = generateDropoutMask({\n          ones: () => tfc.onesLike(inputs),\n          rate: this.dropout,\n          training,\n          dropoutFunc: this.dropoutFunc\n        });\n      }\n\n      if (0 < this.recurrentDropout && this.recurrentDropout < 1 && this.recurrentDropoutMask == null) {\n        this.recurrentDropoutMask = generateDropoutMask({\n          ones: () => tfc.onesLike(prevOutput),\n          rate: this.recurrentDropout,\n          training,\n          dropoutFunc: this.dropoutFunc\n        });\n      }\n\n      let h;\n      const dpMask = this.dropoutMask;\n      const recDpMask = this.recurrentDropoutMask;\n\n      if (dpMask != null) {\n        h = K.dot(tfc.mul(inputs, dpMask), this.kernel.read());\n      } else {\n        h = K.dot(inputs, this.kernel.read());\n      }\n\n      if (this.bias != null) {\n        h = K.biasAdd(h, this.bias.read());\n      }\n\n      if (recDpMask != null) {\n        prevOutput = tfc.mul(prevOutput, recDpMask);\n      }\n\n      let output = tfc.add(h, K.dot(prevOutput, this.recurrentKernel.read()));\n\n      if (this.activation != null) {\n        output = this.activation.apply(output);\n      } // TODO(cais): Properly set learning phase on output tensor?\n\n\n      return [output, output];\n    });\n  }\n\n  getConfig() {\n    const baseConfig = super.getConfig();\n    const config = {\n      units: this.units,\n      activation: serializeActivation(this.activation),\n      useBias: this.useBias,\n      kernelInitializer: serializeInitializer(this.kernelInitializer),\n      recurrentInitializer: serializeInitializer(this.recurrentInitializer),\n      biasInitializer: serializeInitializer(this.biasInitializer),\n      kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n      recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer),\n      biasRegularizer: serializeRegularizer(this.biasRegularizer),\n      activityRegularizer: serializeRegularizer(this.activityRegularizer),\n      kernelConstraint: serializeConstraint(this.kernelConstraint),\n      recurrentConstraint: serializeConstraint(this.recurrentConstraint),\n      biasConstraint: serializeConstraint(this.biasConstraint),\n      dropout: this.dropout,\n      recurrentDropout: this.recurrentDropout\n    };\n    return Object.assign({}, baseConfig, config);\n  }\n\n}\n/** @nocollapse */\n\nSimpleRNNCell.className = 'SimpleRNNCell';\nserialization.registerClass(SimpleRNNCell);\nexport class SimpleRNN extends RNN {\n  constructor(args) {\n    args.cell = new SimpleRNNCell(args);\n    super(args); // TODO(cais): Add activityRegularizer.\n  }\n\n  call(inputs, kwargs) {\n    return tidy(() => {\n      if (this.cell.dropoutMask != null) {\n        tfc.dispose(this.cell.dropoutMask);\n        this.cell.dropoutMask = null;\n      }\n\n      if (this.cell.recurrentDropoutMask != null) {\n        tfc.dispose(this.cell.recurrentDropoutMask);\n        this.cell.recurrentDropoutMask = null;\n      }\n\n      const mask = kwargs == null ? null : kwargs['mask'];\n      const training = kwargs == null ? null : kwargs['training'];\n      const initialState = kwargs == null ? null : kwargs['initialState'];\n      return super.call(inputs, {\n        mask,\n        training,\n        initialState\n      });\n    });\n  }\n  /** @nocollapse */\n\n\n  static fromConfig(cls, config) {\n    return new cls(config);\n  }\n\n}\n/** @nocollapse */\n\nSimpleRNN.className = 'SimpleRNN';\nserialization.registerClass(SimpleRNN);\nexport class GRUCell extends RNNCell {\n  constructor(args) {\n    super(args);\n    this.DEFAULT_ACTIVATION = 'tanh';\n    this.DEFAULT_RECURRENT_ACTIVATION = 'hardSigmoid';\n    this.DEFAULT_KERNEL_INITIALIZER = 'glorotNormal';\n    this.DEFAULT_RECURRENT_INITIALIZER = 'orthogonal';\n    this.DEFAULT_BIAS_INITIALIZER = 'zeros';\n\n    if (args.resetAfter) {\n      throw new ValueError(`GRUCell does not support reset_after parameter set to true.`);\n    }\n\n    this.units = args.units;\n    assertPositiveInteger(this.units, 'units');\n    this.activation = getActivation(args.activation === undefined ? this.DEFAULT_ACTIVATION : args.activation);\n    this.recurrentActivation = getActivation(args.recurrentActivation === undefined ? this.DEFAULT_RECURRENT_ACTIVATION : args.recurrentActivation);\n    this.useBias = args.useBias == null ? true : args.useBias;\n    this.kernelInitializer = getInitializer(args.kernelInitializer || this.DEFAULT_KERNEL_INITIALIZER);\n    this.recurrentInitializer = getInitializer(args.recurrentInitializer || this.DEFAULT_RECURRENT_INITIALIZER);\n    this.biasInitializer = getInitializer(args.biasInitializer || this.DEFAULT_BIAS_INITIALIZER);\n    this.kernelRegularizer = getRegularizer(args.kernelRegularizer);\n    this.recurrentRegularizer = getRegularizer(args.recurrentRegularizer);\n    this.biasRegularizer = getRegularizer(args.biasRegularizer);\n    this.kernelConstraint = getConstraint(args.kernelConstraint);\n    this.recurrentConstraint = getConstraint(args.recurrentConstraint);\n    this.biasConstraint = getConstraint(args.biasConstraint);\n    this.dropout = math_utils.min([1, math_utils.max([0, args.dropout == null ? 0 : args.dropout])]);\n    this.recurrentDropout = math_utils.min([1, math_utils.max([0, args.recurrentDropout == null ? 0 : args.recurrentDropout])]);\n    this.dropoutFunc = args.dropoutFunc;\n    this.implementation = args.implementation;\n    this.stateSize = this.units;\n    this.dropoutMask = null;\n    this.recurrentDropoutMask = null;\n  }\n\n  build(inputShape) {\n    inputShape = getExactlyOneShape(inputShape);\n    const inputDim = inputShape[inputShape.length - 1];\n    this.kernel = this.addWeight('kernel', [inputDim, this.units * 3], null, this.kernelInitializer, this.kernelRegularizer, true, this.kernelConstraint);\n    this.recurrentKernel = this.addWeight('recurrent_kernel', [this.units, this.units * 3], null, this.recurrentInitializer, this.recurrentRegularizer, true, this.recurrentConstraint);\n\n    if (this.useBias) {\n      this.bias = this.addWeight('bias', [this.units * 3], null, this.biasInitializer, this.biasRegularizer, true, this.biasConstraint);\n    } else {\n      this.bias = null;\n    } // Porting Notes: Unlike the PyKeras implementation, we perform slicing\n    //   of the weights and bias in the call() method, at execution time.\n\n\n    this.built = true;\n  }\n\n  call(inputs, kwargs) {\n    return tidy(() => {\n      inputs = inputs;\n\n      if (inputs.length !== 2) {\n        throw new ValueError(`GRUCell expects 2 input Tensors (inputs, h, c), got ` + `${inputs.length}.`);\n      }\n\n      const training = kwargs['training'] == null ? false : kwargs['training'];\n      let hTMinus1 = inputs[1]; // Previous memory state.\n\n      inputs = inputs[0]; // Note: For superior performance, TensorFlow.js always uses\n      // implementation 2, regardless of the actual value of\n      // config.implementation.\n\n      if (0 < this.dropout && this.dropout < 1 && this.dropoutMask == null) {\n        this.dropoutMask = generateDropoutMask({\n          ones: () => tfc.onesLike(inputs),\n          rate: this.dropout,\n          training,\n          count: 3,\n          dropoutFunc: this.dropoutFunc\n        });\n      }\n\n      if (0 < this.recurrentDropout && this.recurrentDropout < 1 && this.recurrentDropoutMask == null) {\n        this.recurrentDropoutMask = generateDropoutMask({\n          ones: () => tfc.onesLike(hTMinus1),\n          rate: this.recurrentDropout,\n          training,\n          count: 3,\n          dropoutFunc: this.dropoutFunc\n        });\n      }\n\n      const dpMask = this.dropoutMask;\n      const recDpMask = this.recurrentDropoutMask;\n      let z;\n      let r;\n      let hh;\n\n      if (0 < this.dropout && this.dropout < 1) {\n        inputs = tfc.mul(inputs, dpMask[0]);\n      }\n\n      let matrixX = K.dot(inputs, this.kernel.read());\n\n      if (this.useBias) {\n        matrixX = K.biasAdd(matrixX, this.bias.read());\n      }\n\n      if (0 < this.recurrentDropout && this.recurrentDropout < 1) {\n        hTMinus1 = tfc.mul(hTMinus1, recDpMask[0]);\n      }\n\n      const recurrentKernelValue = this.recurrentKernel.read();\n      const [rk1, rk2] = tfc.split(recurrentKernelValue, [2 * this.units, this.units], recurrentKernelValue.rank - 1);\n      const matrixInner = K.dot(hTMinus1, rk1);\n      const [xZ, xR, xH] = tfc.split(matrixX, 3, matrixX.rank - 1);\n      const [recurrentZ, recurrentR] = tfc.split(matrixInner, 2, matrixInner.rank - 1);\n      z = this.recurrentActivation.apply(tfc.add(xZ, recurrentZ));\n      r = this.recurrentActivation.apply(tfc.add(xR, recurrentR));\n      const recurrentH = K.dot(tfc.mul(r, hTMinus1), rk2);\n      hh = this.activation.apply(tfc.add(xH, recurrentH));\n      const h = tfc.add(tfc.mul(z, hTMinus1), tfc.mul(tfc.add(1, tfc.neg(z)), hh)); // TODO(cais): Add use_learning_phase flag properly.\n\n      return [h, h];\n    });\n  }\n\n  getConfig() {\n    const baseConfig = super.getConfig();\n    const config = {\n      units: this.units,\n      activation: serializeActivation(this.activation),\n      recurrentActivation: serializeActivation(this.recurrentActivation),\n      useBias: this.useBias,\n      kernelInitializer: serializeInitializer(this.kernelInitializer),\n      recurrentInitializer: serializeInitializer(this.recurrentInitializer),\n      biasInitializer: serializeInitializer(this.biasInitializer),\n      kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n      recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer),\n      biasRegularizer: serializeRegularizer(this.biasRegularizer),\n      activityRegularizer: serializeRegularizer(this.activityRegularizer),\n      kernelConstraint: serializeConstraint(this.kernelConstraint),\n      recurrentConstraint: serializeConstraint(this.recurrentConstraint),\n      biasConstraint: serializeConstraint(this.biasConstraint),\n      dropout: this.dropout,\n      recurrentDropout: this.recurrentDropout,\n      implementation: this.implementation,\n      resetAfter: false\n    };\n    return Object.assign({}, baseConfig, config);\n  }\n\n}\n/** @nocollapse */\n\nGRUCell.className = 'GRUCell';\nserialization.registerClass(GRUCell);\nexport class GRU extends RNN {\n  constructor(args) {\n    if (args.implementation === 0) {\n      console.warn('`implementation=0` has been deprecated, and now defaults to ' + '`implementation=1`. Please update your layer call.');\n    }\n\n    args.cell = new GRUCell(args);\n    super(args); // TODO(cais): Add activityRegularizer.\n  }\n\n  call(inputs, kwargs) {\n    return tidy(() => {\n      if (this.cell.dropoutMask != null) {\n        tfc.dispose(this.cell.dropoutMask);\n        this.cell.dropoutMask = null;\n      }\n\n      if (this.cell.recurrentDropoutMask != null) {\n        tfc.dispose(this.cell.recurrentDropoutMask);\n        this.cell.recurrentDropoutMask = null;\n      }\n\n      const mask = kwargs == null ? null : kwargs['mask'];\n      const training = kwargs == null ? null : kwargs['training'];\n      const initialState = kwargs == null ? null : kwargs['initialState'];\n      return super.call(inputs, {\n        mask,\n        training,\n        initialState\n      });\n    });\n  }\n  /** @nocollapse */\n\n\n  static fromConfig(cls, config) {\n    if (config['implmentation'] === 0) {\n      config['implementation'] = 1;\n    }\n\n    return new cls(config);\n  }\n\n}\n/** @nocollapse */\n\nGRU.className = 'GRU';\nserialization.registerClass(GRU);\nexport class LSTMCell extends RNNCell {\n  constructor(args) {\n    super(args);\n    this.DEFAULT_ACTIVATION = 'tanh';\n    this.DEFAULT_RECURRENT_ACTIVATION = 'hardSigmoid';\n    this.DEFAULT_KERNEL_INITIALIZER = 'glorotNormal';\n    this.DEFAULT_RECURRENT_INITIALIZER = 'orthogonal';\n    this.DEFAULT_BIAS_INITIALIZER = 'zeros';\n    this.units = args.units;\n    assertPositiveInteger(this.units, 'units');\n    this.activation = getActivation(args.activation === undefined ? this.DEFAULT_ACTIVATION : args.activation);\n    this.recurrentActivation = getActivation(args.recurrentActivation === undefined ? this.DEFAULT_RECURRENT_ACTIVATION : args.recurrentActivation);\n    this.useBias = args.useBias == null ? true : args.useBias;\n    this.kernelInitializer = getInitializer(args.kernelInitializer || this.DEFAULT_KERNEL_INITIALIZER);\n    this.recurrentInitializer = getInitializer(args.recurrentInitializer || this.DEFAULT_RECURRENT_INITIALIZER);\n    this.biasInitializer = getInitializer(args.biasInitializer || this.DEFAULT_BIAS_INITIALIZER);\n    this.unitForgetBias = args.unitForgetBias;\n    this.kernelRegularizer = getRegularizer(args.kernelRegularizer);\n    this.recurrentRegularizer = getRegularizer(args.recurrentRegularizer);\n    this.biasRegularizer = getRegularizer(args.biasRegularizer);\n    this.kernelConstraint = getConstraint(args.kernelConstraint);\n    this.recurrentConstraint = getConstraint(args.recurrentConstraint);\n    this.biasConstraint = getConstraint(args.biasConstraint);\n    this.dropout = math_utils.min([1, math_utils.max([0, args.dropout == null ? 0 : args.dropout])]);\n    this.recurrentDropout = math_utils.min([1, math_utils.max([0, args.recurrentDropout == null ? 0 : args.recurrentDropout])]);\n    this.dropoutFunc = args.dropoutFunc;\n    this.implementation = args.implementation;\n    this.stateSize = [this.units, this.units];\n    this.dropoutMask = null;\n    this.recurrentDropoutMask = null;\n  }\n\n  build(inputShape) {\n    var _a;\n\n    inputShape = getExactlyOneShape(inputShape);\n    const inputDim = inputShape[inputShape.length - 1];\n    this.kernel = this.addWeight('kernel', [inputDim, this.units * 4], null, this.kernelInitializer, this.kernelRegularizer, true, this.kernelConstraint);\n    this.recurrentKernel = this.addWeight('recurrent_kernel', [this.units, this.units * 4], null, this.recurrentInitializer, this.recurrentRegularizer, true, this.recurrentConstraint);\n    let biasInitializer;\n\n    if (this.useBias) {\n      if (this.unitForgetBias) {\n        const capturedBiasInit = this.biasInitializer;\n        const capturedUnits = this.units;\n        biasInitializer = new (_a = class CustomInit extends Initializer {\n          apply(shape, dtype) {\n            // TODO(cais): More informative variable names?\n            const bI = capturedBiasInit.apply([capturedUnits]);\n            const bF = new Ones().apply([capturedUnits]);\n            const bCAndH = capturedBiasInit.apply([capturedUnits * 2]);\n            return K.concatAlongFirstAxis(K.concatAlongFirstAxis(bI, bF), bCAndH);\n          }\n\n        },\n        /** @nocollapse */\n        _a.className = 'CustomInit', _a)();\n      } else {\n        biasInitializer = this.biasInitializer;\n      }\n\n      this.bias = this.addWeight('bias', [this.units * 4], null, biasInitializer, this.biasRegularizer, true, this.biasConstraint);\n    } else {\n      this.bias = null;\n    } // Porting Notes: Unlike the PyKeras implementation, we perform slicing\n    //   of the weights and bias in the call() method, at execution time.\n\n\n    this.built = true;\n  }\n\n  call(inputs, kwargs) {\n    return tidy(() => {\n      const training = kwargs['training'] == null ? false : kwargs['training'];\n      inputs = inputs;\n\n      if (inputs.length !== 3) {\n        throw new ValueError(`LSTMCell expects 3 input Tensors (inputs, h, c), got ` + `${inputs.length}.`);\n      }\n\n      let hTMinus1 = inputs[1]; // Previous memory state.\n\n      const cTMinus1 = inputs[2]; // Previous carry state.\n\n      inputs = inputs[0];\n\n      if (0 < this.dropout && this.dropout < 1 && this.dropoutMask == null) {\n        this.dropoutMask = generateDropoutMask({\n          ones: () => tfc.onesLike(inputs),\n          rate: this.dropout,\n          training,\n          count: 4,\n          dropoutFunc: this.dropoutFunc\n        });\n      }\n\n      if (0 < this.recurrentDropout && this.recurrentDropout < 1 && this.recurrentDropoutMask == null) {\n        this.recurrentDropoutMask = generateDropoutMask({\n          ones: () => tfc.onesLike(hTMinus1),\n          rate: this.recurrentDropout,\n          training,\n          count: 4,\n          dropoutFunc: this.dropoutFunc\n        });\n      }\n\n      const dpMask = this.dropoutMask;\n      const recDpMask = this.recurrentDropoutMask; // Note: For superior performance, TensorFlow.js always uses\n      // implementation 2 regardless of the actual value of\n      // config.implementation.\n\n      let i;\n      let f;\n      let c;\n      let o;\n\n      if (0 < this.dropout && this.dropout < 1) {\n        inputs = tfc.mul(inputs, dpMask[0]);\n      }\n\n      let z = K.dot(inputs, this.kernel.read());\n\n      if (0 < this.recurrentDropout && this.recurrentDropout < 1) {\n        hTMinus1 = tfc.mul(hTMinus1, recDpMask[0]);\n      }\n\n      z = tfc.add(z, K.dot(hTMinus1, this.recurrentKernel.read()));\n\n      if (this.useBias) {\n        z = K.biasAdd(z, this.bias.read());\n      }\n\n      const [z0, z1, z2, z3] = tfc.split(z, 4, z.rank - 1);\n      i = this.recurrentActivation.apply(z0);\n      f = this.recurrentActivation.apply(z1);\n      c = tfc.add(tfc.mul(f, cTMinus1), tfc.mul(i, this.activation.apply(z2)));\n      o = this.recurrentActivation.apply(z3);\n      const h = tfc.mul(o, this.activation.apply(c)); // TODO(cais): Add use_learning_phase flag properly.\n\n      return [h, h, c];\n    });\n  }\n\n  getConfig() {\n    const baseConfig = super.getConfig();\n    const config = {\n      units: this.units,\n      activation: serializeActivation(this.activation),\n      recurrentActivation: serializeActivation(this.recurrentActivation),\n      useBias: this.useBias,\n      kernelInitializer: serializeInitializer(this.kernelInitializer),\n      recurrentInitializer: serializeInitializer(this.recurrentInitializer),\n      biasInitializer: serializeInitializer(this.biasInitializer),\n      unitForgetBias: this.unitForgetBias,\n      kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n      recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer),\n      biasRegularizer: serializeRegularizer(this.biasRegularizer),\n      activityRegularizer: serializeRegularizer(this.activityRegularizer),\n      kernelConstraint: serializeConstraint(this.kernelConstraint),\n      recurrentConstraint: serializeConstraint(this.recurrentConstraint),\n      biasConstraint: serializeConstraint(this.biasConstraint),\n      dropout: this.dropout,\n      recurrentDropout: this.recurrentDropout,\n      implementation: this.implementation\n    };\n    return Object.assign({}, baseConfig, config);\n  }\n\n}\n/** @nocollapse */\n\nLSTMCell.className = 'LSTMCell';\nserialization.registerClass(LSTMCell);\nexport class LSTM extends RNN {\n  constructor(args) {\n    if (args.implementation === 0) {\n      console.warn('`implementation=0` has been deprecated, and now defaults to ' + '`implementation=1`. Please update your layer call.');\n    }\n\n    args.cell = new LSTMCell(args);\n    super(args); // TODO(cais): Add activityRegularizer.\n  }\n\n  call(inputs, kwargs) {\n    return tidy(() => {\n      if (this.cell.dropoutMask != null) {\n        tfc.dispose(this.cell.dropoutMask);\n        this.cell.dropoutMask = null;\n      }\n\n      if (this.cell.recurrentDropoutMask != null) {\n        tfc.dispose(this.cell.recurrentDropoutMask);\n        this.cell.recurrentDropoutMask = null;\n      }\n\n      const mask = kwargs == null ? null : kwargs['mask'];\n      const training = kwargs == null ? null : kwargs['training'];\n      const initialState = kwargs == null ? null : kwargs['initialState'];\n      return super.call(inputs, {\n        mask,\n        training,\n        initialState\n      });\n    });\n  }\n  /** @nocollapse */\n\n\n  static fromConfig(cls, config) {\n    if (config['implmentation'] === 0) {\n      config['implementation'] = 1;\n    }\n\n    return new cls(config);\n  }\n\n}\n/** @nocollapse */\n\nLSTM.className = 'LSTM';\nserialization.registerClass(LSTM);\nexport class StackedRNNCells extends RNNCell {\n  constructor(args) {\n    super(args);\n    this.cells = args.cells;\n  }\n\n  get stateSize() {\n    // States are a flat list in reverse order of the cell stack.\n    // This allows perserving the requirement `stack.statesize[0] ===\n    // outputDim`. E.g., states of a 2-layer LSTM would be `[h2, c2, h1, c1]`,\n    // assuming one LSTM has states `[h, c]`.\n    const stateSize = [];\n\n    for (const cell of this.cells.slice().reverse()) {\n      if (Array.isArray(cell.stateSize)) {\n        stateSize.push(...cell.stateSize);\n      } else {\n        stateSize.push(cell.stateSize);\n      }\n    }\n\n    return stateSize;\n  }\n\n  call(inputs, kwargs) {\n    return tidy(() => {\n      inputs = inputs;\n      let states = inputs.slice(1); // Recover per-cell states.\n\n      const nestedStates = [];\n\n      for (const cell of this.cells.slice().reverse()) {\n        if (Array.isArray(cell.stateSize)) {\n          nestedStates.push(states.splice(0, cell.stateSize.length));\n        } else {\n          nestedStates.push(states.splice(0, 1));\n        }\n      }\n\n      nestedStates.reverse(); // Call the cells in order and store the returned states.\n\n      const newNestedStates = [];\n      let callInputs;\n\n      for (let i = 0; i < this.cells.length; ++i) {\n        const cell = this.cells[i];\n        states = nestedStates[i]; // TODO(cais): Take care of constants.\n\n        if (i === 0) {\n          callInputs = [inputs[0]].concat(states);\n        } else {\n          callInputs = [callInputs[0]].concat(states);\n        }\n\n        callInputs = cell.call(callInputs, kwargs);\n        newNestedStates.push(callInputs.slice(1));\n      } // Format the new states as a flat list in reverse cell order.\n\n\n      states = [];\n\n      for (const cellStates of newNestedStates.slice().reverse()) {\n        states.push(...cellStates);\n      }\n\n      return [callInputs[0]].concat(states);\n    });\n  }\n\n  build(inputShape) {\n    if (isArrayOfShapes(inputShape)) {\n      // TODO(cais): Take care of input constants.\n      // const constantShape = inputShape.slice(1);\n      inputShape = inputShape[0];\n    }\n\n    inputShape = inputShape;\n    let outputDim;\n    this.cells.forEach((cell, i) => {\n      nameScope(`RNNCell_${i}`, () => {\n        // TODO(cais): Take care of input constants.\n        cell.build(inputShape);\n\n        if (Array.isArray(cell.stateSize)) {\n          outputDim = cell.stateSize[0];\n        } else {\n          outputDim = cell.stateSize;\n        }\n\n        inputShape = [inputShape[0], outputDim];\n      });\n    });\n    this.built = true;\n  }\n\n  getConfig() {\n    const baseConfig = super.getConfig();\n\n    const getCellConfig = cell => {\n      return {\n        'className': cell.getClassName(),\n        'config': cell.getConfig()\n      };\n    };\n\n    const cellConfigs = this.cells.map(getCellConfig);\n    const config = {\n      'cells': cellConfigs\n    };\n    return Object.assign({}, baseConfig, config);\n  }\n  /** @nocollapse */\n\n\n  static fromConfig(cls, config, customObjects = {}) {\n    const cells = [];\n\n    for (const cellConfig of config['cells']) {\n      cells.push(deserialize(cellConfig, customObjects));\n    }\n\n    return new cls({\n      cells\n    });\n  }\n\n  get trainableWeights() {\n    if (!this.trainable) {\n      return [];\n    }\n\n    const weights = [];\n\n    for (const cell of this.cells) {\n      weights.push(...cell.trainableWeights);\n    }\n\n    return weights;\n  }\n\n  get nonTrainableWeights() {\n    const weights = [];\n\n    for (const cell of this.cells) {\n      weights.push(...cell.nonTrainableWeights);\n    }\n\n    if (!this.trainable) {\n      const trainableWeights = [];\n\n      for (const cell of this.cells) {\n        trainableWeights.push(...cell.trainableWeights);\n      }\n\n      return trainableWeights.concat(weights);\n    }\n\n    return weights;\n  }\n  /**\n   * Retrieve the weights of a the model.\n   *\n   * @returns A flat `Array` of `tf.Tensor`s.\n   */\n\n\n  getWeights() {\n    const weights = [];\n\n    for (const cell of this.cells) {\n      weights.push(...cell.weights);\n    }\n\n    return batchGetValue(weights);\n  }\n  /**\n   * Set the weights of the model.\n   *\n   * @param weights An `Array` of `tf.Tensor`s with shapes and types matching\n   *     the output of `getWeights()`.\n   */\n\n\n  setWeights(weights) {\n    const tuples = [];\n\n    for (const cell of this.cells) {\n      const numParams = cell.weights.length;\n      const inputWeights = weights.splice(numParams);\n\n      for (let i = 0; i < cell.weights.length; ++i) {\n        tuples.push([cell.weights[i], inputWeights[i]]);\n      }\n    }\n\n    batchSetValue(tuples);\n  }\n\n}\n/** @nocollapse */\n\nStackedRNNCells.className = 'StackedRNNCells';\nserialization.registerClass(StackedRNNCells);\nexport function generateDropoutMask(args) {\n  const {\n    ones,\n    rate,\n    training = false,\n    count = 1,\n    dropoutFunc\n  } = args;\n\n  const droppedInputs = () => dropoutFunc != null ? dropoutFunc(ones(), rate) : K.dropout(ones(), rate);\n\n  const createMask = () => K.inTrainPhase(droppedInputs, ones, training); // just in case count is provided with null or undefined\n\n\n  if (!count || count <= 1) {\n    return tfc.keep(createMask().clone());\n  }\n\n  const masks = Array(count).fill(undefined).map(createMask);\n  return masks.map(m => tfc.keep(m.clone()));\n}","map":{"version":3,"names":["tfc","serialization","tidy","util","getActivation","serializeActivation","K","nameScope","getConstraint","serializeConstraint","InputSpec","SymbolicTensor","Layer","AttributeError","NotImplementedError","ValueError","getInitializer","Initializer","Ones","serializeInitializer","getRegularizer","serializeRegularizer","assertPositiveInteger","math_utils","getExactlyOneShape","getExactlyOneTensor","isArrayOfShapes","batchGetValue","batchSetValue","deserialize","standardizeArgs","inputs","initialState","constants","numConstants","Array","isArray","slice","length","toListOrNull","x","rnn","stepFunction","initialStates","goBackwards","mask","unroll","needPerStepOutputs","ndim","shape","axes","concat","range","transpose","console","warn","cast","rank","expandDims","reverse","perStepOutputs","lastOutput","states","timeSteps","perStepInputs","unstack","perStepMasks","t","currentInput","stepOutputs","maskedOutputs","stepMask","negStepMask","sub","onesLike","output","add","mul","newStates","map","state","i","push","outputs","axis","stack","RNN","constructor","args","cell","StackedRNNCells","cells","stateSize","returnSequences","returnState","_stateful","stateful","supportsMasking","inputSpec","stateSpec","states_","keptStates","getStates","numStates","setStates","computeOutputShape","inputShape","outputDim","outputShape","stateShape","dim","computeMask","outputMask","stateMask","s","build","constantShape","batchSize","inputDim","stepInputShape","arraysEqual","spec","resetStates","training","zeros","dispose","name","index","value","expectedShape","keep","clone","apply","kwargs","standardized","additionalInputs","additionalSpecs","isTensor","fullInput","fullInputSpec","originalInputSpec","call","getInitialState","cellCallKwargs","step","rnnOutputs","sum","tile","trainableWeights","trainable","nonTrainableWeights","weights","setFastWeightInitDuringBuild","getConfig","baseConfig","config","cellConfig","getClassName","className","Object","assign","fromConfig","cls","customObjects","registerClass","RNNCell","SimpleRNNCell","DEFAULT_ACTIVATION","DEFAULT_KERNEL_INITIALIZER","DEFAULT_RECURRENT_INITIALIZER","DEFAULT_BIAS_INITIALIZER","units","activation","useBias","kernelInitializer","recurrentInitializer","biasInitializer","kernelRegularizer","recurrentRegularizer","biasRegularizer","kernelConstraint","recurrentConstraint","biasConstraint","dropout","min","max","recurrentDropout","dropoutFunc","dropoutMask","recurrentDropoutMask","kernel","addWeight","recurrentKernel","bias","built","prevOutput","generateDropoutMask","ones","rate","h","dpMask","recDpMask","dot","read","biasAdd","activityRegularizer","SimpleRNN","GRUCell","DEFAULT_RECURRENT_ACTIVATION","resetAfter","undefined","recurrentActivation","implementation","hTMinus1","count","z","r","hh","matrixX","recurrentKernelValue","rk1","rk2","split","matrixInner","xZ","xR","xH","recurrentZ","recurrentR","recurrentH","neg","GRU","LSTMCell","unitForgetBias","_a","capturedBiasInit","capturedUnits","CustomInit","dtype","bI","bF","bCAndH","concatAlongFirstAxis","cTMinus1","f","c","o","z0","z1","z2","z3","LSTM","nestedStates","splice","newNestedStates","callInputs","cellStates","forEach","getCellConfig","cellConfigs","getWeights","setWeights","tuples","numParams","inputWeights","droppedInputs","createMask","inTrainPhase","masks","fill","m"],"sources":["C:/Users/Anke/Documents/Feelix documents/Feelix2.0-dev/Feelix v2/node_modules/@tensorflow/tfjs-layers/dist/layers/recurrent.js"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n * TensorFlow.js Layers: Recurrent Neural Network Layers.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { serialization, tidy, util } from '@tensorflow/tfjs-core';\nimport { getActivation, serializeActivation } from '../activations';\nimport * as K from '../backend/tfjs_backend';\nimport { nameScope } from '../common';\nimport { getConstraint, serializeConstraint } from '../constraints';\nimport { InputSpec, SymbolicTensor } from '../engine/topology';\nimport { Layer } from '../engine/topology';\nimport { AttributeError, NotImplementedError, ValueError } from '../errors';\nimport { getInitializer, Initializer, Ones, serializeInitializer } from '../initializers';\nimport { getRegularizer, serializeRegularizer } from '../regularizers';\nimport { assertPositiveInteger } from '../utils/generic_utils';\nimport * as math_utils from '../utils/math_utils';\nimport { getExactlyOneShape, getExactlyOneTensor, isArrayOfShapes } from '../utils/types_utils';\nimport { batchGetValue, batchSetValue } from '../variables';\nimport { deserialize } from './serialization';\n/**\n * Standardize `apply()` args to a single list of tensor inputs.\n *\n * When running a model loaded from file, the input tensors `initialState` and\n * `constants` are passed to `RNN.apply()` as part of `inputs` instead of the\n * dedicated kwargs fields. `inputs` consists of\n * `[inputs, initialState0, initialState1, ..., constant0, constant1]` in this\n * case.\n * This method makes sure that arguments are\n * separated and that `initialState` and `constants` are `Array`s of tensors\n * (or None).\n *\n * @param inputs Tensor or `Array` of  tensors.\n * @param initialState Tensor or `Array` of tensors or `null`/`undefined`.\n * @param constants Tensor or `Array` of tensors or `null`/`undefined`.\n * @returns An object consisting of\n *   inputs: A tensor.\n *   initialState: `Array` of tensors or `null`.\n *   constants: `Array` of tensors or `null`.\n * @throws ValueError, if `inputs` is an `Array` but either `initialState` or\n *   `constants` is provided.\n */\nexport function standardizeArgs(inputs, initialState, constants, numConstants) {\n    if (Array.isArray(inputs)) {\n        if (initialState != null || constants != null) {\n            throw new ValueError('When inputs is an array, neither initialState or constants ' +\n                'should be provided');\n        }\n        if (numConstants != null) {\n            constants = inputs.slice(inputs.length - numConstants, inputs.length);\n            inputs = inputs.slice(0, inputs.length - numConstants);\n        }\n        if (inputs.length > 1) {\n            initialState = inputs.slice(1, inputs.length);\n        }\n        inputs = inputs[0];\n    }\n    function toListOrNull(x) {\n        if (x == null || Array.isArray(x)) {\n            return x;\n        }\n        else {\n            return [x];\n        }\n    }\n    initialState = toListOrNull(initialState);\n    constants = toListOrNull(constants);\n    return { inputs, initialState, constants };\n}\n/**\n * Iterates over the time dimension of a tensor.\n *\n * @param stepFunction RNN step function.\n *   Parameters:\n *     inputs: tensor with shape `[samples, ...]` (no time dimension),\n *       representing input for the batch of samples at a certain time step.\n *     states: an Array of tensors.\n *   Returns:\n *     outputs: tensor with shape `[samples, outputDim]` (no time dimension).\n *     newStates: list of tensors, same length and shapes as `states`. The first\n *       state in the list must be the output tensor at the previous timestep.\n * @param inputs Tensor of temporal data of shape `[samples, time, ...]` (at\n *   least 3D).\n * @param initialStates Tensor with shape `[samples, outputDim]` (no time\n *   dimension), containing the initial values of the states used in the step\n *   function.\n * @param goBackwards If `true`, do the iteration over the time dimension in\n *   reverse order and return the reversed sequence.\n * @param mask Binary tensor with shape `[sample, time, 1]`, with a zero for\n *   every element that is masked.\n * @param constants An Array of constant values passed at each step.\n * @param unroll Whether to unroll the RNN or to use a symbolic loop. *Not*\n *   applicable to this imperative deeplearn.js backend. Its value is ignored.\n * @param needPerStepOutputs Whether the per-step outputs are to be\n *   concatenated into a single tensor and returned (as the second return\n *   value). Default: `false`. This arg is included so that the relatively\n *   expensive concatenation of the stepwise outputs can be omitted unless\n *   the stepwise outputs need to be kept (e.g., for an LSTM layer of which\n *   `returnSequence` is `true`.)\n * @returns An Array: `[lastOutput, outputs, newStates]`.\n *   lastOutput: the lastest output of the RNN, of shape `[samples, ...]`.\n *   outputs: tensor with shape `[samples, time, ...]` where each entry\n *     `output[s, t]` is the output of the step function at time `t` for sample\n *     `s`. This return value is provided if and only if the\n *     `needPerStepOutputs` is set as `true`. If it is set as `false`, this\n *     return value will be `undefined`.\n *   newStates: Array of tensors, latest states returned by the step function,\n *      of shape `(samples, ...)`.\n * @throws ValueError If input dimension is less than 3.\n *\n * TODO(nielsene): This needs to be tidy-ed.\n */\nexport function rnn(stepFunction, inputs, initialStates, goBackwards = false, mask, constants, unroll = false, needPerStepOutputs = false) {\n    return tfc.tidy(() => {\n        const ndim = inputs.shape.length;\n        if (ndim < 3) {\n            throw new ValueError(`Input should be at least 3D, but is ${ndim}D.`);\n        }\n        // Transpose to time-major, i.e., from [batch, time, ...] to [time, batch,\n        // ...].\n        const axes = [1, 0].concat(math_utils.range(2, ndim));\n        inputs = tfc.transpose(inputs, axes);\n        if (constants != null) {\n            throw new NotImplementedError('The rnn() functoin of the deeplearn.js backend does not support ' +\n                'constants yet.');\n        }\n        // Porting Note: the unroll option is ignored by the imperative backend.\n        if (unroll) {\n            console.warn('Backend rnn(): the unroll = true option is not applicable to the ' +\n                'imperative deeplearn.js backend.');\n        }\n        if (mask != null) {\n            mask = tfc.cast(tfc.cast(mask, 'bool'), 'float32');\n            if (mask.rank === ndim - 1) {\n                mask = tfc.expandDims(mask, -1);\n            }\n            mask = tfc.transpose(mask, axes);\n        }\n        if (goBackwards) {\n            inputs = tfc.reverse(inputs, 0);\n            if (mask != null) {\n                mask = tfc.reverse(mask, 0);\n            }\n        }\n        // Porting Note: PyKeras with TensorFlow backend uses a symbolic loop\n        //   (tf.while_loop). But for the imperative deeplearn.js backend, we just\n        //   use the usual TypeScript control flow to iterate over the time steps in\n        //   the inputs.\n        // Porting Note: PyKeras patches a \"_use_learning_phase\" attribute to\n        // outputs.\n        //   This is not idiomatic in TypeScript. The info regarding whether we are\n        //   in a learning (i.e., training) phase for RNN is passed in a different\n        //   way.\n        const perStepOutputs = [];\n        let lastOutput;\n        let states = initialStates;\n        const timeSteps = inputs.shape[0];\n        const perStepInputs = tfc.unstack(inputs);\n        let perStepMasks;\n        if (mask != null) {\n            perStepMasks = tfc.unstack(mask);\n        }\n        for (let t = 0; t < timeSteps; ++t) {\n            const currentInput = perStepInputs[t];\n            const stepOutputs = tfc.tidy(() => stepFunction(currentInput, states));\n            if (mask == null) {\n                lastOutput = stepOutputs[0];\n                states = stepOutputs[1];\n            }\n            else {\n                const maskedOutputs = tfc.tidy(() => {\n                    const stepMask = perStepMasks[t];\n                    const negStepMask = tfc.sub(tfc.onesLike(stepMask), stepMask);\n                    // TODO(cais): Would tfc.where() be better for performance?\n                    const output = tfc.add(tfc.mul(stepOutputs[0], stepMask), tfc.mul(states[0], negStepMask));\n                    const newStates = states.map((state, i) => {\n                        return tfc.add(tfc.mul(stepOutputs[1][i], stepMask), tfc.mul(state, negStepMask));\n                    });\n                    return { output, newStates };\n                });\n                lastOutput = maskedOutputs.output;\n                states = maskedOutputs.newStates;\n            }\n            if (needPerStepOutputs) {\n                perStepOutputs.push(lastOutput);\n            }\n        }\n        let outputs;\n        if (needPerStepOutputs) {\n            const axis = 1;\n            outputs = tfc.stack(perStepOutputs, axis);\n        }\n        return [lastOutput, outputs, states];\n    });\n}\nexport class RNN extends Layer {\n    constructor(args) {\n        super(args);\n        let cell;\n        if (args.cell == null) {\n            throw new ValueError('cell property is missing for the constructor of RNN.');\n        }\n        else if (Array.isArray(args.cell)) {\n            cell = new StackedRNNCells({ cells: args.cell });\n        }\n        else {\n            cell = args.cell;\n        }\n        if (cell.stateSize == null) {\n            throw new ValueError('The RNN cell should have an attribute `stateSize` (tuple of ' +\n                'integers, one integer per RNN state).');\n        }\n        this.cell = cell;\n        this.returnSequences =\n            args.returnSequences == null ? false : args.returnSequences;\n        this.returnState = args.returnState == null ? false : args.returnState;\n        this.goBackwards = args.goBackwards == null ? false : args.goBackwards;\n        this._stateful = args.stateful == null ? false : args.stateful;\n        this.unroll = args.unroll == null ? false : args.unroll;\n        this.supportsMasking = true;\n        this.inputSpec = [new InputSpec({ ndim: 3 })];\n        this.stateSpec = null;\n        this.states_ = null;\n        // TODO(cais): Add constantsSpec and numConstants.\n        this.numConstants = null;\n        // TODO(cais): Look into the use of initial_state in the kwargs of the\n        //   constructor.\n        this.keptStates = [];\n    }\n    // Porting Note: This is the equivalent of `RNN.states` property getter in\n    //   PyKeras.\n    getStates() {\n        if (this.states_ == null) {\n            const numStates = Array.isArray(this.cell.stateSize) ? this.cell.stateSize.length : 1;\n            return math_utils.range(0, numStates).map(x => null);\n        }\n        else {\n            return this.states_;\n        }\n    }\n    // Porting Note: This is the equivalent of the `RNN.states` property setter in\n    //   PyKeras.\n    setStates(states) {\n        this.states_ = states;\n    }\n    computeOutputShape(inputShape) {\n        if (isArrayOfShapes(inputShape)) {\n            inputShape = inputShape[0];\n        }\n        inputShape = inputShape;\n        // TODO(cais): Remove the casting once stacked RNN cells become supported.\n        let stateSize = this.cell.stateSize;\n        if (!Array.isArray(stateSize)) {\n            stateSize = [stateSize];\n        }\n        const outputDim = stateSize[0];\n        let outputShape;\n        if (this.returnSequences) {\n            outputShape = [inputShape[0], inputShape[1], outputDim];\n        }\n        else {\n            outputShape = [inputShape[0], outputDim];\n        }\n        if (this.returnState) {\n            const stateShape = [];\n            for (const dim of stateSize) {\n                stateShape.push([inputShape[0], dim]);\n            }\n            return [outputShape].concat(stateShape);\n        }\n        else {\n            return outputShape;\n        }\n    }\n    computeMask(inputs, mask) {\n        return tfc.tidy(() => {\n            if (Array.isArray(mask)) {\n                mask = mask[0];\n            }\n            const outputMask = this.returnSequences ? mask : null;\n            if (this.returnState) {\n                const stateMask = this.states.map(s => null);\n                return [outputMask].concat(stateMask);\n            }\n            else {\n                return outputMask;\n            }\n        });\n    }\n    /**\n     * Get the current state tensors of the RNN.\n     *\n     * If the state hasn't been set, return an array of `null`s of the correct\n     * length.\n     */\n    get states() {\n        if (this.states_ == null) {\n            const numStates = Array.isArray(this.cell.stateSize) ? this.cell.stateSize.length : 1;\n            const output = [];\n            for (let i = 0; i < numStates; ++i) {\n                output.push(null);\n            }\n            return output;\n        }\n        else {\n            return this.states_;\n        }\n    }\n    set states(s) {\n        this.states_ = s;\n    }\n    build(inputShape) {\n        // Note inputShape will be an Array of Shapes of initial states and\n        // constants if these are passed in apply().\n        const constantShape = null;\n        if (this.numConstants != null) {\n            throw new NotImplementedError('Constants support is not implemented in RNN yet.');\n        }\n        if (isArrayOfShapes(inputShape)) {\n            inputShape = inputShape[0];\n        }\n        inputShape = inputShape;\n        const batchSize = this.stateful ? inputShape[0] : null;\n        const inputDim = inputShape.slice(2);\n        this.inputSpec[0] = new InputSpec({ shape: [batchSize, null, ...inputDim] });\n        // Allow cell (if RNNCell Layer) to build before we set or validate\n        // stateSpec.\n        const stepInputShape = [inputShape[0]].concat(inputShape.slice(2));\n        if (constantShape != null) {\n            throw new NotImplementedError('Constants support is not implemented in RNN yet.');\n        }\n        else {\n            this.cell.build(stepInputShape);\n        }\n        // Set or validate stateSpec.\n        let stateSize;\n        if (Array.isArray(this.cell.stateSize)) {\n            stateSize = this.cell.stateSize;\n        }\n        else {\n            stateSize = [this.cell.stateSize];\n        }\n        if (this.stateSpec != null) {\n            if (!util.arraysEqual(this.stateSpec.map(spec => spec.shape[spec.shape.length - 1]), stateSize)) {\n                throw new ValueError(`An initialState was passed that is not compatible with ` +\n                    `cell.stateSize. Received stateSpec=${this.stateSpec}; ` +\n                    `However cell.stateSize is ${this.cell.stateSize}`);\n            }\n        }\n        else {\n            this.stateSpec =\n                stateSize.map(dim => new InputSpec({ shape: [null, dim] }));\n        }\n        if (this.stateful) {\n            this.resetStates();\n        }\n    }\n    /**\n     * Reset the state tensors of the RNN.\n     *\n     * If the `states` argument is `undefined` or `null`, will set the\n     * state tensor(s) of the RNN to all-zero tensors of the appropriate\n     * shape(s).\n     *\n     * If `states` is provided, will set the state tensors of the RNN to its\n     * value.\n     *\n     * @param states Optional externally-provided initial states.\n     * @param training Whether this call is done during training. For stateful\n     *   RNNs, this affects whether the old states are kept or discarded. In\n     *   particular, if `training` is `true`, the old states will be kept so\n     *   that subsequent backpropgataion through time (BPTT) may work properly.\n     *   Else, the old states will be discarded.\n     */\n    resetStates(states, training = false) {\n        tidy(() => {\n            if (!this.stateful) {\n                throw new AttributeError('Cannot call resetStates() on an RNN Layer that is not stateful.');\n            }\n            const batchSize = this.inputSpec[0].shape[0];\n            if (batchSize == null) {\n                throw new ValueError('If an RNN is stateful, it needs to know its batch size. Specify ' +\n                    'the batch size of your input tensors: \\n' +\n                    '- If using a Sequential model, specify the batch size by ' +\n                    'passing a `batchInputShape` option to your first layer.\\n' +\n                    '- If using the functional API, specify the batch size by ' +\n                    'passing a `batchShape` option to your Input layer.');\n            }\n            // Initialize state if null.\n            if (this.states_ == null) {\n                if (Array.isArray(this.cell.stateSize)) {\n                    this.states_ =\n                        this.cell.stateSize.map(dim => tfc.zeros([batchSize, dim]));\n                }\n                else {\n                    this.states_ = [tfc.zeros([batchSize, this.cell.stateSize])];\n                }\n            }\n            else if (states == null) {\n                // Dispose old state tensors.\n                tfc.dispose(this.states_);\n                // For stateful RNNs, fully dispose kept old states.\n                if (this.keptStates != null) {\n                    tfc.dispose(this.keptStates);\n                    this.keptStates = [];\n                }\n                if (Array.isArray(this.cell.stateSize)) {\n                    this.states_ =\n                        this.cell.stateSize.map(dim => tfc.zeros([batchSize, dim]));\n                }\n                else {\n                    this.states_[0] = tfc.zeros([batchSize, this.cell.stateSize]);\n                }\n            }\n            else {\n                if (!Array.isArray(states)) {\n                    states = [states];\n                }\n                if (states.length !== this.states_.length) {\n                    throw new ValueError(`Layer ${this.name} expects ${this.states_.length} state(s), ` +\n                        `but it received ${states.length} state value(s). Input ` +\n                        `received: ${states}`);\n                }\n                if (training === true) {\n                    // Store old state tensors for complete disposal later, i.e., during\n                    // the next no-arg call to this method. We do not dispose the old\n                    // states immediately because that BPTT (among other things) require\n                    // them.\n                    this.keptStates.push(this.states_.slice());\n                }\n                else {\n                    tfc.dispose(this.states_);\n                }\n                for (let index = 0; index < this.states_.length; ++index) {\n                    const value = states[index];\n                    const dim = Array.isArray(this.cell.stateSize) ?\n                        this.cell.stateSize[index] :\n                        this.cell.stateSize;\n                    const expectedShape = [batchSize, dim];\n                    if (!util.arraysEqual(value.shape, expectedShape)) {\n                        throw new ValueError(`State ${index} is incompatible with layer ${this.name}: ` +\n                            `expected shape=${expectedShape}, received shape=${value.shape}`);\n                    }\n                    this.states_[index] = value;\n                }\n            }\n            this.states_ = this.states_.map(state => tfc.keep(state.clone()));\n        });\n    }\n    apply(inputs, kwargs) {\n        // TODO(cais): Figure out whether initialState is in kwargs or inputs.\n        let initialState = kwargs == null ? null : kwargs['initialState'];\n        let constants = kwargs == null ? null : kwargs['constants'];\n        if (kwargs == null) {\n            kwargs = {};\n        }\n        const standardized = standardizeArgs(inputs, initialState, constants, this.numConstants);\n        inputs = standardized.inputs;\n        initialState = standardized.initialState;\n        constants = standardized.constants;\n        // If any of `initial_state` or `constants` are specified and are\n        // `tf.SymbolicTensor`s, then add them to the inputs and temporarily modify\n        // the input_spec to include them.\n        let additionalInputs = [];\n        let additionalSpecs = [];\n        if (initialState != null) {\n            kwargs['initialState'] = initialState;\n            additionalInputs = additionalInputs.concat(initialState);\n            this.stateSpec = [];\n            for (const state of initialState) {\n                this.stateSpec.push(new InputSpec({ shape: state.shape }));\n            }\n            // TODO(cais): Use the following instead.\n            // this.stateSpec = initialState.map(state => new InputSpec({shape:\n            // state.shape}));\n            additionalSpecs = additionalSpecs.concat(this.stateSpec);\n        }\n        if (constants != null) {\n            kwargs['constants'] = constants;\n            additionalInputs = additionalInputs.concat(constants);\n            // TODO(cais): Add this.constantsSpec.\n            this.numConstants = constants.length;\n        }\n        const isTensor = additionalInputs[0] instanceof SymbolicTensor;\n        if (isTensor) {\n            // Compute full input spec, including state and constants.\n            const fullInput = [inputs].concat(additionalInputs);\n            const fullInputSpec = this.inputSpec.concat(additionalSpecs);\n            // Perform the call with temporarily replaced inputSpec.\n            const originalInputSpec = this.inputSpec;\n            this.inputSpec = fullInputSpec;\n            const output = super.apply(fullInput, kwargs);\n            this.inputSpec = originalInputSpec;\n            return output;\n        }\n        else {\n            return super.apply(inputs, kwargs);\n        }\n    }\n    // tslint:disable-next-line:no-any\n    call(inputs, kwargs) {\n        // Input shape: `[samples, time (padded with zeros), input_dim]`.\n        // Note that the .build() method of subclasses **must** define\n        // this.inputSpec and this.stateSpec owith complete input shapes.\n        return tidy(() => {\n            const mask = kwargs == null ? null : kwargs['mask'];\n            const training = kwargs == null ? null : kwargs['training'];\n            let initialState = kwargs == null ? null : kwargs['initialState'];\n            inputs = getExactlyOneTensor(inputs);\n            if (initialState == null) {\n                if (this.stateful) {\n                    initialState = this.states_;\n                }\n                else {\n                    initialState = this.getInitialState(inputs);\n                }\n            }\n            const numStates = Array.isArray(this.cell.stateSize) ? this.cell.stateSize.length : 1;\n            if (initialState.length !== numStates) {\n                throw new ValueError(`RNN Layer has ${numStates} state(s) but was passed ` +\n                    `${initialState.length} initial state(s).`);\n            }\n            if (this.unroll) {\n                console.warn('Ignoring unroll = true for RNN layer, due to imperative backend.');\n            }\n            const cellCallKwargs = { training };\n            // TODO(cais): Add support for constants.\n            const step = (inputs, states) => {\n                // `inputs` and `states` are concatenated to form a single `Array` of\n                // `tf.Tensor`s as the input to `cell.call()`.\n                const outputs = this.cell.call([inputs].concat(states), cellCallKwargs);\n                // Marshall the return value into output and new states.\n                return [outputs[0], outputs.slice(1)];\n            };\n            // TODO(cais): Add support for constants.\n            const rnnOutputs = rnn(step, inputs, initialState, this.goBackwards, mask, null, this.unroll, this.returnSequences);\n            const lastOutput = rnnOutputs[0];\n            const outputs = rnnOutputs[1];\n            const states = rnnOutputs[2];\n            if (this.stateful) {\n                this.resetStates(states, training);\n            }\n            const output = this.returnSequences ? outputs : lastOutput;\n            // TODO(cais): Porperty set learning phase flag.\n            if (this.returnState) {\n                return [output].concat(states);\n            }\n            else {\n                return output;\n            }\n        });\n    }\n    getInitialState(inputs) {\n        return tidy(() => {\n            // Build an all-zero tensor of shape [samples, outputDim].\n            // [Samples, timeSteps, inputDim].\n            let initialState = tfc.zeros(inputs.shape);\n            // [Samples].\n            initialState = tfc.sum(initialState, [1, 2]);\n            initialState = K.expandDims(initialState); // [Samples, 1].\n            if (Array.isArray(this.cell.stateSize)) {\n                return this.cell.stateSize.map(dim => dim > 1 ? K.tile(initialState, [1, dim]) : initialState);\n            }\n            else {\n                return this.cell.stateSize > 1 ?\n                    [K.tile(initialState, [1, this.cell.stateSize])] :\n                    [initialState];\n            }\n        });\n    }\n    get trainableWeights() {\n        if (!this.trainable) {\n            return [];\n        }\n        // Porting Note: In TypeScript, `this` is always an instance of `Layer`.\n        return this.cell.trainableWeights;\n    }\n    get nonTrainableWeights() {\n        // Porting Note: In TypeScript, `this` is always an instance of `Layer`.\n        if (!this.trainable) {\n            return this.cell.weights;\n        }\n        return this.cell.nonTrainableWeights;\n    }\n    setFastWeightInitDuringBuild(value) {\n        super.setFastWeightInitDuringBuild(value);\n        if (this.cell != null) {\n            this.cell.setFastWeightInitDuringBuild(value);\n        }\n    }\n    getConfig() {\n        const baseConfig = super.getConfig();\n        const config = {\n            returnSequences: this.returnSequences,\n            returnState: this.returnState,\n            goBackwards: this.goBackwards,\n            stateful: this.stateful,\n            unroll: this.unroll,\n        };\n        if (this.numConstants != null) {\n            config['numConstants'] = this.numConstants;\n        }\n        const cellConfig = this.cell.getConfig();\n        if (this.getClassName() === RNN.className) {\n            config['cell'] = {\n                'className': this.cell.getClassName(),\n                'config': cellConfig,\n            };\n        }\n        // this order is necessary, to prevent cell name from replacing layer name\n        return Object.assign({}, cellConfig, baseConfig, config);\n    }\n    /** @nocollapse */\n    static fromConfig(cls, config, customObjects = {}) {\n        const cellConfig = config['cell'];\n        const cell = deserialize(cellConfig, customObjects);\n        return new cls(Object.assign(config, { cell }));\n    }\n}\n/** @nocollapse */\nRNN.className = 'RNN';\nserialization.registerClass(RNN);\n// Porting Note: This is a common parent class for RNN cells. There is no\n// equivalent of this in PyKeras. Having a common parent class forgoes the\n//  need for `has_attr(cell, ...)` checks or its TypeScript equivalent.\n/**\n * An RNNCell layer.\n *\n * @doc {heading: 'Layers', subheading: 'Classes'}\n */\nexport class RNNCell extends Layer {\n}\nexport class SimpleRNNCell extends RNNCell {\n    constructor(args) {\n        super(args);\n        this.DEFAULT_ACTIVATION = 'tanh';\n        this.DEFAULT_KERNEL_INITIALIZER = 'glorotNormal';\n        this.DEFAULT_RECURRENT_INITIALIZER = 'orthogonal';\n        this.DEFAULT_BIAS_INITIALIZER = 'zeros';\n        this.units = args.units;\n        assertPositiveInteger(this.units, `units`);\n        this.activation = getActivation(args.activation == null ? this.DEFAULT_ACTIVATION : args.activation);\n        this.useBias = args.useBias == null ? true : args.useBias;\n        this.kernelInitializer = getInitializer(args.kernelInitializer || this.DEFAULT_KERNEL_INITIALIZER);\n        this.recurrentInitializer = getInitializer(args.recurrentInitializer || this.DEFAULT_RECURRENT_INITIALIZER);\n        this.biasInitializer =\n            getInitializer(args.biasInitializer || this.DEFAULT_BIAS_INITIALIZER);\n        this.kernelRegularizer = getRegularizer(args.kernelRegularizer);\n        this.recurrentRegularizer = getRegularizer(args.recurrentRegularizer);\n        this.biasRegularizer = getRegularizer(args.biasRegularizer);\n        this.kernelConstraint = getConstraint(args.kernelConstraint);\n        this.recurrentConstraint = getConstraint(args.recurrentConstraint);\n        this.biasConstraint = getConstraint(args.biasConstraint);\n        this.dropout = math_utils.min([1, math_utils.max([0, args.dropout == null ? 0 : args.dropout])]);\n        this.recurrentDropout = math_utils.min([\n            1,\n            math_utils.max([0, args.recurrentDropout == null ? 0 : args.recurrentDropout])\n        ]);\n        this.dropoutFunc = args.dropoutFunc;\n        this.stateSize = this.units;\n        this.dropoutMask = null;\n        this.recurrentDropoutMask = null;\n    }\n    build(inputShape) {\n        inputShape = getExactlyOneShape(inputShape);\n        // TODO(cais): Use regularizer.\n        this.kernel = this.addWeight('kernel', [inputShape[inputShape.length - 1], this.units], null, this.kernelInitializer, this.kernelRegularizer, true, this.kernelConstraint);\n        this.recurrentKernel = this.addWeight('recurrent_kernel', [this.units, this.units], null, this.recurrentInitializer, this.recurrentRegularizer, true, this.recurrentConstraint);\n        if (this.useBias) {\n            this.bias = this.addWeight('bias', [this.units], null, this.biasInitializer, this.biasRegularizer, true, this.biasConstraint);\n        }\n        else {\n            this.bias = null;\n        }\n        this.built = true;\n    }\n    // Porting Note: PyKeras' equivalent of this method takes two tensor inputs:\n    //   `inputs` and `states`. Here, the two tensors are combined into an\n    //   `Tensor[]` Array as the first input argument.\n    //   Similarly, PyKeras' equivalent of this method returns two values:\n    //    `output` and `[output]`. Here the two are combined into one length-2\n    //    `Tensor[]`, consisting of `output` repeated.\n    call(inputs, kwargs) {\n        return tidy(() => {\n            inputs = inputs;\n            if (inputs.length !== 2) {\n                throw new ValueError(`SimpleRNNCell expects 2 input Tensors, got ${inputs.length}.`);\n            }\n            let prevOutput = inputs[1];\n            inputs = inputs[0];\n            const training = kwargs['training'] == null ? false : kwargs['training'];\n            if (0 < this.dropout && this.dropout < 1 && this.dropoutMask == null) {\n                this.dropoutMask = generateDropoutMask({\n                    ones: () => tfc.onesLike(inputs),\n                    rate: this.dropout,\n                    training,\n                    dropoutFunc: this.dropoutFunc,\n                });\n            }\n            if (0 < this.recurrentDropout && this.recurrentDropout < 1 &&\n                this.recurrentDropoutMask == null) {\n                this.recurrentDropoutMask = generateDropoutMask({\n                    ones: () => tfc.onesLike(prevOutput),\n                    rate: this.recurrentDropout,\n                    training,\n                    dropoutFunc: this.dropoutFunc,\n                });\n            }\n            let h;\n            const dpMask = this.dropoutMask;\n            const recDpMask = this.recurrentDropoutMask;\n            if (dpMask != null) {\n                h = K.dot(tfc.mul(inputs, dpMask), this.kernel.read());\n            }\n            else {\n                h = K.dot(inputs, this.kernel.read());\n            }\n            if (this.bias != null) {\n                h = K.biasAdd(h, this.bias.read());\n            }\n            if (recDpMask != null) {\n                prevOutput = tfc.mul(prevOutput, recDpMask);\n            }\n            let output = tfc.add(h, K.dot(prevOutput, this.recurrentKernel.read()));\n            if (this.activation != null) {\n                output = this.activation.apply(output);\n            }\n            // TODO(cais): Properly set learning phase on output tensor?\n            return [output, output];\n        });\n    }\n    getConfig() {\n        const baseConfig = super.getConfig();\n        const config = {\n            units: this.units,\n            activation: serializeActivation(this.activation),\n            useBias: this.useBias,\n            kernelInitializer: serializeInitializer(this.kernelInitializer),\n            recurrentInitializer: serializeInitializer(this.recurrentInitializer),\n            biasInitializer: serializeInitializer(this.biasInitializer),\n            kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n            recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer),\n            biasRegularizer: serializeRegularizer(this.biasRegularizer),\n            activityRegularizer: serializeRegularizer(this.activityRegularizer),\n            kernelConstraint: serializeConstraint(this.kernelConstraint),\n            recurrentConstraint: serializeConstraint(this.recurrentConstraint),\n            biasConstraint: serializeConstraint(this.biasConstraint),\n            dropout: this.dropout,\n            recurrentDropout: this.recurrentDropout,\n        };\n        return Object.assign({}, baseConfig, config);\n    }\n}\n/** @nocollapse */\nSimpleRNNCell.className = 'SimpleRNNCell';\nserialization.registerClass(SimpleRNNCell);\nexport class SimpleRNN extends RNN {\n    constructor(args) {\n        args.cell = new SimpleRNNCell(args);\n        super(args);\n        // TODO(cais): Add activityRegularizer.\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            if (this.cell.dropoutMask != null) {\n                tfc.dispose(this.cell.dropoutMask);\n                this.cell.dropoutMask = null;\n            }\n            if (this.cell.recurrentDropoutMask != null) {\n                tfc.dispose(this.cell.recurrentDropoutMask);\n                this.cell.recurrentDropoutMask = null;\n            }\n            const mask = kwargs == null ? null : kwargs['mask'];\n            const training = kwargs == null ? null : kwargs['training'];\n            const initialState = kwargs == null ? null : kwargs['initialState'];\n            return super.call(inputs, { mask, training, initialState });\n        });\n    }\n    /** @nocollapse */\n    static fromConfig(cls, config) {\n        return new cls(config);\n    }\n}\n/** @nocollapse */\nSimpleRNN.className = 'SimpleRNN';\nserialization.registerClass(SimpleRNN);\nexport class GRUCell extends RNNCell {\n    constructor(args) {\n        super(args);\n        this.DEFAULT_ACTIVATION = 'tanh';\n        this.DEFAULT_RECURRENT_ACTIVATION = 'hardSigmoid';\n        this.DEFAULT_KERNEL_INITIALIZER = 'glorotNormal';\n        this.DEFAULT_RECURRENT_INITIALIZER = 'orthogonal';\n        this.DEFAULT_BIAS_INITIALIZER = 'zeros';\n        if (args.resetAfter) {\n            throw new ValueError(`GRUCell does not support reset_after parameter set to true.`);\n        }\n        this.units = args.units;\n        assertPositiveInteger(this.units, 'units');\n        this.activation = getActivation(args.activation === undefined ? this.DEFAULT_ACTIVATION :\n            args.activation);\n        this.recurrentActivation = getActivation(args.recurrentActivation === undefined ?\n            this.DEFAULT_RECURRENT_ACTIVATION :\n            args.recurrentActivation);\n        this.useBias = args.useBias == null ? true : args.useBias;\n        this.kernelInitializer = getInitializer(args.kernelInitializer || this.DEFAULT_KERNEL_INITIALIZER);\n        this.recurrentInitializer = getInitializer(args.recurrentInitializer || this.DEFAULT_RECURRENT_INITIALIZER);\n        this.biasInitializer =\n            getInitializer(args.biasInitializer || this.DEFAULT_BIAS_INITIALIZER);\n        this.kernelRegularizer = getRegularizer(args.kernelRegularizer);\n        this.recurrentRegularizer = getRegularizer(args.recurrentRegularizer);\n        this.biasRegularizer = getRegularizer(args.biasRegularizer);\n        this.kernelConstraint = getConstraint(args.kernelConstraint);\n        this.recurrentConstraint = getConstraint(args.recurrentConstraint);\n        this.biasConstraint = getConstraint(args.biasConstraint);\n        this.dropout = math_utils.min([1, math_utils.max([0, args.dropout == null ? 0 : args.dropout])]);\n        this.recurrentDropout = math_utils.min([\n            1,\n            math_utils.max([0, args.recurrentDropout == null ? 0 : args.recurrentDropout])\n        ]);\n        this.dropoutFunc = args.dropoutFunc;\n        this.implementation = args.implementation;\n        this.stateSize = this.units;\n        this.dropoutMask = null;\n        this.recurrentDropoutMask = null;\n    }\n    build(inputShape) {\n        inputShape = getExactlyOneShape(inputShape);\n        const inputDim = inputShape[inputShape.length - 1];\n        this.kernel = this.addWeight('kernel', [inputDim, this.units * 3], null, this.kernelInitializer, this.kernelRegularizer, true, this.kernelConstraint);\n        this.recurrentKernel = this.addWeight('recurrent_kernel', [this.units, this.units * 3], null, this.recurrentInitializer, this.recurrentRegularizer, true, this.recurrentConstraint);\n        if (this.useBias) {\n            this.bias = this.addWeight('bias', [this.units * 3], null, this.biasInitializer, this.biasRegularizer, true, this.biasConstraint);\n        }\n        else {\n            this.bias = null;\n        }\n        // Porting Notes: Unlike the PyKeras implementation, we perform slicing\n        //   of the weights and bias in the call() method, at execution time.\n        this.built = true;\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            inputs = inputs;\n            if (inputs.length !== 2) {\n                throw new ValueError(`GRUCell expects 2 input Tensors (inputs, h, c), got ` +\n                    `${inputs.length}.`);\n            }\n            const training = kwargs['training'] == null ? false : kwargs['training'];\n            let hTMinus1 = inputs[1]; // Previous memory state.\n            inputs = inputs[0];\n            // Note: For superior performance, TensorFlow.js always uses\n            // implementation 2, regardless of the actual value of\n            // config.implementation.\n            if (0 < this.dropout && this.dropout < 1 && this.dropoutMask == null) {\n                this.dropoutMask = generateDropoutMask({\n                    ones: () => tfc.onesLike(inputs),\n                    rate: this.dropout,\n                    training,\n                    count: 3,\n                    dropoutFunc: this.dropoutFunc,\n                });\n            }\n            if (0 < this.recurrentDropout && this.recurrentDropout < 1 &&\n                this.recurrentDropoutMask == null) {\n                this.recurrentDropoutMask = generateDropoutMask({\n                    ones: () => tfc.onesLike(hTMinus1),\n                    rate: this.recurrentDropout,\n                    training,\n                    count: 3,\n                    dropoutFunc: this.dropoutFunc,\n                });\n            }\n            const dpMask = this.dropoutMask;\n            const recDpMask = this.recurrentDropoutMask;\n            let z;\n            let r;\n            let hh;\n            if (0 < this.dropout && this.dropout < 1) {\n                inputs = tfc.mul(inputs, dpMask[0]);\n            }\n            let matrixX = K.dot(inputs, this.kernel.read());\n            if (this.useBias) {\n                matrixX = K.biasAdd(matrixX, this.bias.read());\n            }\n            if (0 < this.recurrentDropout && this.recurrentDropout < 1) {\n                hTMinus1 = tfc.mul(hTMinus1, recDpMask[0]);\n            }\n            const recurrentKernelValue = this.recurrentKernel.read();\n            const [rk1, rk2] = tfc.split(recurrentKernelValue, [2 * this.units, this.units], recurrentKernelValue.rank - 1);\n            const matrixInner = K.dot(hTMinus1, rk1);\n            const [xZ, xR, xH] = tfc.split(matrixX, 3, matrixX.rank - 1);\n            const [recurrentZ, recurrentR] = tfc.split(matrixInner, 2, matrixInner.rank - 1);\n            z = this.recurrentActivation.apply(tfc.add(xZ, recurrentZ));\n            r = this.recurrentActivation.apply(tfc.add(xR, recurrentR));\n            const recurrentH = K.dot(tfc.mul(r, hTMinus1), rk2);\n            hh = this.activation.apply(tfc.add(xH, recurrentH));\n            const h = tfc.add(tfc.mul(z, hTMinus1), tfc.mul(tfc.add(1, tfc.neg(z)), hh));\n            // TODO(cais): Add use_learning_phase flag properly.\n            return [h, h];\n        });\n    }\n    getConfig() {\n        const baseConfig = super.getConfig();\n        const config = {\n            units: this.units,\n            activation: serializeActivation(this.activation),\n            recurrentActivation: serializeActivation(this.recurrentActivation),\n            useBias: this.useBias,\n            kernelInitializer: serializeInitializer(this.kernelInitializer),\n            recurrentInitializer: serializeInitializer(this.recurrentInitializer),\n            biasInitializer: serializeInitializer(this.biasInitializer),\n            kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n            recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer),\n            biasRegularizer: serializeRegularizer(this.biasRegularizer),\n            activityRegularizer: serializeRegularizer(this.activityRegularizer),\n            kernelConstraint: serializeConstraint(this.kernelConstraint),\n            recurrentConstraint: serializeConstraint(this.recurrentConstraint),\n            biasConstraint: serializeConstraint(this.biasConstraint),\n            dropout: this.dropout,\n            recurrentDropout: this.recurrentDropout,\n            implementation: this.implementation,\n            resetAfter: false\n        };\n        return Object.assign({}, baseConfig, config);\n    }\n}\n/** @nocollapse */\nGRUCell.className = 'GRUCell';\nserialization.registerClass(GRUCell);\nexport class GRU extends RNN {\n    constructor(args) {\n        if (args.implementation === 0) {\n            console.warn('`implementation=0` has been deprecated, and now defaults to ' +\n                '`implementation=1`. Please update your layer call.');\n        }\n        args.cell = new GRUCell(args);\n        super(args);\n        // TODO(cais): Add activityRegularizer.\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            if (this.cell.dropoutMask != null) {\n                tfc.dispose(this.cell.dropoutMask);\n                this.cell.dropoutMask = null;\n            }\n            if (this.cell.recurrentDropoutMask != null) {\n                tfc.dispose(this.cell.recurrentDropoutMask);\n                this.cell.recurrentDropoutMask = null;\n            }\n            const mask = kwargs == null ? null : kwargs['mask'];\n            const training = kwargs == null ? null : kwargs['training'];\n            const initialState = kwargs == null ? null : kwargs['initialState'];\n            return super.call(inputs, { mask, training, initialState });\n        });\n    }\n    /** @nocollapse */\n    static fromConfig(cls, config) {\n        if (config['implmentation'] === 0) {\n            config['implementation'] = 1;\n        }\n        return new cls(config);\n    }\n}\n/** @nocollapse */\nGRU.className = 'GRU';\nserialization.registerClass(GRU);\nexport class LSTMCell extends RNNCell {\n    constructor(args) {\n        super(args);\n        this.DEFAULT_ACTIVATION = 'tanh';\n        this.DEFAULT_RECURRENT_ACTIVATION = 'hardSigmoid';\n        this.DEFAULT_KERNEL_INITIALIZER = 'glorotNormal';\n        this.DEFAULT_RECURRENT_INITIALIZER = 'orthogonal';\n        this.DEFAULT_BIAS_INITIALIZER = 'zeros';\n        this.units = args.units;\n        assertPositiveInteger(this.units, 'units');\n        this.activation = getActivation(args.activation === undefined ? this.DEFAULT_ACTIVATION :\n            args.activation);\n        this.recurrentActivation = getActivation(args.recurrentActivation === undefined ?\n            this.DEFAULT_RECURRENT_ACTIVATION :\n            args.recurrentActivation);\n        this.useBias = args.useBias == null ? true : args.useBias;\n        this.kernelInitializer = getInitializer(args.kernelInitializer || this.DEFAULT_KERNEL_INITIALIZER);\n        this.recurrentInitializer = getInitializer(args.recurrentInitializer || this.DEFAULT_RECURRENT_INITIALIZER);\n        this.biasInitializer =\n            getInitializer(args.biasInitializer || this.DEFAULT_BIAS_INITIALIZER);\n        this.unitForgetBias = args.unitForgetBias;\n        this.kernelRegularizer = getRegularizer(args.kernelRegularizer);\n        this.recurrentRegularizer = getRegularizer(args.recurrentRegularizer);\n        this.biasRegularizer = getRegularizer(args.biasRegularizer);\n        this.kernelConstraint = getConstraint(args.kernelConstraint);\n        this.recurrentConstraint = getConstraint(args.recurrentConstraint);\n        this.biasConstraint = getConstraint(args.biasConstraint);\n        this.dropout = math_utils.min([1, math_utils.max([0, args.dropout == null ? 0 : args.dropout])]);\n        this.recurrentDropout = math_utils.min([\n            1,\n            math_utils.max([0, args.recurrentDropout == null ? 0 : args.recurrentDropout])\n        ]);\n        this.dropoutFunc = args.dropoutFunc;\n        this.implementation = args.implementation;\n        this.stateSize = [this.units, this.units];\n        this.dropoutMask = null;\n        this.recurrentDropoutMask = null;\n    }\n    build(inputShape) {\n        var _a;\n        inputShape = getExactlyOneShape(inputShape);\n        const inputDim = inputShape[inputShape.length - 1];\n        this.kernel = this.addWeight('kernel', [inputDim, this.units * 4], null, this.kernelInitializer, this.kernelRegularizer, true, this.kernelConstraint);\n        this.recurrentKernel = this.addWeight('recurrent_kernel', [this.units, this.units * 4], null, this.recurrentInitializer, this.recurrentRegularizer, true, this.recurrentConstraint);\n        let biasInitializer;\n        if (this.useBias) {\n            if (this.unitForgetBias) {\n                const capturedBiasInit = this.biasInitializer;\n                const capturedUnits = this.units;\n                biasInitializer = new (_a = class CustomInit extends Initializer {\n                        apply(shape, dtype) {\n                            // TODO(cais): More informative variable names?\n                            const bI = capturedBiasInit.apply([capturedUnits]);\n                            const bF = (new Ones()).apply([capturedUnits]);\n                            const bCAndH = capturedBiasInit.apply([capturedUnits * 2]);\n                            return K.concatAlongFirstAxis(K.concatAlongFirstAxis(bI, bF), bCAndH);\n                        }\n                    },\n                    /** @nocollapse */\n                    _a.className = 'CustomInit',\n                    _a)();\n            }\n            else {\n                biasInitializer = this.biasInitializer;\n            }\n            this.bias = this.addWeight('bias', [this.units * 4], null, biasInitializer, this.biasRegularizer, true, this.biasConstraint);\n        }\n        else {\n            this.bias = null;\n        }\n        // Porting Notes: Unlike the PyKeras implementation, we perform slicing\n        //   of the weights and bias in the call() method, at execution time.\n        this.built = true;\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            const training = kwargs['training'] == null ? false : kwargs['training'];\n            inputs = inputs;\n            if (inputs.length !== 3) {\n                throw new ValueError(`LSTMCell expects 3 input Tensors (inputs, h, c), got ` +\n                    `${inputs.length}.`);\n            }\n            let hTMinus1 = inputs[1]; // Previous memory state.\n            const cTMinus1 = inputs[2]; // Previous carry state.\n            inputs = inputs[0];\n            if (0 < this.dropout && this.dropout < 1 && this.dropoutMask == null) {\n                this.dropoutMask = generateDropoutMask({\n                    ones: () => tfc.onesLike(inputs),\n                    rate: this.dropout,\n                    training,\n                    count: 4,\n                    dropoutFunc: this.dropoutFunc\n                });\n            }\n            if (0 < this.recurrentDropout && this.recurrentDropout < 1 &&\n                this.recurrentDropoutMask == null) {\n                this.recurrentDropoutMask = generateDropoutMask({\n                    ones: () => tfc.onesLike(hTMinus1),\n                    rate: this.recurrentDropout,\n                    training,\n                    count: 4,\n                    dropoutFunc: this.dropoutFunc\n                });\n            }\n            const dpMask = this.dropoutMask;\n            const recDpMask = this.recurrentDropoutMask;\n            // Note: For superior performance, TensorFlow.js always uses\n            // implementation 2 regardless of the actual value of\n            // config.implementation.\n            let i;\n            let f;\n            let c;\n            let o;\n            if (0 < this.dropout && this.dropout < 1) {\n                inputs = tfc.mul(inputs, dpMask[0]);\n            }\n            let z = K.dot(inputs, this.kernel.read());\n            if (0 < this.recurrentDropout && this.recurrentDropout < 1) {\n                hTMinus1 = tfc.mul(hTMinus1, recDpMask[0]);\n            }\n            z = tfc.add(z, K.dot(hTMinus1, this.recurrentKernel.read()));\n            if (this.useBias) {\n                z = K.biasAdd(z, this.bias.read());\n            }\n            const [z0, z1, z2, z3] = tfc.split(z, 4, z.rank - 1);\n            i = this.recurrentActivation.apply(z0);\n            f = this.recurrentActivation.apply(z1);\n            c = tfc.add(tfc.mul(f, cTMinus1), tfc.mul(i, this.activation.apply(z2)));\n            o = this.recurrentActivation.apply(z3);\n            const h = tfc.mul(o, this.activation.apply(c));\n            // TODO(cais): Add use_learning_phase flag properly.\n            return [h, h, c];\n        });\n    }\n    getConfig() {\n        const baseConfig = super.getConfig();\n        const config = {\n            units: this.units,\n            activation: serializeActivation(this.activation),\n            recurrentActivation: serializeActivation(this.recurrentActivation),\n            useBias: this.useBias,\n            kernelInitializer: serializeInitializer(this.kernelInitializer),\n            recurrentInitializer: serializeInitializer(this.recurrentInitializer),\n            biasInitializer: serializeInitializer(this.biasInitializer),\n            unitForgetBias: this.unitForgetBias,\n            kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n            recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer),\n            biasRegularizer: serializeRegularizer(this.biasRegularizer),\n            activityRegularizer: serializeRegularizer(this.activityRegularizer),\n            kernelConstraint: serializeConstraint(this.kernelConstraint),\n            recurrentConstraint: serializeConstraint(this.recurrentConstraint),\n            biasConstraint: serializeConstraint(this.biasConstraint),\n            dropout: this.dropout,\n            recurrentDropout: this.recurrentDropout,\n            implementation: this.implementation,\n        };\n        return Object.assign({}, baseConfig, config);\n    }\n}\n/** @nocollapse */\nLSTMCell.className = 'LSTMCell';\nserialization.registerClass(LSTMCell);\nexport class LSTM extends RNN {\n    constructor(args) {\n        if (args.implementation === 0) {\n            console.warn('`implementation=0` has been deprecated, and now defaults to ' +\n                '`implementation=1`. Please update your layer call.');\n        }\n        args.cell = new LSTMCell(args);\n        super(args);\n        // TODO(cais): Add activityRegularizer.\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            if (this.cell.dropoutMask != null) {\n                tfc.dispose(this.cell.dropoutMask);\n                this.cell.dropoutMask = null;\n            }\n            if (this.cell.recurrentDropoutMask != null) {\n                tfc.dispose(this.cell.recurrentDropoutMask);\n                this.cell.recurrentDropoutMask = null;\n            }\n            const mask = kwargs == null ? null : kwargs['mask'];\n            const training = kwargs == null ? null : kwargs['training'];\n            const initialState = kwargs == null ? null : kwargs['initialState'];\n            return super.call(inputs, { mask, training, initialState });\n        });\n    }\n    /** @nocollapse */\n    static fromConfig(cls, config) {\n        if (config['implmentation'] === 0) {\n            config['implementation'] = 1;\n        }\n        return new cls(config);\n    }\n}\n/** @nocollapse */\nLSTM.className = 'LSTM';\nserialization.registerClass(LSTM);\nexport class StackedRNNCells extends RNNCell {\n    constructor(args) {\n        super(args);\n        this.cells = args.cells;\n    }\n    get stateSize() {\n        // States are a flat list in reverse order of the cell stack.\n        // This allows perserving the requirement `stack.statesize[0] ===\n        // outputDim`. E.g., states of a 2-layer LSTM would be `[h2, c2, h1, c1]`,\n        // assuming one LSTM has states `[h, c]`.\n        const stateSize = [];\n        for (const cell of this.cells.slice().reverse()) {\n            if (Array.isArray(cell.stateSize)) {\n                stateSize.push(...cell.stateSize);\n            }\n            else {\n                stateSize.push(cell.stateSize);\n            }\n        }\n        return stateSize;\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            inputs = inputs;\n            let states = inputs.slice(1);\n            // Recover per-cell states.\n            const nestedStates = [];\n            for (const cell of this.cells.slice().reverse()) {\n                if (Array.isArray(cell.stateSize)) {\n                    nestedStates.push(states.splice(0, cell.stateSize.length));\n                }\n                else {\n                    nestedStates.push(states.splice(0, 1));\n                }\n            }\n            nestedStates.reverse();\n            // Call the cells in order and store the returned states.\n            const newNestedStates = [];\n            let callInputs;\n            for (let i = 0; i < this.cells.length; ++i) {\n                const cell = this.cells[i];\n                states = nestedStates[i];\n                // TODO(cais): Take care of constants.\n                if (i === 0) {\n                    callInputs = [inputs[0]].concat(states);\n                }\n                else {\n                    callInputs = [callInputs[0]].concat(states);\n                }\n                callInputs = cell.call(callInputs, kwargs);\n                newNestedStates.push(callInputs.slice(1));\n            }\n            // Format the new states as a flat list in reverse cell order.\n            states = [];\n            for (const cellStates of newNestedStates.slice().reverse()) {\n                states.push(...cellStates);\n            }\n            return [callInputs[0]].concat(states);\n        });\n    }\n    build(inputShape) {\n        if (isArrayOfShapes(inputShape)) {\n            // TODO(cais): Take care of input constants.\n            // const constantShape = inputShape.slice(1);\n            inputShape = inputShape[0];\n        }\n        inputShape = inputShape;\n        let outputDim;\n        this.cells.forEach((cell, i) => {\n            nameScope(`RNNCell_${i}`, () => {\n                // TODO(cais): Take care of input constants.\n                cell.build(inputShape);\n                if (Array.isArray(cell.stateSize)) {\n                    outputDim = cell.stateSize[0];\n                }\n                else {\n                    outputDim = cell.stateSize;\n                }\n                inputShape = [inputShape[0], outputDim];\n            });\n        });\n        this.built = true;\n    }\n    getConfig() {\n        const baseConfig = super.getConfig();\n        const getCellConfig = (cell) => {\n            return {\n                'className': cell.getClassName(),\n                'config': cell.getConfig(),\n            };\n        };\n        const cellConfigs = this.cells.map(getCellConfig);\n        const config = { 'cells': cellConfigs };\n        return Object.assign({}, baseConfig, config);\n    }\n    /** @nocollapse */\n    static fromConfig(cls, config, customObjects = {}) {\n        const cells = [];\n        for (const cellConfig of config['cells']) {\n            cells.push(deserialize(cellConfig, customObjects));\n        }\n        return new cls({ cells });\n    }\n    get trainableWeights() {\n        if (!this.trainable) {\n            return [];\n        }\n        const weights = [];\n        for (const cell of this.cells) {\n            weights.push(...cell.trainableWeights);\n        }\n        return weights;\n    }\n    get nonTrainableWeights() {\n        const weights = [];\n        for (const cell of this.cells) {\n            weights.push(...cell.nonTrainableWeights);\n        }\n        if (!this.trainable) {\n            const trainableWeights = [];\n            for (const cell of this.cells) {\n                trainableWeights.push(...cell.trainableWeights);\n            }\n            return trainableWeights.concat(weights);\n        }\n        return weights;\n    }\n    /**\n     * Retrieve the weights of a the model.\n     *\n     * @returns A flat `Array` of `tf.Tensor`s.\n     */\n    getWeights() {\n        const weights = [];\n        for (const cell of this.cells) {\n            weights.push(...cell.weights);\n        }\n        return batchGetValue(weights);\n    }\n    /**\n     * Set the weights of the model.\n     *\n     * @param weights An `Array` of `tf.Tensor`s with shapes and types matching\n     *     the output of `getWeights()`.\n     */\n    setWeights(weights) {\n        const tuples = [];\n        for (const cell of this.cells) {\n            const numParams = cell.weights.length;\n            const inputWeights = weights.splice(numParams);\n            for (let i = 0; i < cell.weights.length; ++i) {\n                tuples.push([cell.weights[i], inputWeights[i]]);\n            }\n        }\n        batchSetValue(tuples);\n    }\n}\n/** @nocollapse */\nStackedRNNCells.className = 'StackedRNNCells';\nserialization.registerClass(StackedRNNCells);\nexport function generateDropoutMask(args) {\n    const { ones, rate, training = false, count = 1, dropoutFunc } = args;\n    const droppedInputs = () => dropoutFunc != null ? dropoutFunc(ones(), rate) : K.dropout(ones(), rate);\n    const createMask = () => K.inTrainPhase(droppedInputs, ones, training);\n    // just in case count is provided with null or undefined\n    if (!count || count <= 1) {\n        return tfc.keep(createMask().clone());\n    }\n    const masks = Array(count).fill(undefined).map(createMask);\n    return masks.map(m => tfc.keep(m.clone()));\n}\n"],"mappings":"AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AACA;AACA;AACA;AACA,OAAO,KAAKA,GAAZ,MAAqB,uBAArB;AACA,SAASC,aAAT,EAAwBC,IAAxB,EAA8BC,IAA9B,QAA0C,uBAA1C;AACA,SAASC,aAAT,EAAwBC,mBAAxB,QAAmD,gBAAnD;AACA,OAAO,KAAKC,CAAZ,MAAmB,yBAAnB;AACA,SAASC,SAAT,QAA0B,WAA1B;AACA,SAASC,aAAT,EAAwBC,mBAAxB,QAAmD,gBAAnD;AACA,SAASC,SAAT,EAAoBC,cAApB,QAA0C,oBAA1C;AACA,SAASC,KAAT,QAAsB,oBAAtB;AACA,SAASC,cAAT,EAAyBC,mBAAzB,EAA8CC,UAA9C,QAAgE,WAAhE;AACA,SAASC,cAAT,EAAyBC,WAAzB,EAAsCC,IAAtC,EAA4CC,oBAA5C,QAAwE,iBAAxE;AACA,SAASC,cAAT,EAAyBC,oBAAzB,QAAqD,iBAArD;AACA,SAASC,qBAAT,QAAsC,wBAAtC;AACA,OAAO,KAAKC,UAAZ,MAA4B,qBAA5B;AACA,SAASC,kBAAT,EAA6BC,mBAA7B,EAAkDC,eAAlD,QAAyE,sBAAzE;AACA,SAASC,aAAT,EAAwBC,aAAxB,QAA6C,cAA7C;AACA,SAASC,WAAT,QAA4B,iBAA5B;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AACA,OAAO,SAASC,eAAT,CAAyBC,MAAzB,EAAiCC,YAAjC,EAA+CC,SAA/C,EAA0DC,YAA1D,EAAwE;EAC3E,IAAIC,KAAK,CAACC,OAAN,CAAcL,MAAd,CAAJ,EAA2B;IACvB,IAAIC,YAAY,IAAI,IAAhB,IAAwBC,SAAS,IAAI,IAAzC,EAA+C;MAC3C,MAAM,IAAIlB,UAAJ,CAAe,gEACjB,oBADE,CAAN;IAEH;;IACD,IAAImB,YAAY,IAAI,IAApB,EAA0B;MACtBD,SAAS,GAAGF,MAAM,CAACM,KAAP,CAAaN,MAAM,CAACO,MAAP,GAAgBJ,YAA7B,EAA2CH,MAAM,CAACO,MAAlD,CAAZ;MACAP,MAAM,GAAGA,MAAM,CAACM,KAAP,CAAa,CAAb,EAAgBN,MAAM,CAACO,MAAP,GAAgBJ,YAAhC,CAAT;IACH;;IACD,IAAIH,MAAM,CAACO,MAAP,GAAgB,CAApB,EAAuB;MACnBN,YAAY,GAAGD,MAAM,CAACM,KAAP,CAAa,CAAb,EAAgBN,MAAM,CAACO,MAAvB,CAAf;IACH;;IACDP,MAAM,GAAGA,MAAM,CAAC,CAAD,CAAf;EACH;;EACD,SAASQ,YAAT,CAAsBC,CAAtB,EAAyB;IACrB,IAAIA,CAAC,IAAI,IAAL,IAAaL,KAAK,CAACC,OAAN,CAAcI,CAAd,CAAjB,EAAmC;MAC/B,OAAOA,CAAP;IACH,CAFD,MAGK;MACD,OAAO,CAACA,CAAD,CAAP;IACH;EACJ;;EACDR,YAAY,GAAGO,YAAY,CAACP,YAAD,CAA3B;EACAC,SAAS,GAAGM,YAAY,CAACN,SAAD,CAAxB;EACA,OAAO;IAAEF,MAAF;IAAUC,YAAV;IAAwBC;EAAxB,CAAP;AACH;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AACA,OAAO,SAASQ,GAAT,CAAaC,YAAb,EAA2BX,MAA3B,EAAmCY,aAAnC,EAAkDC,WAAW,GAAG,KAAhE,EAAuEC,IAAvE,EAA6EZ,SAA7E,EAAwFa,MAAM,GAAG,KAAjG,EAAwGC,kBAAkB,GAAG,KAA7H,EAAoI;EACvI,OAAO/C,GAAG,CAACE,IAAJ,CAAS,MAAM;IAClB,MAAM8C,IAAI,GAAGjB,MAAM,CAACkB,KAAP,CAAaX,MAA1B;;IACA,IAAIU,IAAI,GAAG,CAAX,EAAc;MACV,MAAM,IAAIjC,UAAJ,CAAgB,uCAAsCiC,IAAK,IAA3D,CAAN;IACH,CAJiB,CAKlB;IACA;;;IACA,MAAME,IAAI,GAAG,CAAC,CAAD,EAAI,CAAJ,EAAOC,MAAP,CAAc5B,UAAU,CAAC6B,KAAX,CAAiB,CAAjB,EAAoBJ,IAApB,CAAd,CAAb;IACAjB,MAAM,GAAG/B,GAAG,CAACqD,SAAJ,CAActB,MAAd,EAAsBmB,IAAtB,CAAT;;IACA,IAAIjB,SAAS,IAAI,IAAjB,EAAuB;MACnB,MAAM,IAAInB,mBAAJ,CAAwB,qEAC1B,gBADE,CAAN;IAEH,CAZiB,CAalB;;;IACA,IAAIgC,MAAJ,EAAY;MACRQ,OAAO,CAACC,IAAR,CAAa,sEACT,kCADJ;IAEH;;IACD,IAAIV,IAAI,IAAI,IAAZ,EAAkB;MACdA,IAAI,GAAG7C,GAAG,CAACwD,IAAJ,CAASxD,GAAG,CAACwD,IAAJ,CAASX,IAAT,EAAe,MAAf,CAAT,EAAiC,SAAjC,CAAP;;MACA,IAAIA,IAAI,CAACY,IAAL,KAAcT,IAAI,GAAG,CAAzB,EAA4B;QACxBH,IAAI,GAAG7C,GAAG,CAAC0D,UAAJ,CAAeb,IAAf,EAAqB,CAAC,CAAtB,CAAP;MACH;;MACDA,IAAI,GAAG7C,GAAG,CAACqD,SAAJ,CAAcR,IAAd,EAAoBK,IAApB,CAAP;IACH;;IACD,IAAIN,WAAJ,EAAiB;MACbb,MAAM,GAAG/B,GAAG,CAAC2D,OAAJ,CAAY5B,MAAZ,EAAoB,CAApB,CAAT;;MACA,IAAIc,IAAI,IAAI,IAAZ,EAAkB;QACdA,IAAI,GAAG7C,GAAG,CAAC2D,OAAJ,CAAYd,IAAZ,EAAkB,CAAlB,CAAP;MACH;IACJ,CA9BiB,CA+BlB;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;;;IACA,MAAMe,cAAc,GAAG,EAAvB;IACA,IAAIC,UAAJ;IACA,IAAIC,MAAM,GAAGnB,aAAb;IACA,MAAMoB,SAAS,GAAGhC,MAAM,CAACkB,KAAP,CAAa,CAAb,CAAlB;IACA,MAAMe,aAAa,GAAGhE,GAAG,CAACiE,OAAJ,CAAYlC,MAAZ,CAAtB;IACA,IAAImC,YAAJ;;IACA,IAAIrB,IAAI,IAAI,IAAZ,EAAkB;MACdqB,YAAY,GAAGlE,GAAG,CAACiE,OAAJ,CAAYpB,IAAZ,CAAf;IACH;;IACD,KAAK,IAAIsB,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGJ,SAApB,EAA+B,EAAEI,CAAjC,EAAoC;MAChC,MAAMC,YAAY,GAAGJ,aAAa,CAACG,CAAD,CAAlC;MACA,MAAME,WAAW,GAAGrE,GAAG,CAACE,IAAJ,CAAS,MAAMwC,YAAY,CAAC0B,YAAD,EAAeN,MAAf,CAA3B,CAApB;;MACA,IAAIjB,IAAI,IAAI,IAAZ,EAAkB;QACdgB,UAAU,GAAGQ,WAAW,CAAC,CAAD,CAAxB;QACAP,MAAM,GAAGO,WAAW,CAAC,CAAD,CAApB;MACH,CAHD,MAIK;QACD,MAAMC,aAAa,GAAGtE,GAAG,CAACE,IAAJ,CAAS,MAAM;UACjC,MAAMqE,QAAQ,GAAGL,YAAY,CAACC,CAAD,CAA7B;UACA,MAAMK,WAAW,GAAGxE,GAAG,CAACyE,GAAJ,CAAQzE,GAAG,CAAC0E,QAAJ,CAAaH,QAAb,CAAR,EAAgCA,QAAhC,CAApB,CAFiC,CAGjC;;UACA,MAAMI,MAAM,GAAG3E,GAAG,CAAC4E,GAAJ,CAAQ5E,GAAG,CAAC6E,GAAJ,CAAQR,WAAW,CAAC,CAAD,CAAnB,EAAwBE,QAAxB,CAAR,EAA2CvE,GAAG,CAAC6E,GAAJ,CAAQf,MAAM,CAAC,CAAD,CAAd,EAAmBU,WAAnB,CAA3C,CAAf;UACA,MAAMM,SAAS,GAAGhB,MAAM,CAACiB,GAAP,CAAW,CAACC,KAAD,EAAQC,CAAR,KAAc;YACvC,OAAOjF,GAAG,CAAC4E,GAAJ,CAAQ5E,GAAG,CAAC6E,GAAJ,CAAQR,WAAW,CAAC,CAAD,CAAX,CAAeY,CAAf,CAAR,EAA2BV,QAA3B,CAAR,EAA8CvE,GAAG,CAAC6E,GAAJ,CAAQG,KAAR,EAAeR,WAAf,CAA9C,CAAP;UACH,CAFiB,CAAlB;UAGA,OAAO;YAAEG,MAAF;YAAUG;UAAV,CAAP;QACH,CATqB,CAAtB;QAUAjB,UAAU,GAAGS,aAAa,CAACK,MAA3B;QACAb,MAAM,GAAGQ,aAAa,CAACQ,SAAvB;MACH;;MACD,IAAI/B,kBAAJ,EAAwB;QACpBa,cAAc,CAACsB,IAAf,CAAoBrB,UAApB;MACH;IACJ;;IACD,IAAIsB,OAAJ;;IACA,IAAIpC,kBAAJ,EAAwB;MACpB,MAAMqC,IAAI,GAAG,CAAb;MACAD,OAAO,GAAGnF,GAAG,CAACqF,KAAJ,CAAUzB,cAAV,EAA0BwB,IAA1B,CAAV;IACH;;IACD,OAAO,CAACvB,UAAD,EAAasB,OAAb,EAAsBrB,MAAtB,CAAP;EACH,CAhFM,CAAP;AAiFH;AACD,OAAO,MAAMwB,GAAN,SAAkB1E,KAAlB,CAAwB;EAC3B2E,WAAW,CAACC,IAAD,EAAO;IACd,MAAMA,IAAN;IACA,IAAIC,IAAJ;;IACA,IAAID,IAAI,CAACC,IAAL,IAAa,IAAjB,EAAuB;MACnB,MAAM,IAAI1E,UAAJ,CAAe,sDAAf,CAAN;IACH,CAFD,MAGK,IAAIoB,KAAK,CAACC,OAAN,CAAcoD,IAAI,CAACC,IAAnB,CAAJ,EAA8B;MAC/BA,IAAI,GAAG,IAAIC,eAAJ,CAAoB;QAAEC,KAAK,EAAEH,IAAI,CAACC;MAAd,CAApB,CAAP;IACH,CAFI,MAGA;MACDA,IAAI,GAAGD,IAAI,CAACC,IAAZ;IACH;;IACD,IAAIA,IAAI,CAACG,SAAL,IAAkB,IAAtB,EAA4B;MACxB,MAAM,IAAI7E,UAAJ,CAAe,iEACjB,uCADE,CAAN;IAEH;;IACD,KAAK0E,IAAL,GAAYA,IAAZ;IACA,KAAKI,eAAL,GACIL,IAAI,CAACK,eAAL,IAAwB,IAAxB,GAA+B,KAA/B,GAAuCL,IAAI,CAACK,eADhD;IAEA,KAAKC,WAAL,GAAmBN,IAAI,CAACM,WAAL,IAAoB,IAApB,GAA2B,KAA3B,GAAmCN,IAAI,CAACM,WAA3D;IACA,KAAKlD,WAAL,GAAmB4C,IAAI,CAAC5C,WAAL,IAAoB,IAApB,GAA2B,KAA3B,GAAmC4C,IAAI,CAAC5C,WAA3D;IACA,KAAKmD,SAAL,GAAiBP,IAAI,CAACQ,QAAL,IAAiB,IAAjB,GAAwB,KAAxB,GAAgCR,IAAI,CAACQ,QAAtD;IACA,KAAKlD,MAAL,GAAc0C,IAAI,CAAC1C,MAAL,IAAe,IAAf,GAAsB,KAAtB,GAA8B0C,IAAI,CAAC1C,MAAjD;IACA,KAAKmD,eAAL,GAAuB,IAAvB;IACA,KAAKC,SAAL,GAAiB,CAAC,IAAIxF,SAAJ,CAAc;MAAEsC,IAAI,EAAE;IAAR,CAAd,CAAD,CAAjB;IACA,KAAKmD,SAAL,GAAiB,IAAjB;IACA,KAAKC,OAAL,GAAe,IAAf,CA1Bc,CA2Bd;;IACA,KAAKlE,YAAL,GAAoB,IAApB,CA5Bc,CA6Bd;IACA;;IACA,KAAKmE,UAAL,GAAkB,EAAlB;EACH,CAjC0B,CAkC3B;EACA;;;EACAC,SAAS,GAAG;IACR,IAAI,KAAKF,OAAL,IAAgB,IAApB,EAA0B;MACtB,MAAMG,SAAS,GAAGpE,KAAK,CAACC,OAAN,CAAc,KAAKqD,IAAL,CAAUG,SAAxB,IAAqC,KAAKH,IAAL,CAAUG,SAAV,CAAoBtD,MAAzD,GAAkE,CAApF;MACA,OAAOf,UAAU,CAAC6B,KAAX,CAAiB,CAAjB,EAAoBmD,SAApB,EAA+BxB,GAA/B,CAAmCvC,CAAC,IAAI,IAAxC,CAAP;IACH,CAHD,MAIK;MACD,OAAO,KAAK4D,OAAZ;IACH;EACJ,CA5C0B,CA6C3B;EACA;;;EACAI,SAAS,CAAC1C,MAAD,EAAS;IACd,KAAKsC,OAAL,GAAetC,MAAf;EACH;;EACD2C,kBAAkB,CAACC,UAAD,EAAa;IAC3B,IAAIhF,eAAe,CAACgF,UAAD,CAAnB,EAAiC;MAC7BA,UAAU,GAAGA,UAAU,CAAC,CAAD,CAAvB;IACH;;IACDA,UAAU,GAAGA,UAAb,CAJ2B,CAK3B;;IACA,IAAId,SAAS,GAAG,KAAKH,IAAL,CAAUG,SAA1B;;IACA,IAAI,CAACzD,KAAK,CAACC,OAAN,CAAcwD,SAAd,CAAL,EAA+B;MAC3BA,SAAS,GAAG,CAACA,SAAD,CAAZ;IACH;;IACD,MAAMe,SAAS,GAAGf,SAAS,CAAC,CAAD,CAA3B;IACA,IAAIgB,WAAJ;;IACA,IAAI,KAAKf,eAAT,EAA0B;MACtBe,WAAW,GAAG,CAACF,UAAU,CAAC,CAAD,CAAX,EAAgBA,UAAU,CAAC,CAAD,CAA1B,EAA+BC,SAA/B,CAAd;IACH,CAFD,MAGK;MACDC,WAAW,GAAG,CAACF,UAAU,CAAC,CAAD,CAAX,EAAgBC,SAAhB,CAAd;IACH;;IACD,IAAI,KAAKb,WAAT,EAAsB;MAClB,MAAMe,UAAU,GAAG,EAAnB;;MACA,KAAK,MAAMC,GAAX,IAAkBlB,SAAlB,EAA6B;QACzBiB,UAAU,CAAC3B,IAAX,CAAgB,CAACwB,UAAU,CAAC,CAAD,CAAX,EAAgBI,GAAhB,CAAhB;MACH;;MACD,OAAO,CAACF,WAAD,EAAczD,MAAd,CAAqB0D,UAArB,CAAP;IACH,CAND,MAOK;MACD,OAAOD,WAAP;IACH;EACJ;;EACDG,WAAW,CAAChF,MAAD,EAASc,IAAT,EAAe;IACtB,OAAO7C,GAAG,CAACE,IAAJ,CAAS,MAAM;MAClB,IAAIiC,KAAK,CAACC,OAAN,CAAcS,IAAd,CAAJ,EAAyB;QACrBA,IAAI,GAAGA,IAAI,CAAC,CAAD,CAAX;MACH;;MACD,MAAMmE,UAAU,GAAG,KAAKnB,eAAL,GAAuBhD,IAAvB,GAA8B,IAAjD;;MACA,IAAI,KAAKiD,WAAT,EAAsB;QAClB,MAAMmB,SAAS,GAAG,KAAKnD,MAAL,CAAYiB,GAAZ,CAAgBmC,CAAC,IAAI,IAArB,CAAlB;QACA,OAAO,CAACF,UAAD,EAAa7D,MAAb,CAAoB8D,SAApB,CAAP;MACH,CAHD,MAIK;QACD,OAAOD,UAAP;MACH;IACJ,CAZM,CAAP;EAaH;EACD;AACJ;AACA;AACA;AACA;AACA;;;EACc,IAANlD,MAAM,GAAG;IACT,IAAI,KAAKsC,OAAL,IAAgB,IAApB,EAA0B;MACtB,MAAMG,SAAS,GAAGpE,KAAK,CAACC,OAAN,CAAc,KAAKqD,IAAL,CAAUG,SAAxB,IAAqC,KAAKH,IAAL,CAAUG,SAAV,CAAoBtD,MAAzD,GAAkE,CAApF;MACA,MAAMqC,MAAM,GAAG,EAAf;;MACA,KAAK,IAAIM,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGsB,SAApB,EAA+B,EAAEtB,CAAjC,EAAoC;QAChCN,MAAM,CAACO,IAAP,CAAY,IAAZ;MACH;;MACD,OAAOP,MAAP;IACH,CAPD,MAQK;MACD,OAAO,KAAKyB,OAAZ;IACH;EACJ;;EACS,IAANtC,MAAM,CAACoD,CAAD,EAAI;IACV,KAAKd,OAAL,GAAec,CAAf;EACH;;EACDC,KAAK,CAACT,UAAD,EAAa;IACd;IACA;IACA,MAAMU,aAAa,GAAG,IAAtB;;IACA,IAAI,KAAKlF,YAAL,IAAqB,IAAzB,EAA+B;MAC3B,MAAM,IAAIpB,mBAAJ,CAAwB,kDAAxB,CAAN;IACH;;IACD,IAAIY,eAAe,CAACgF,UAAD,CAAnB,EAAiC;MAC7BA,UAAU,GAAGA,UAAU,CAAC,CAAD,CAAvB;IACH;;IACDA,UAAU,GAAGA,UAAb;IACA,MAAMW,SAAS,GAAG,KAAKrB,QAAL,GAAgBU,UAAU,CAAC,CAAD,CAA1B,GAAgC,IAAlD;IACA,MAAMY,QAAQ,GAAGZ,UAAU,CAACrE,KAAX,CAAiB,CAAjB,CAAjB;IACA,KAAK6D,SAAL,CAAe,CAAf,IAAoB,IAAIxF,SAAJ,CAAc;MAAEuC,KAAK,EAAE,CAACoE,SAAD,EAAY,IAAZ,EAAkB,GAAGC,QAArB;IAAT,CAAd,CAApB,CAbc,CAcd;IACA;;IACA,MAAMC,cAAc,GAAG,CAACb,UAAU,CAAC,CAAD,CAAX,EAAgBvD,MAAhB,CAAuBuD,UAAU,CAACrE,KAAX,CAAiB,CAAjB,CAAvB,CAAvB;;IACA,IAAI+E,aAAa,IAAI,IAArB,EAA2B;MACvB,MAAM,IAAItG,mBAAJ,CAAwB,kDAAxB,CAAN;IACH,CAFD,MAGK;MACD,KAAK2E,IAAL,CAAU0B,KAAV,CAAgBI,cAAhB;IACH,CAtBa,CAuBd;;;IACA,IAAI3B,SAAJ;;IACA,IAAIzD,KAAK,CAACC,OAAN,CAAc,KAAKqD,IAAL,CAAUG,SAAxB,CAAJ,EAAwC;MACpCA,SAAS,GAAG,KAAKH,IAAL,CAAUG,SAAtB;IACH,CAFD,MAGK;MACDA,SAAS,GAAG,CAAC,KAAKH,IAAL,CAAUG,SAAX,CAAZ;IACH;;IACD,IAAI,KAAKO,SAAL,IAAkB,IAAtB,EAA4B;MACxB,IAAI,CAAChG,IAAI,CAACqH,WAAL,CAAiB,KAAKrB,SAAL,CAAepB,GAAf,CAAmB0C,IAAI,IAAIA,IAAI,CAACxE,KAAL,CAAWwE,IAAI,CAACxE,KAAL,CAAWX,MAAX,GAAoB,CAA/B,CAA3B,CAAjB,EAAgFsD,SAAhF,CAAL,EAAiG;QAC7F,MAAM,IAAI7E,UAAJ,CAAgB,yDAAD,GAChB,sCAAqC,KAAKoF,SAAU,IADpC,GAEhB,6BAA4B,KAAKV,IAAL,CAAUG,SAAU,EAF/C,CAAN;MAGH;IACJ,CAND,MAOK;MACD,KAAKO,SAAL,GACIP,SAAS,CAACb,GAAV,CAAc+B,GAAG,IAAI,IAAIpG,SAAJ,CAAc;QAAEuC,KAAK,EAAE,CAAC,IAAD,EAAO6D,GAAP;MAAT,CAAd,CAArB,CADJ;IAEH;;IACD,IAAI,KAAKd,QAAT,EAAmB;MACf,KAAK0B,WAAL;IACH;EACJ;EACD;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;EACIA,WAAW,CAAC5D,MAAD,EAAS6D,QAAQ,GAAG,KAApB,EAA2B;IAClCzH,IAAI,CAAC,MAAM;MACP,IAAI,CAAC,KAAK8F,QAAV,EAAoB;QAChB,MAAM,IAAInF,cAAJ,CAAmB,iEAAnB,CAAN;MACH;;MACD,MAAMwG,SAAS,GAAG,KAAKnB,SAAL,CAAe,CAAf,EAAkBjD,KAAlB,CAAwB,CAAxB,CAAlB;;MACA,IAAIoE,SAAS,IAAI,IAAjB,EAAuB;QACnB,MAAM,IAAItG,UAAJ,CAAe,qEACjB,0CADiB,GAEjB,2DAFiB,GAGjB,2DAHiB,GAIjB,2DAJiB,GAKjB,oDALE,CAAN;MAMH,CAZM,CAaP;;;MACA,IAAI,KAAKqF,OAAL,IAAgB,IAApB,EAA0B;QACtB,IAAIjE,KAAK,CAACC,OAAN,CAAc,KAAKqD,IAAL,CAAUG,SAAxB,CAAJ,EAAwC;UACpC,KAAKQ,OAAL,GACI,KAAKX,IAAL,CAAUG,SAAV,CAAoBb,GAApB,CAAwB+B,GAAG,IAAI9G,GAAG,CAAC4H,KAAJ,CAAU,CAACP,SAAD,EAAYP,GAAZ,CAAV,CAA/B,CADJ;QAEH,CAHD,MAIK;UACD,KAAKV,OAAL,GAAe,CAACpG,GAAG,CAAC4H,KAAJ,CAAU,CAACP,SAAD,EAAY,KAAK5B,IAAL,CAAUG,SAAtB,CAAV,CAAD,CAAf;QACH;MACJ,CARD,MASK,IAAI9B,MAAM,IAAI,IAAd,EAAoB;QACrB;QACA9D,GAAG,CAAC6H,OAAJ,CAAY,KAAKzB,OAAjB,EAFqB,CAGrB;;QACA,IAAI,KAAKC,UAAL,IAAmB,IAAvB,EAA6B;UACzBrG,GAAG,CAAC6H,OAAJ,CAAY,KAAKxB,UAAjB;UACA,KAAKA,UAAL,GAAkB,EAAlB;QACH;;QACD,IAAIlE,KAAK,CAACC,OAAN,CAAc,KAAKqD,IAAL,CAAUG,SAAxB,CAAJ,EAAwC;UACpC,KAAKQ,OAAL,GACI,KAAKX,IAAL,CAAUG,SAAV,CAAoBb,GAApB,CAAwB+B,GAAG,IAAI9G,GAAG,CAAC4H,KAAJ,CAAU,CAACP,SAAD,EAAYP,GAAZ,CAAV,CAA/B,CADJ;QAEH,CAHD,MAIK;UACD,KAAKV,OAAL,CAAa,CAAb,IAAkBpG,GAAG,CAAC4H,KAAJ,CAAU,CAACP,SAAD,EAAY,KAAK5B,IAAL,CAAUG,SAAtB,CAAV,CAAlB;QACH;MACJ,CAfI,MAgBA;QACD,IAAI,CAACzD,KAAK,CAACC,OAAN,CAAc0B,MAAd,CAAL,EAA4B;UACxBA,MAAM,GAAG,CAACA,MAAD,CAAT;QACH;;QACD,IAAIA,MAAM,CAACxB,MAAP,KAAkB,KAAK8D,OAAL,CAAa9D,MAAnC,EAA2C;UACvC,MAAM,IAAIvB,UAAJ,CAAgB,SAAQ,KAAK+G,IAAK,YAAW,KAAK1B,OAAL,CAAa9D,MAAO,aAAlD,GAChB,mBAAkBwB,MAAM,CAACxB,MAAO,yBADhB,GAEhB,aAAYwB,MAAO,EAFlB,CAAN;QAGH;;QACD,IAAI6D,QAAQ,KAAK,IAAjB,EAAuB;UACnB;UACA;UACA;UACA;UACA,KAAKtB,UAAL,CAAgBnB,IAAhB,CAAqB,KAAKkB,OAAL,CAAa/D,KAAb,EAArB;QACH,CAND,MAOK;UACDrC,GAAG,CAAC6H,OAAJ,CAAY,KAAKzB,OAAjB;QACH;;QACD,KAAK,IAAI2B,KAAK,GAAG,CAAjB,EAAoBA,KAAK,GAAG,KAAK3B,OAAL,CAAa9D,MAAzC,EAAiD,EAAEyF,KAAnD,EAA0D;UACtD,MAAMC,KAAK,GAAGlE,MAAM,CAACiE,KAAD,CAApB;UACA,MAAMjB,GAAG,GAAG3E,KAAK,CAACC,OAAN,CAAc,KAAKqD,IAAL,CAAUG,SAAxB,IACR,KAAKH,IAAL,CAAUG,SAAV,CAAoBmC,KAApB,CADQ,GAER,KAAKtC,IAAL,CAAUG,SAFd;UAGA,MAAMqC,aAAa,GAAG,CAACZ,SAAD,EAAYP,GAAZ,CAAtB;;UACA,IAAI,CAAC3G,IAAI,CAACqH,WAAL,CAAiBQ,KAAK,CAAC/E,KAAvB,EAA8BgF,aAA9B,CAAL,EAAmD;YAC/C,MAAM,IAAIlH,UAAJ,CAAgB,SAAQgH,KAAM,+BAA8B,KAAKD,IAAK,IAAvD,GAChB,kBAAiBG,aAAc,oBAAmBD,KAAK,CAAC/E,KAAM,EAD7D,CAAN;UAEH;;UACD,KAAKmD,OAAL,CAAa2B,KAAb,IAAsBC,KAAtB;QACH;MACJ;;MACD,KAAK5B,OAAL,GAAe,KAAKA,OAAL,CAAarB,GAAb,CAAiBC,KAAK,IAAIhF,GAAG,CAACkI,IAAJ,CAASlD,KAAK,CAACmD,KAAN,EAAT,CAA1B,CAAf;IACH,CAxEG,CAAJ;EAyEH;;EACDC,KAAK,CAACrG,MAAD,EAASsG,MAAT,EAAiB;IAClB;IACA,IAAIrG,YAAY,GAAGqG,MAAM,IAAI,IAAV,GAAiB,IAAjB,GAAwBA,MAAM,CAAC,cAAD,CAAjD;IACA,IAAIpG,SAAS,GAAGoG,MAAM,IAAI,IAAV,GAAiB,IAAjB,GAAwBA,MAAM,CAAC,WAAD,CAA9C;;IACA,IAAIA,MAAM,IAAI,IAAd,EAAoB;MAChBA,MAAM,GAAG,EAAT;IACH;;IACD,MAAMC,YAAY,GAAGxG,eAAe,CAACC,MAAD,EAASC,YAAT,EAAuBC,SAAvB,EAAkC,KAAKC,YAAvC,CAApC;IACAH,MAAM,GAAGuG,YAAY,CAACvG,MAAtB;IACAC,YAAY,GAAGsG,YAAY,CAACtG,YAA5B;IACAC,SAAS,GAAGqG,YAAY,CAACrG,SAAzB,CAVkB,CAWlB;IACA;IACA;;IACA,IAAIsG,gBAAgB,GAAG,EAAvB;IACA,IAAIC,eAAe,GAAG,EAAtB;;IACA,IAAIxG,YAAY,IAAI,IAApB,EAA0B;MACtBqG,MAAM,CAAC,cAAD,CAAN,GAAyBrG,YAAzB;MACAuG,gBAAgB,GAAGA,gBAAgB,CAACpF,MAAjB,CAAwBnB,YAAxB,CAAnB;MACA,KAAKmE,SAAL,GAAiB,EAAjB;;MACA,KAAK,MAAMnB,KAAX,IAAoBhD,YAApB,EAAkC;QAC9B,KAAKmE,SAAL,CAAejB,IAAf,CAAoB,IAAIxE,SAAJ,CAAc;UAAEuC,KAAK,EAAE+B,KAAK,CAAC/B;QAAf,CAAd,CAApB;MACH,CANqB,CAOtB;MACA;MACA;;;MACAuF,eAAe,GAAGA,eAAe,CAACrF,MAAhB,CAAuB,KAAKgD,SAA5B,CAAlB;IACH;;IACD,IAAIlE,SAAS,IAAI,IAAjB,EAAuB;MACnBoG,MAAM,CAAC,WAAD,CAAN,GAAsBpG,SAAtB;MACAsG,gBAAgB,GAAGA,gBAAgB,CAACpF,MAAjB,CAAwBlB,SAAxB,CAAnB,CAFmB,CAGnB;;MACA,KAAKC,YAAL,GAAoBD,SAAS,CAACK,MAA9B;IACH;;IACD,MAAMmG,QAAQ,GAAGF,gBAAgB,CAAC,CAAD,CAAhB,YAA+B5H,cAAhD;;IACA,IAAI8H,QAAJ,EAAc;MACV;MACA,MAAMC,SAAS,GAAG,CAAC3G,MAAD,EAASoB,MAAT,CAAgBoF,gBAAhB,CAAlB;MACA,MAAMI,aAAa,GAAG,KAAKzC,SAAL,CAAe/C,MAAf,CAAsBqF,eAAtB,CAAtB,CAHU,CAIV;;MACA,MAAMI,iBAAiB,GAAG,KAAK1C,SAA/B;MACA,KAAKA,SAAL,GAAiByC,aAAjB;MACA,MAAMhE,MAAM,GAAG,MAAMyD,KAAN,CAAYM,SAAZ,EAAuBL,MAAvB,CAAf;MACA,KAAKnC,SAAL,GAAiB0C,iBAAjB;MACA,OAAOjE,MAAP;IACH,CAVD,MAWK;MACD,OAAO,MAAMyD,KAAN,CAAYrG,MAAZ,EAAoBsG,MAApB,CAAP;IACH;EACJ,CA/S0B,CAgT3B;;;EACAQ,IAAI,CAAC9G,MAAD,EAASsG,MAAT,EAAiB;IACjB;IACA;IACA;IACA,OAAOnI,IAAI,CAAC,MAAM;MACd,MAAM2C,IAAI,GAAGwF,MAAM,IAAI,IAAV,GAAiB,IAAjB,GAAwBA,MAAM,CAAC,MAAD,CAA3C;MACA,MAAMV,QAAQ,GAAGU,MAAM,IAAI,IAAV,GAAiB,IAAjB,GAAwBA,MAAM,CAAC,UAAD,CAA/C;MACA,IAAIrG,YAAY,GAAGqG,MAAM,IAAI,IAAV,GAAiB,IAAjB,GAAwBA,MAAM,CAAC,cAAD,CAAjD;MACAtG,MAAM,GAAGN,mBAAmB,CAACM,MAAD,CAA5B;;MACA,IAAIC,YAAY,IAAI,IAApB,EAA0B;QACtB,IAAI,KAAKgE,QAAT,EAAmB;UACfhE,YAAY,GAAG,KAAKoE,OAApB;QACH,CAFD,MAGK;UACDpE,YAAY,GAAG,KAAK8G,eAAL,CAAqB/G,MAArB,CAAf;QACH;MACJ;;MACD,MAAMwE,SAAS,GAAGpE,KAAK,CAACC,OAAN,CAAc,KAAKqD,IAAL,CAAUG,SAAxB,IAAqC,KAAKH,IAAL,CAAUG,SAAV,CAAoBtD,MAAzD,GAAkE,CAApF;;MACA,IAAIN,YAAY,CAACM,MAAb,KAAwBiE,SAA5B,EAAuC;QACnC,MAAM,IAAIxF,UAAJ,CAAgB,iBAAgBwF,SAAU,2BAA3B,GAChB,GAAEvE,YAAY,CAACM,MAAO,oBADrB,CAAN;MAEH;;MACD,IAAI,KAAKQ,MAAT,EAAiB;QACbQ,OAAO,CAACC,IAAR,CAAa,kEAAb;MACH;;MACD,MAAMwF,cAAc,GAAG;QAAEpB;MAAF,CAAvB,CArBc,CAsBd;;MACA,MAAMqB,IAAI,GAAG,CAACjH,MAAD,EAAS+B,MAAT,KAAoB;QAC7B;QACA;QACA,MAAMqB,OAAO,GAAG,KAAKM,IAAL,CAAUoD,IAAV,CAAe,CAAC9G,MAAD,EAASoB,MAAT,CAAgBW,MAAhB,CAAf,EAAwCiF,cAAxC,CAAhB,CAH6B,CAI7B;;QACA,OAAO,CAAC5D,OAAO,CAAC,CAAD,CAAR,EAAaA,OAAO,CAAC9C,KAAR,CAAc,CAAd,CAAb,CAAP;MACH,CAND,CAvBc,CA8Bd;;;MACA,MAAM4G,UAAU,GAAGxG,GAAG,CAACuG,IAAD,EAAOjH,MAAP,EAAeC,YAAf,EAA6B,KAAKY,WAAlC,EAA+CC,IAA/C,EAAqD,IAArD,EAA2D,KAAKC,MAAhE,EAAwE,KAAK+C,eAA7E,CAAtB;MACA,MAAMhC,UAAU,GAAGoF,UAAU,CAAC,CAAD,CAA7B;MACA,MAAM9D,OAAO,GAAG8D,UAAU,CAAC,CAAD,CAA1B;MACA,MAAMnF,MAAM,GAAGmF,UAAU,CAAC,CAAD,CAAzB;;MACA,IAAI,KAAKjD,QAAT,EAAmB;QACf,KAAK0B,WAAL,CAAiB5D,MAAjB,EAAyB6D,QAAzB;MACH;;MACD,MAAMhD,MAAM,GAAG,KAAKkB,eAAL,GAAuBV,OAAvB,GAAiCtB,UAAhD,CAtCc,CAuCd;;MACA,IAAI,KAAKiC,WAAT,EAAsB;QAClB,OAAO,CAACnB,MAAD,EAASxB,MAAT,CAAgBW,MAAhB,CAAP;MACH,CAFD,MAGK;QACD,OAAOa,MAAP;MACH;IACJ,CA9CU,CAAX;EA+CH;;EACDmE,eAAe,CAAC/G,MAAD,EAAS;IACpB,OAAO7B,IAAI,CAAC,MAAM;MACd;MACA;MACA,IAAI8B,YAAY,GAAGhC,GAAG,CAAC4H,KAAJ,CAAU7F,MAAM,CAACkB,KAAjB,CAAnB,CAHc,CAId;;MACAjB,YAAY,GAAGhC,GAAG,CAACkJ,GAAJ,CAAQlH,YAAR,EAAsB,CAAC,CAAD,EAAI,CAAJ,CAAtB,CAAf;MACAA,YAAY,GAAG1B,CAAC,CAACoD,UAAF,CAAa1B,YAAb,CAAf,CANc,CAM6B;;MAC3C,IAAIG,KAAK,CAACC,OAAN,CAAc,KAAKqD,IAAL,CAAUG,SAAxB,CAAJ,EAAwC;QACpC,OAAO,KAAKH,IAAL,CAAUG,SAAV,CAAoBb,GAApB,CAAwB+B,GAAG,IAAIA,GAAG,GAAG,CAAN,GAAUxG,CAAC,CAAC6I,IAAF,CAAOnH,YAAP,EAAqB,CAAC,CAAD,EAAI8E,GAAJ,CAArB,CAAV,GAA2C9E,YAA1E,CAAP;MACH,CAFD,MAGK;QACD,OAAO,KAAKyD,IAAL,CAAUG,SAAV,GAAsB,CAAtB,GACH,CAACtF,CAAC,CAAC6I,IAAF,CAAOnH,YAAP,EAAqB,CAAC,CAAD,EAAI,KAAKyD,IAAL,CAAUG,SAAd,CAArB,CAAD,CADG,GAEH,CAAC5D,YAAD,CAFJ;MAGH;IACJ,CAfU,CAAX;EAgBH;;EACmB,IAAhBoH,gBAAgB,GAAG;IACnB,IAAI,CAAC,KAAKC,SAAV,EAAqB;MACjB,OAAO,EAAP;IACH,CAHkB,CAInB;;;IACA,OAAO,KAAK5D,IAAL,CAAU2D,gBAAjB;EACH;;EACsB,IAAnBE,mBAAmB,GAAG;IACtB;IACA,IAAI,CAAC,KAAKD,SAAV,EAAqB;MACjB,OAAO,KAAK5D,IAAL,CAAU8D,OAAjB;IACH;;IACD,OAAO,KAAK9D,IAAL,CAAU6D,mBAAjB;EACH;;EACDE,4BAA4B,CAACxB,KAAD,EAAQ;IAChC,MAAMwB,4BAAN,CAAmCxB,KAAnC;;IACA,IAAI,KAAKvC,IAAL,IAAa,IAAjB,EAAuB;MACnB,KAAKA,IAAL,CAAU+D,4BAAV,CAAuCxB,KAAvC;IACH;EACJ;;EACDyB,SAAS,GAAG;IACR,MAAMC,UAAU,GAAG,MAAMD,SAAN,EAAnB;IACA,MAAME,MAAM,GAAG;MACX9D,eAAe,EAAE,KAAKA,eADX;MAEXC,WAAW,EAAE,KAAKA,WAFP;MAGXlD,WAAW,EAAE,KAAKA,WAHP;MAIXoD,QAAQ,EAAE,KAAKA,QAJJ;MAKXlD,MAAM,EAAE,KAAKA;IALF,CAAf;;IAOA,IAAI,KAAKZ,YAAL,IAAqB,IAAzB,EAA+B;MAC3ByH,MAAM,CAAC,cAAD,CAAN,GAAyB,KAAKzH,YAA9B;IACH;;IACD,MAAM0H,UAAU,GAAG,KAAKnE,IAAL,CAAUgE,SAAV,EAAnB;;IACA,IAAI,KAAKI,YAAL,OAAwBvE,GAAG,CAACwE,SAAhC,EAA2C;MACvCH,MAAM,CAAC,MAAD,CAAN,GAAiB;QACb,aAAa,KAAKlE,IAAL,CAAUoE,YAAV,EADA;QAEb,UAAUD;MAFG,CAAjB;IAIH,CAlBO,CAmBR;;;IACA,OAAOG,MAAM,CAACC,MAAP,CAAc,EAAd,EAAkBJ,UAAlB,EAA8BF,UAA9B,EAA0CC,MAA1C,CAAP;EACH;EACD;;;EACiB,OAAVM,UAAU,CAACC,GAAD,EAAMP,MAAN,EAAcQ,aAAa,GAAG,EAA9B,EAAkC;IAC/C,MAAMP,UAAU,GAAGD,MAAM,CAAC,MAAD,CAAzB;IACA,MAAMlE,IAAI,GAAG5D,WAAW,CAAC+H,UAAD,EAAaO,aAAb,CAAxB;IACA,OAAO,IAAID,GAAJ,CAAQH,MAAM,CAACC,MAAP,CAAcL,MAAd,EAAsB;MAAElE;IAAF,CAAtB,CAAR,CAAP;EACH;;AAta0B;AAwa/B;;AACAH,GAAG,CAACwE,SAAJ,GAAgB,KAAhB;AACA7J,aAAa,CAACmK,aAAd,CAA4B9E,GAA5B,E,CACA;AACA;AACA;;AACA;AACA;AACA;AACA;AACA;;AACA,OAAO,MAAM+E,OAAN,SAAsBzJ,KAAtB,CAA4B;AAEnC,OAAO,MAAM0J,aAAN,SAA4BD,OAA5B,CAAoC;EACvC9E,WAAW,CAACC,IAAD,EAAO;IACd,MAAMA,IAAN;IACA,KAAK+E,kBAAL,GAA0B,MAA1B;IACA,KAAKC,0BAAL,GAAkC,cAAlC;IACA,KAAKC,6BAAL,GAAqC,YAArC;IACA,KAAKC,wBAAL,GAAgC,OAAhC;IACA,KAAKC,KAAL,GAAanF,IAAI,CAACmF,KAAlB;IACArJ,qBAAqB,CAAC,KAAKqJ,KAAN,EAAc,OAAd,CAArB;IACA,KAAKC,UAAL,GAAkBxK,aAAa,CAACoF,IAAI,CAACoF,UAAL,IAAmB,IAAnB,GAA0B,KAAKL,kBAA/B,GAAoD/E,IAAI,CAACoF,UAA1D,CAA/B;IACA,KAAKC,OAAL,GAAerF,IAAI,CAACqF,OAAL,IAAgB,IAAhB,GAAuB,IAAvB,GAA8BrF,IAAI,CAACqF,OAAlD;IACA,KAAKC,iBAAL,GAAyB9J,cAAc,CAACwE,IAAI,CAACsF,iBAAL,IAA0B,KAAKN,0BAAhC,CAAvC;IACA,KAAKO,oBAAL,GAA4B/J,cAAc,CAACwE,IAAI,CAACuF,oBAAL,IAA6B,KAAKN,6BAAnC,CAA1C;IACA,KAAKO,eAAL,GACIhK,cAAc,CAACwE,IAAI,CAACwF,eAAL,IAAwB,KAAKN,wBAA9B,CADlB;IAEA,KAAKO,iBAAL,GAAyB7J,cAAc,CAACoE,IAAI,CAACyF,iBAAN,CAAvC;IACA,KAAKC,oBAAL,GAA4B9J,cAAc,CAACoE,IAAI,CAAC0F,oBAAN,CAA1C;IACA,KAAKC,eAAL,GAAuB/J,cAAc,CAACoE,IAAI,CAAC2F,eAAN,CAArC;IACA,KAAKC,gBAAL,GAAwB5K,aAAa,CAACgF,IAAI,CAAC4F,gBAAN,CAArC;IACA,KAAKC,mBAAL,GAA2B7K,aAAa,CAACgF,IAAI,CAAC6F,mBAAN,CAAxC;IACA,KAAKC,cAAL,GAAsB9K,aAAa,CAACgF,IAAI,CAAC8F,cAAN,CAAnC;IACA,KAAKC,OAAL,GAAehK,UAAU,CAACiK,GAAX,CAAe,CAAC,CAAD,EAAIjK,UAAU,CAACkK,GAAX,CAAe,CAAC,CAAD,EAAIjG,IAAI,CAAC+F,OAAL,IAAgB,IAAhB,GAAuB,CAAvB,GAA2B/F,IAAI,CAAC+F,OAApC,CAAf,CAAJ,CAAf,CAAf;IACA,KAAKG,gBAAL,GAAwBnK,UAAU,CAACiK,GAAX,CAAe,CACnC,CADmC,EAEnCjK,UAAU,CAACkK,GAAX,CAAe,CAAC,CAAD,EAAIjG,IAAI,CAACkG,gBAAL,IAAyB,IAAzB,GAAgC,CAAhC,GAAoClG,IAAI,CAACkG,gBAA7C,CAAf,CAFmC,CAAf,CAAxB;IAIA,KAAKC,WAAL,GAAmBnG,IAAI,CAACmG,WAAxB;IACA,KAAK/F,SAAL,GAAiB,KAAK+E,KAAtB;IACA,KAAKiB,WAAL,GAAmB,IAAnB;IACA,KAAKC,oBAAL,GAA4B,IAA5B;EACH;;EACD1E,KAAK,CAACT,UAAD,EAAa;IACdA,UAAU,GAAGlF,kBAAkB,CAACkF,UAAD,CAA/B,CADc,CAEd;;IACA,KAAKoF,MAAL,GAAc,KAAKC,SAAL,CAAe,QAAf,EAAyB,CAACrF,UAAU,CAACA,UAAU,CAACpE,MAAX,GAAoB,CAArB,CAAX,EAAoC,KAAKqI,KAAzC,CAAzB,EAA0E,IAA1E,EAAgF,KAAKG,iBAArF,EAAwG,KAAKG,iBAA7G,EAAgI,IAAhI,EAAsI,KAAKG,gBAA3I,CAAd;IACA,KAAKY,eAAL,GAAuB,KAAKD,SAAL,CAAe,kBAAf,EAAmC,CAAC,KAAKpB,KAAN,EAAa,KAAKA,KAAlB,CAAnC,EAA6D,IAA7D,EAAmE,KAAKI,oBAAxE,EAA8F,KAAKG,oBAAnG,EAAyH,IAAzH,EAA+H,KAAKG,mBAApI,CAAvB;;IACA,IAAI,KAAKR,OAAT,EAAkB;MACd,KAAKoB,IAAL,GAAY,KAAKF,SAAL,CAAe,MAAf,EAAuB,CAAC,KAAKpB,KAAN,CAAvB,EAAqC,IAArC,EAA2C,KAAKK,eAAhD,EAAiE,KAAKG,eAAtE,EAAuF,IAAvF,EAA6F,KAAKG,cAAlG,CAAZ;IACH,CAFD,MAGK;MACD,KAAKW,IAAL,GAAY,IAAZ;IACH;;IACD,KAAKC,KAAL,GAAa,IAAb;EACH,CA3CsC,CA4CvC;EACA;EACA;EACA;EACA;EACA;;;EACArD,IAAI,CAAC9G,MAAD,EAASsG,MAAT,EAAiB;IACjB,OAAOnI,IAAI,CAAC,MAAM;MACd6B,MAAM,GAAGA,MAAT;;MACA,IAAIA,MAAM,CAACO,MAAP,KAAkB,CAAtB,EAAyB;QACrB,MAAM,IAAIvB,UAAJ,CAAgB,8CAA6CgB,MAAM,CAACO,MAAO,GAA3E,CAAN;MACH;;MACD,IAAI6J,UAAU,GAAGpK,MAAM,CAAC,CAAD,CAAvB;MACAA,MAAM,GAAGA,MAAM,CAAC,CAAD,CAAf;MACA,MAAM4F,QAAQ,GAAGU,MAAM,CAAC,UAAD,CAAN,IAAsB,IAAtB,GAA6B,KAA7B,GAAqCA,MAAM,CAAC,UAAD,CAA5D;;MACA,IAAI,IAAI,KAAKkD,OAAT,IAAoB,KAAKA,OAAL,GAAe,CAAnC,IAAwC,KAAKK,WAAL,IAAoB,IAAhE,EAAsE;QAClE,KAAKA,WAAL,GAAmBQ,mBAAmB,CAAC;UACnCC,IAAI,EAAE,MAAMrM,GAAG,CAAC0E,QAAJ,CAAa3C,MAAb,CADuB;UAEnCuK,IAAI,EAAE,KAAKf,OAFwB;UAGnC5D,QAHmC;UAInCgE,WAAW,EAAE,KAAKA;QAJiB,CAAD,CAAtC;MAMH;;MACD,IAAI,IAAI,KAAKD,gBAAT,IAA6B,KAAKA,gBAAL,GAAwB,CAArD,IACA,KAAKG,oBAAL,IAA6B,IADjC,EACuC;QACnC,KAAKA,oBAAL,GAA4BO,mBAAmB,CAAC;UAC5CC,IAAI,EAAE,MAAMrM,GAAG,CAAC0E,QAAJ,CAAayH,UAAb,CADgC;UAE5CG,IAAI,EAAE,KAAKZ,gBAFiC;UAG5C/D,QAH4C;UAI5CgE,WAAW,EAAE,KAAKA;QAJ0B,CAAD,CAA/C;MAMH;;MACD,IAAIY,CAAJ;MACA,MAAMC,MAAM,GAAG,KAAKZ,WAApB;MACA,MAAMa,SAAS,GAAG,KAAKZ,oBAAvB;;MACA,IAAIW,MAAM,IAAI,IAAd,EAAoB;QAChBD,CAAC,GAAGjM,CAAC,CAACoM,GAAF,CAAM1M,GAAG,CAAC6E,GAAJ,CAAQ9C,MAAR,EAAgByK,MAAhB,CAAN,EAA+B,KAAKV,MAAL,CAAYa,IAAZ,EAA/B,CAAJ;MACH,CAFD,MAGK;QACDJ,CAAC,GAAGjM,CAAC,CAACoM,GAAF,CAAM3K,MAAN,EAAc,KAAK+J,MAAL,CAAYa,IAAZ,EAAd,CAAJ;MACH;;MACD,IAAI,KAAKV,IAAL,IAAa,IAAjB,EAAuB;QACnBM,CAAC,GAAGjM,CAAC,CAACsM,OAAF,CAAUL,CAAV,EAAa,KAAKN,IAAL,CAAUU,IAAV,EAAb,CAAJ;MACH;;MACD,IAAIF,SAAS,IAAI,IAAjB,EAAuB;QACnBN,UAAU,GAAGnM,GAAG,CAAC6E,GAAJ,CAAQsH,UAAR,EAAoBM,SAApB,CAAb;MACH;;MACD,IAAI9H,MAAM,GAAG3E,GAAG,CAAC4E,GAAJ,CAAQ2H,CAAR,EAAWjM,CAAC,CAACoM,GAAF,CAAMP,UAAN,EAAkB,KAAKH,eAAL,CAAqBW,IAArB,EAAlB,CAAX,CAAb;;MACA,IAAI,KAAK/B,UAAL,IAAmB,IAAvB,EAA6B;QACzBjG,MAAM,GAAG,KAAKiG,UAAL,CAAgBxC,KAAhB,CAAsBzD,MAAtB,CAAT;MACH,CA3Ca,CA4Cd;;;MACA,OAAO,CAACA,MAAD,EAASA,MAAT,CAAP;IACH,CA9CU,CAAX;EA+CH;;EACD8E,SAAS,GAAG;IACR,MAAMC,UAAU,GAAG,MAAMD,SAAN,EAAnB;IACA,MAAME,MAAM,GAAG;MACXgB,KAAK,EAAE,KAAKA,KADD;MAEXC,UAAU,EAAEvK,mBAAmB,CAAC,KAAKuK,UAAN,CAFpB;MAGXC,OAAO,EAAE,KAAKA,OAHH;MAIXC,iBAAiB,EAAE3J,oBAAoB,CAAC,KAAK2J,iBAAN,CAJ5B;MAKXC,oBAAoB,EAAE5J,oBAAoB,CAAC,KAAK4J,oBAAN,CAL/B;MAMXC,eAAe,EAAE7J,oBAAoB,CAAC,KAAK6J,eAAN,CAN1B;MAOXC,iBAAiB,EAAE5J,oBAAoB,CAAC,KAAK4J,iBAAN,CAP5B;MAQXC,oBAAoB,EAAE7J,oBAAoB,CAAC,KAAK6J,oBAAN,CAR/B;MASXC,eAAe,EAAE9J,oBAAoB,CAAC,KAAK8J,eAAN,CAT1B;MAUX0B,mBAAmB,EAAExL,oBAAoB,CAAC,KAAKwL,mBAAN,CAV9B;MAWXzB,gBAAgB,EAAE3K,mBAAmB,CAAC,KAAK2K,gBAAN,CAX1B;MAYXC,mBAAmB,EAAE5K,mBAAmB,CAAC,KAAK4K,mBAAN,CAZ7B;MAaXC,cAAc,EAAE7K,mBAAmB,CAAC,KAAK6K,cAAN,CAbxB;MAcXC,OAAO,EAAE,KAAKA,OAdH;MAeXG,gBAAgB,EAAE,KAAKA;IAfZ,CAAf;IAiBA,OAAO3B,MAAM,CAACC,MAAP,CAAc,EAAd,EAAkBN,UAAlB,EAA8BC,MAA9B,CAAP;EACH;;AAvHsC;AAyH3C;;AACAW,aAAa,CAACR,SAAd,GAA0B,eAA1B;AACA7J,aAAa,CAACmK,aAAd,CAA4BE,aAA5B;AACA,OAAO,MAAMwC,SAAN,SAAwBxH,GAAxB,CAA4B;EAC/BC,WAAW,CAACC,IAAD,EAAO;IACdA,IAAI,CAACC,IAAL,GAAY,IAAI6E,aAAJ,CAAkB9E,IAAlB,CAAZ;IACA,MAAMA,IAAN,EAFc,CAGd;EACH;;EACDqD,IAAI,CAAC9G,MAAD,EAASsG,MAAT,EAAiB;IACjB,OAAOnI,IAAI,CAAC,MAAM;MACd,IAAI,KAAKuF,IAAL,CAAUmG,WAAV,IAAyB,IAA7B,EAAmC;QAC/B5L,GAAG,CAAC6H,OAAJ,CAAY,KAAKpC,IAAL,CAAUmG,WAAtB;QACA,KAAKnG,IAAL,CAAUmG,WAAV,GAAwB,IAAxB;MACH;;MACD,IAAI,KAAKnG,IAAL,CAAUoG,oBAAV,IAAkC,IAAtC,EAA4C;QACxC7L,GAAG,CAAC6H,OAAJ,CAAY,KAAKpC,IAAL,CAAUoG,oBAAtB;QACA,KAAKpG,IAAL,CAAUoG,oBAAV,GAAiC,IAAjC;MACH;;MACD,MAAMhJ,IAAI,GAAGwF,MAAM,IAAI,IAAV,GAAiB,IAAjB,GAAwBA,MAAM,CAAC,MAAD,CAA3C;MACA,MAAMV,QAAQ,GAAGU,MAAM,IAAI,IAAV,GAAiB,IAAjB,GAAwBA,MAAM,CAAC,UAAD,CAA/C;MACA,MAAMrG,YAAY,GAAGqG,MAAM,IAAI,IAAV,GAAiB,IAAjB,GAAwBA,MAAM,CAAC,cAAD,CAAnD;MACA,OAAO,MAAMQ,IAAN,CAAW9G,MAAX,EAAmB;QAAEc,IAAF;QAAQ8E,QAAR;QAAkB3F;MAAlB,CAAnB,CAAP;IACH,CAbU,CAAX;EAcH;EACD;;;EACiB,OAAViI,UAAU,CAACC,GAAD,EAAMP,MAAN,EAAc;IAC3B,OAAO,IAAIO,GAAJ,CAAQP,MAAR,CAAP;EACH;;AAzB8B;AA2BnC;;AACAmD,SAAS,CAAChD,SAAV,GAAsB,WAAtB;AACA7J,aAAa,CAACmK,aAAd,CAA4B0C,SAA5B;AACA,OAAO,MAAMC,OAAN,SAAsB1C,OAAtB,CAA8B;EACjC9E,WAAW,CAACC,IAAD,EAAO;IACd,MAAMA,IAAN;IACA,KAAK+E,kBAAL,GAA0B,MAA1B;IACA,KAAKyC,4BAAL,GAAoC,aAApC;IACA,KAAKxC,0BAAL,GAAkC,cAAlC;IACA,KAAKC,6BAAL,GAAqC,YAArC;IACA,KAAKC,wBAAL,GAAgC,OAAhC;;IACA,IAAIlF,IAAI,CAACyH,UAAT,EAAqB;MACjB,MAAM,IAAIlM,UAAJ,CAAgB,6DAAhB,CAAN;IACH;;IACD,KAAK4J,KAAL,GAAanF,IAAI,CAACmF,KAAlB;IACArJ,qBAAqB,CAAC,KAAKqJ,KAAN,EAAa,OAAb,CAArB;IACA,KAAKC,UAAL,GAAkBxK,aAAa,CAACoF,IAAI,CAACoF,UAAL,KAAoBsC,SAApB,GAAgC,KAAK3C,kBAArC,GAC5B/E,IAAI,CAACoF,UADsB,CAA/B;IAEA,KAAKuC,mBAAL,GAA2B/M,aAAa,CAACoF,IAAI,CAAC2H,mBAAL,KAA6BD,SAA7B,GACrC,KAAKF,4BADgC,GAErCxH,IAAI,CAAC2H,mBAF+B,CAAxC;IAGA,KAAKtC,OAAL,GAAerF,IAAI,CAACqF,OAAL,IAAgB,IAAhB,GAAuB,IAAvB,GAA8BrF,IAAI,CAACqF,OAAlD;IACA,KAAKC,iBAAL,GAAyB9J,cAAc,CAACwE,IAAI,CAACsF,iBAAL,IAA0B,KAAKN,0BAAhC,CAAvC;IACA,KAAKO,oBAAL,GAA4B/J,cAAc,CAACwE,IAAI,CAACuF,oBAAL,IAA6B,KAAKN,6BAAnC,CAA1C;IACA,KAAKO,eAAL,GACIhK,cAAc,CAACwE,IAAI,CAACwF,eAAL,IAAwB,KAAKN,wBAA9B,CADlB;IAEA,KAAKO,iBAAL,GAAyB7J,cAAc,CAACoE,IAAI,CAACyF,iBAAN,CAAvC;IACA,KAAKC,oBAAL,GAA4B9J,cAAc,CAACoE,IAAI,CAAC0F,oBAAN,CAA1C;IACA,KAAKC,eAAL,GAAuB/J,cAAc,CAACoE,IAAI,CAAC2F,eAAN,CAArC;IACA,KAAKC,gBAAL,GAAwB5K,aAAa,CAACgF,IAAI,CAAC4F,gBAAN,CAArC;IACA,KAAKC,mBAAL,GAA2B7K,aAAa,CAACgF,IAAI,CAAC6F,mBAAN,CAAxC;IACA,KAAKC,cAAL,GAAsB9K,aAAa,CAACgF,IAAI,CAAC8F,cAAN,CAAnC;IACA,KAAKC,OAAL,GAAehK,UAAU,CAACiK,GAAX,CAAe,CAAC,CAAD,EAAIjK,UAAU,CAACkK,GAAX,CAAe,CAAC,CAAD,EAAIjG,IAAI,CAAC+F,OAAL,IAAgB,IAAhB,GAAuB,CAAvB,GAA2B/F,IAAI,CAAC+F,OAApC,CAAf,CAAJ,CAAf,CAAf;IACA,KAAKG,gBAAL,GAAwBnK,UAAU,CAACiK,GAAX,CAAe,CACnC,CADmC,EAEnCjK,UAAU,CAACkK,GAAX,CAAe,CAAC,CAAD,EAAIjG,IAAI,CAACkG,gBAAL,IAAyB,IAAzB,GAAgC,CAAhC,GAAoClG,IAAI,CAACkG,gBAA7C,CAAf,CAFmC,CAAf,CAAxB;IAIA,KAAKC,WAAL,GAAmBnG,IAAI,CAACmG,WAAxB;IACA,KAAKyB,cAAL,GAAsB5H,IAAI,CAAC4H,cAA3B;IACA,KAAKxH,SAAL,GAAiB,KAAK+E,KAAtB;IACA,KAAKiB,WAAL,GAAmB,IAAnB;IACA,KAAKC,oBAAL,GAA4B,IAA5B;EACH;;EACD1E,KAAK,CAACT,UAAD,EAAa;IACdA,UAAU,GAAGlF,kBAAkB,CAACkF,UAAD,CAA/B;IACA,MAAMY,QAAQ,GAAGZ,UAAU,CAACA,UAAU,CAACpE,MAAX,GAAoB,CAArB,CAA3B;IACA,KAAKwJ,MAAL,GAAc,KAAKC,SAAL,CAAe,QAAf,EAAyB,CAACzE,QAAD,EAAW,KAAKqD,KAAL,GAAa,CAAxB,CAAzB,EAAqD,IAArD,EAA2D,KAAKG,iBAAhE,EAAmF,KAAKG,iBAAxF,EAA2G,IAA3G,EAAiH,KAAKG,gBAAtH,CAAd;IACA,KAAKY,eAAL,GAAuB,KAAKD,SAAL,CAAe,kBAAf,EAAmC,CAAC,KAAKpB,KAAN,EAAa,KAAKA,KAAL,GAAa,CAA1B,CAAnC,EAAiE,IAAjE,EAAuE,KAAKI,oBAA5E,EAAkG,KAAKG,oBAAvG,EAA6H,IAA7H,EAAmI,KAAKG,mBAAxI,CAAvB;;IACA,IAAI,KAAKR,OAAT,EAAkB;MACd,KAAKoB,IAAL,GAAY,KAAKF,SAAL,CAAe,MAAf,EAAuB,CAAC,KAAKpB,KAAL,GAAa,CAAd,CAAvB,EAAyC,IAAzC,EAA+C,KAAKK,eAApD,EAAqE,KAAKG,eAA1E,EAA2F,IAA3F,EAAiG,KAAKG,cAAtG,CAAZ;IACH,CAFD,MAGK;MACD,KAAKW,IAAL,GAAY,IAAZ;IACH,CAVa,CAWd;IACA;;;IACA,KAAKC,KAAL,GAAa,IAAb;EACH;;EACDrD,IAAI,CAAC9G,MAAD,EAASsG,MAAT,EAAiB;IACjB,OAAOnI,IAAI,CAAC,MAAM;MACd6B,MAAM,GAAGA,MAAT;;MACA,IAAIA,MAAM,CAACO,MAAP,KAAkB,CAAtB,EAAyB;QACrB,MAAM,IAAIvB,UAAJ,CAAgB,sDAAD,GAChB,GAAEgB,MAAM,CAACO,MAAO,GADf,CAAN;MAEH;;MACD,MAAMqF,QAAQ,GAAGU,MAAM,CAAC,UAAD,CAAN,IAAsB,IAAtB,GAA6B,KAA7B,GAAqCA,MAAM,CAAC,UAAD,CAA5D;MACA,IAAIgF,QAAQ,GAAGtL,MAAM,CAAC,CAAD,CAArB,CAPc,CAOY;;MAC1BA,MAAM,GAAGA,MAAM,CAAC,CAAD,CAAf,CARc,CASd;MACA;MACA;;MACA,IAAI,IAAI,KAAKwJ,OAAT,IAAoB,KAAKA,OAAL,GAAe,CAAnC,IAAwC,KAAKK,WAAL,IAAoB,IAAhE,EAAsE;QAClE,KAAKA,WAAL,GAAmBQ,mBAAmB,CAAC;UACnCC,IAAI,EAAE,MAAMrM,GAAG,CAAC0E,QAAJ,CAAa3C,MAAb,CADuB;UAEnCuK,IAAI,EAAE,KAAKf,OAFwB;UAGnC5D,QAHmC;UAInC2F,KAAK,EAAE,CAJ4B;UAKnC3B,WAAW,EAAE,KAAKA;QALiB,CAAD,CAAtC;MAOH;;MACD,IAAI,IAAI,KAAKD,gBAAT,IAA6B,KAAKA,gBAAL,GAAwB,CAArD,IACA,KAAKG,oBAAL,IAA6B,IADjC,EACuC;QACnC,KAAKA,oBAAL,GAA4BO,mBAAmB,CAAC;UAC5CC,IAAI,EAAE,MAAMrM,GAAG,CAAC0E,QAAJ,CAAa2I,QAAb,CADgC;UAE5Cf,IAAI,EAAE,KAAKZ,gBAFiC;UAG5C/D,QAH4C;UAI5C2F,KAAK,EAAE,CAJqC;UAK5C3B,WAAW,EAAE,KAAKA;QAL0B,CAAD,CAA/C;MAOH;;MACD,MAAMa,MAAM,GAAG,KAAKZ,WAApB;MACA,MAAMa,SAAS,GAAG,KAAKZ,oBAAvB;MACA,IAAI0B,CAAJ;MACA,IAAIC,CAAJ;MACA,IAAIC,EAAJ;;MACA,IAAI,IAAI,KAAKlC,OAAT,IAAoB,KAAKA,OAAL,GAAe,CAAvC,EAA0C;QACtCxJ,MAAM,GAAG/B,GAAG,CAAC6E,GAAJ,CAAQ9C,MAAR,EAAgByK,MAAM,CAAC,CAAD,CAAtB,CAAT;MACH;;MACD,IAAIkB,OAAO,GAAGpN,CAAC,CAACoM,GAAF,CAAM3K,MAAN,EAAc,KAAK+J,MAAL,CAAYa,IAAZ,EAAd,CAAd;;MACA,IAAI,KAAK9B,OAAT,EAAkB;QACd6C,OAAO,GAAGpN,CAAC,CAACsM,OAAF,CAAUc,OAAV,EAAmB,KAAKzB,IAAL,CAAUU,IAAV,EAAnB,CAAV;MACH;;MACD,IAAI,IAAI,KAAKjB,gBAAT,IAA6B,KAAKA,gBAAL,GAAwB,CAAzD,EAA4D;QACxD2B,QAAQ,GAAGrN,GAAG,CAAC6E,GAAJ,CAAQwI,QAAR,EAAkBZ,SAAS,CAAC,CAAD,CAA3B,CAAX;MACH;;MACD,MAAMkB,oBAAoB,GAAG,KAAK3B,eAAL,CAAqBW,IAArB,EAA7B;MACA,MAAM,CAACiB,GAAD,EAAMC,GAAN,IAAa7N,GAAG,CAAC8N,KAAJ,CAAUH,oBAAV,EAAgC,CAAC,IAAI,KAAKhD,KAAV,EAAiB,KAAKA,KAAtB,CAAhC,EAA8DgD,oBAAoB,CAAClK,IAArB,GAA4B,CAA1F,CAAnB;MACA,MAAMsK,WAAW,GAAGzN,CAAC,CAACoM,GAAF,CAAMW,QAAN,EAAgBO,GAAhB,CAApB;MACA,MAAM,CAACI,EAAD,EAAKC,EAAL,EAASC,EAAT,IAAelO,GAAG,CAAC8N,KAAJ,CAAUJ,OAAV,EAAmB,CAAnB,EAAsBA,OAAO,CAACjK,IAAR,GAAe,CAArC,CAArB;MACA,MAAM,CAAC0K,UAAD,EAAaC,UAAb,IAA2BpO,GAAG,CAAC8N,KAAJ,CAAUC,WAAV,EAAuB,CAAvB,EAA0BA,WAAW,CAACtK,IAAZ,GAAmB,CAA7C,CAAjC;MACA8J,CAAC,GAAG,KAAKJ,mBAAL,CAAyB/E,KAAzB,CAA+BpI,GAAG,CAAC4E,GAAJ,CAAQoJ,EAAR,EAAYG,UAAZ,CAA/B,CAAJ;MACAX,CAAC,GAAG,KAAKL,mBAAL,CAAyB/E,KAAzB,CAA+BpI,GAAG,CAAC4E,GAAJ,CAAQqJ,EAAR,EAAYG,UAAZ,CAA/B,CAAJ;MACA,MAAMC,UAAU,GAAG/N,CAAC,CAACoM,GAAF,CAAM1M,GAAG,CAAC6E,GAAJ,CAAQ2I,CAAR,EAAWH,QAAX,CAAN,EAA4BQ,GAA5B,CAAnB;MACAJ,EAAE,GAAG,KAAK7C,UAAL,CAAgBxC,KAAhB,CAAsBpI,GAAG,CAAC4E,GAAJ,CAAQsJ,EAAR,EAAYG,UAAZ,CAAtB,CAAL;MACA,MAAM9B,CAAC,GAAGvM,GAAG,CAAC4E,GAAJ,CAAQ5E,GAAG,CAAC6E,GAAJ,CAAQ0I,CAAR,EAAWF,QAAX,CAAR,EAA8BrN,GAAG,CAAC6E,GAAJ,CAAQ7E,GAAG,CAAC4E,GAAJ,CAAQ,CAAR,EAAW5E,GAAG,CAACsO,GAAJ,CAAQf,CAAR,CAAX,CAAR,EAAgCE,EAAhC,CAA9B,CAAV,CAvDc,CAwDd;;MACA,OAAO,CAAClB,CAAD,EAAIA,CAAJ,CAAP;IACH,CA1DU,CAAX;EA2DH;;EACD9C,SAAS,GAAG;IACR,MAAMC,UAAU,GAAG,MAAMD,SAAN,EAAnB;IACA,MAAME,MAAM,GAAG;MACXgB,KAAK,EAAE,KAAKA,KADD;MAEXC,UAAU,EAAEvK,mBAAmB,CAAC,KAAKuK,UAAN,CAFpB;MAGXuC,mBAAmB,EAAE9M,mBAAmB,CAAC,KAAK8M,mBAAN,CAH7B;MAIXtC,OAAO,EAAE,KAAKA,OAJH;MAKXC,iBAAiB,EAAE3J,oBAAoB,CAAC,KAAK2J,iBAAN,CAL5B;MAMXC,oBAAoB,EAAE5J,oBAAoB,CAAC,KAAK4J,oBAAN,CAN/B;MAOXC,eAAe,EAAE7J,oBAAoB,CAAC,KAAK6J,eAAN,CAP1B;MAQXC,iBAAiB,EAAE5J,oBAAoB,CAAC,KAAK4J,iBAAN,CAR5B;MASXC,oBAAoB,EAAE7J,oBAAoB,CAAC,KAAK6J,oBAAN,CAT/B;MAUXC,eAAe,EAAE9J,oBAAoB,CAAC,KAAK8J,eAAN,CAV1B;MAWX0B,mBAAmB,EAAExL,oBAAoB,CAAC,KAAKwL,mBAAN,CAX9B;MAYXzB,gBAAgB,EAAE3K,mBAAmB,CAAC,KAAK2K,gBAAN,CAZ1B;MAaXC,mBAAmB,EAAE5K,mBAAmB,CAAC,KAAK4K,mBAAN,CAb7B;MAcXC,cAAc,EAAE7K,mBAAmB,CAAC,KAAK6K,cAAN,CAdxB;MAeXC,OAAO,EAAE,KAAKA,OAfH;MAgBXG,gBAAgB,EAAE,KAAKA,gBAhBZ;MAiBX0B,cAAc,EAAE,KAAKA,cAjBV;MAkBXH,UAAU,EAAE;IAlBD,CAAf;IAoBA,OAAOlD,MAAM,CAACC,MAAP,CAAc,EAAd,EAAkBN,UAAlB,EAA8BC,MAA9B,CAAP;EACH;;AA3IgC;AA6IrC;;AACAoD,OAAO,CAACjD,SAAR,GAAoB,SAApB;AACA7J,aAAa,CAACmK,aAAd,CAA4B2C,OAA5B;AACA,OAAO,MAAMwB,GAAN,SAAkBjJ,GAAlB,CAAsB;EACzBC,WAAW,CAACC,IAAD,EAAO;IACd,IAAIA,IAAI,CAAC4H,cAAL,KAAwB,CAA5B,EAA+B;MAC3B9J,OAAO,CAACC,IAAR,CAAa,iEACT,oDADJ;IAEH;;IACDiC,IAAI,CAACC,IAAL,GAAY,IAAIsH,OAAJ,CAAYvH,IAAZ,CAAZ;IACA,MAAMA,IAAN,EANc,CAOd;EACH;;EACDqD,IAAI,CAAC9G,MAAD,EAASsG,MAAT,EAAiB;IACjB,OAAOnI,IAAI,CAAC,MAAM;MACd,IAAI,KAAKuF,IAAL,CAAUmG,WAAV,IAAyB,IAA7B,EAAmC;QAC/B5L,GAAG,CAAC6H,OAAJ,CAAY,KAAKpC,IAAL,CAAUmG,WAAtB;QACA,KAAKnG,IAAL,CAAUmG,WAAV,GAAwB,IAAxB;MACH;;MACD,IAAI,KAAKnG,IAAL,CAAUoG,oBAAV,IAAkC,IAAtC,EAA4C;QACxC7L,GAAG,CAAC6H,OAAJ,CAAY,KAAKpC,IAAL,CAAUoG,oBAAtB;QACA,KAAKpG,IAAL,CAAUoG,oBAAV,GAAiC,IAAjC;MACH;;MACD,MAAMhJ,IAAI,GAAGwF,MAAM,IAAI,IAAV,GAAiB,IAAjB,GAAwBA,MAAM,CAAC,MAAD,CAA3C;MACA,MAAMV,QAAQ,GAAGU,MAAM,IAAI,IAAV,GAAiB,IAAjB,GAAwBA,MAAM,CAAC,UAAD,CAA/C;MACA,MAAMrG,YAAY,GAAGqG,MAAM,IAAI,IAAV,GAAiB,IAAjB,GAAwBA,MAAM,CAAC,cAAD,CAAnD;MACA,OAAO,MAAMQ,IAAN,CAAW9G,MAAX,EAAmB;QAAEc,IAAF;QAAQ8E,QAAR;QAAkB3F;MAAlB,CAAnB,CAAP;IACH,CAbU,CAAX;EAcH;EACD;;;EACiB,OAAViI,UAAU,CAACC,GAAD,EAAMP,MAAN,EAAc;IAC3B,IAAIA,MAAM,CAAC,eAAD,CAAN,KAA4B,CAAhC,EAAmC;MAC/BA,MAAM,CAAC,gBAAD,CAAN,GAA2B,CAA3B;IACH;;IACD,OAAO,IAAIO,GAAJ,CAAQP,MAAR,CAAP;EACH;;AAhCwB;AAkC7B;;AACA4E,GAAG,CAACzE,SAAJ,GAAgB,KAAhB;AACA7J,aAAa,CAACmK,aAAd,CAA4BmE,GAA5B;AACA,OAAO,MAAMC,QAAN,SAAuBnE,OAAvB,CAA+B;EAClC9E,WAAW,CAACC,IAAD,EAAO;IACd,MAAMA,IAAN;IACA,KAAK+E,kBAAL,GAA0B,MAA1B;IACA,KAAKyC,4BAAL,GAAoC,aAApC;IACA,KAAKxC,0BAAL,GAAkC,cAAlC;IACA,KAAKC,6BAAL,GAAqC,YAArC;IACA,KAAKC,wBAAL,GAAgC,OAAhC;IACA,KAAKC,KAAL,GAAanF,IAAI,CAACmF,KAAlB;IACArJ,qBAAqB,CAAC,KAAKqJ,KAAN,EAAa,OAAb,CAArB;IACA,KAAKC,UAAL,GAAkBxK,aAAa,CAACoF,IAAI,CAACoF,UAAL,KAAoBsC,SAApB,GAAgC,KAAK3C,kBAArC,GAC5B/E,IAAI,CAACoF,UADsB,CAA/B;IAEA,KAAKuC,mBAAL,GAA2B/M,aAAa,CAACoF,IAAI,CAAC2H,mBAAL,KAA6BD,SAA7B,GACrC,KAAKF,4BADgC,GAErCxH,IAAI,CAAC2H,mBAF+B,CAAxC;IAGA,KAAKtC,OAAL,GAAerF,IAAI,CAACqF,OAAL,IAAgB,IAAhB,GAAuB,IAAvB,GAA8BrF,IAAI,CAACqF,OAAlD;IACA,KAAKC,iBAAL,GAAyB9J,cAAc,CAACwE,IAAI,CAACsF,iBAAL,IAA0B,KAAKN,0BAAhC,CAAvC;IACA,KAAKO,oBAAL,GAA4B/J,cAAc,CAACwE,IAAI,CAACuF,oBAAL,IAA6B,KAAKN,6BAAnC,CAA1C;IACA,KAAKO,eAAL,GACIhK,cAAc,CAACwE,IAAI,CAACwF,eAAL,IAAwB,KAAKN,wBAA9B,CADlB;IAEA,KAAK+D,cAAL,GAAsBjJ,IAAI,CAACiJ,cAA3B;IACA,KAAKxD,iBAAL,GAAyB7J,cAAc,CAACoE,IAAI,CAACyF,iBAAN,CAAvC;IACA,KAAKC,oBAAL,GAA4B9J,cAAc,CAACoE,IAAI,CAAC0F,oBAAN,CAA1C;IACA,KAAKC,eAAL,GAAuB/J,cAAc,CAACoE,IAAI,CAAC2F,eAAN,CAArC;IACA,KAAKC,gBAAL,GAAwB5K,aAAa,CAACgF,IAAI,CAAC4F,gBAAN,CAArC;IACA,KAAKC,mBAAL,GAA2B7K,aAAa,CAACgF,IAAI,CAAC6F,mBAAN,CAAxC;IACA,KAAKC,cAAL,GAAsB9K,aAAa,CAACgF,IAAI,CAAC8F,cAAN,CAAnC;IACA,KAAKC,OAAL,GAAehK,UAAU,CAACiK,GAAX,CAAe,CAAC,CAAD,EAAIjK,UAAU,CAACkK,GAAX,CAAe,CAAC,CAAD,EAAIjG,IAAI,CAAC+F,OAAL,IAAgB,IAAhB,GAAuB,CAAvB,GAA2B/F,IAAI,CAAC+F,OAApC,CAAf,CAAJ,CAAf,CAAf;IACA,KAAKG,gBAAL,GAAwBnK,UAAU,CAACiK,GAAX,CAAe,CACnC,CADmC,EAEnCjK,UAAU,CAACkK,GAAX,CAAe,CAAC,CAAD,EAAIjG,IAAI,CAACkG,gBAAL,IAAyB,IAAzB,GAAgC,CAAhC,GAAoClG,IAAI,CAACkG,gBAA7C,CAAf,CAFmC,CAAf,CAAxB;IAIA,KAAKC,WAAL,GAAmBnG,IAAI,CAACmG,WAAxB;IACA,KAAKyB,cAAL,GAAsB5H,IAAI,CAAC4H,cAA3B;IACA,KAAKxH,SAAL,GAAiB,CAAC,KAAK+E,KAAN,EAAa,KAAKA,KAAlB,CAAjB;IACA,KAAKiB,WAAL,GAAmB,IAAnB;IACA,KAAKC,oBAAL,GAA4B,IAA5B;EACH;;EACD1E,KAAK,CAACT,UAAD,EAAa;IACd,IAAIgI,EAAJ;;IACAhI,UAAU,GAAGlF,kBAAkB,CAACkF,UAAD,CAA/B;IACA,MAAMY,QAAQ,GAAGZ,UAAU,CAACA,UAAU,CAACpE,MAAX,GAAoB,CAArB,CAA3B;IACA,KAAKwJ,MAAL,GAAc,KAAKC,SAAL,CAAe,QAAf,EAAyB,CAACzE,QAAD,EAAW,KAAKqD,KAAL,GAAa,CAAxB,CAAzB,EAAqD,IAArD,EAA2D,KAAKG,iBAAhE,EAAmF,KAAKG,iBAAxF,EAA2G,IAA3G,EAAiH,KAAKG,gBAAtH,CAAd;IACA,KAAKY,eAAL,GAAuB,KAAKD,SAAL,CAAe,kBAAf,EAAmC,CAAC,KAAKpB,KAAN,EAAa,KAAKA,KAAL,GAAa,CAA1B,CAAnC,EAAiE,IAAjE,EAAuE,KAAKI,oBAA5E,EAAkG,KAAKG,oBAAvG,EAA6H,IAA7H,EAAmI,KAAKG,mBAAxI,CAAvB;IACA,IAAIL,eAAJ;;IACA,IAAI,KAAKH,OAAT,EAAkB;MACd,IAAI,KAAK4D,cAAT,EAAyB;QACrB,MAAME,gBAAgB,GAAG,KAAK3D,eAA9B;QACA,MAAM4D,aAAa,GAAG,KAAKjE,KAA3B;QACAK,eAAe,GAAG,KAAK0D,EAAE,GAAG,MAAMG,UAAN,SAAyB5N,WAAzB,CAAqC;UACzDmH,KAAK,CAACnF,KAAD,EAAQ6L,KAAR,EAAe;YAChB;YACA,MAAMC,EAAE,GAAGJ,gBAAgB,CAACvG,KAAjB,CAAuB,CAACwG,aAAD,CAAvB,CAAX;YACA,MAAMI,EAAE,GAAI,IAAI9N,IAAJ,EAAD,CAAakH,KAAb,CAAmB,CAACwG,aAAD,CAAnB,CAAX;YACA,MAAMK,MAAM,GAAGN,gBAAgB,CAACvG,KAAjB,CAAuB,CAACwG,aAAa,GAAG,CAAjB,CAAvB,CAAf;YACA,OAAOtO,CAAC,CAAC4O,oBAAF,CAAuB5O,CAAC,CAAC4O,oBAAF,CAAuBH,EAAvB,EAA2BC,EAA3B,CAAvB,EAAuDC,MAAvD,CAAP;UACH;;QAPwD,CAA1C;QASnB;QACAP,EAAE,CAAC5E,SAAH,GAAe,YAVI,EAWnB4E,EAXc,GAAlB;MAYH,CAfD,MAgBK;QACD1D,eAAe,GAAG,KAAKA,eAAvB;MACH;;MACD,KAAKiB,IAAL,GAAY,KAAKF,SAAL,CAAe,MAAf,EAAuB,CAAC,KAAKpB,KAAL,GAAa,CAAd,CAAvB,EAAyC,IAAzC,EAA+CK,eAA/C,EAAgE,KAAKG,eAArE,EAAsF,IAAtF,EAA4F,KAAKG,cAAjG,CAAZ;IACH,CArBD,MAsBK;MACD,KAAKW,IAAL,GAAY,IAAZ;IACH,CA/Ba,CAgCd;IACA;;;IACA,KAAKC,KAAL,GAAa,IAAb;EACH;;EACDrD,IAAI,CAAC9G,MAAD,EAASsG,MAAT,EAAiB;IACjB,OAAOnI,IAAI,CAAC,MAAM;MACd,MAAMyH,QAAQ,GAAGU,MAAM,CAAC,UAAD,CAAN,IAAsB,IAAtB,GAA6B,KAA7B,GAAqCA,MAAM,CAAC,UAAD,CAA5D;MACAtG,MAAM,GAAGA,MAAT;;MACA,IAAIA,MAAM,CAACO,MAAP,KAAkB,CAAtB,EAAyB;QACrB,MAAM,IAAIvB,UAAJ,CAAgB,uDAAD,GAChB,GAAEgB,MAAM,CAACO,MAAO,GADf,CAAN;MAEH;;MACD,IAAI+K,QAAQ,GAAGtL,MAAM,CAAC,CAAD,CAArB,CAPc,CAOY;;MAC1B,MAAMoN,QAAQ,GAAGpN,MAAM,CAAC,CAAD,CAAvB,CARc,CAQc;;MAC5BA,MAAM,GAAGA,MAAM,CAAC,CAAD,CAAf;;MACA,IAAI,IAAI,KAAKwJ,OAAT,IAAoB,KAAKA,OAAL,GAAe,CAAnC,IAAwC,KAAKK,WAAL,IAAoB,IAAhE,EAAsE;QAClE,KAAKA,WAAL,GAAmBQ,mBAAmB,CAAC;UACnCC,IAAI,EAAE,MAAMrM,GAAG,CAAC0E,QAAJ,CAAa3C,MAAb,CADuB;UAEnCuK,IAAI,EAAE,KAAKf,OAFwB;UAGnC5D,QAHmC;UAInC2F,KAAK,EAAE,CAJ4B;UAKnC3B,WAAW,EAAE,KAAKA;QALiB,CAAD,CAAtC;MAOH;;MACD,IAAI,IAAI,KAAKD,gBAAT,IAA6B,KAAKA,gBAAL,GAAwB,CAArD,IACA,KAAKG,oBAAL,IAA6B,IADjC,EACuC;QACnC,KAAKA,oBAAL,GAA4BO,mBAAmB,CAAC;UAC5CC,IAAI,EAAE,MAAMrM,GAAG,CAAC0E,QAAJ,CAAa2I,QAAb,CADgC;UAE5Cf,IAAI,EAAE,KAAKZ,gBAFiC;UAG5C/D,QAH4C;UAI5C2F,KAAK,EAAE,CAJqC;UAK5C3B,WAAW,EAAE,KAAKA;QAL0B,CAAD,CAA/C;MAOH;;MACD,MAAMa,MAAM,GAAG,KAAKZ,WAApB;MACA,MAAMa,SAAS,GAAG,KAAKZ,oBAAvB,CA9Bc,CA+Bd;MACA;MACA;;MACA,IAAI5G,CAAJ;MACA,IAAImK,CAAJ;MACA,IAAIC,CAAJ;MACA,IAAIC,CAAJ;;MACA,IAAI,IAAI,KAAK/D,OAAT,IAAoB,KAAKA,OAAL,GAAe,CAAvC,EAA0C;QACtCxJ,MAAM,GAAG/B,GAAG,CAAC6E,GAAJ,CAAQ9C,MAAR,EAAgByK,MAAM,CAAC,CAAD,CAAtB,CAAT;MACH;;MACD,IAAIe,CAAC,GAAGjN,CAAC,CAACoM,GAAF,CAAM3K,MAAN,EAAc,KAAK+J,MAAL,CAAYa,IAAZ,EAAd,CAAR;;MACA,IAAI,IAAI,KAAKjB,gBAAT,IAA6B,KAAKA,gBAAL,GAAwB,CAAzD,EAA4D;QACxD2B,QAAQ,GAAGrN,GAAG,CAAC6E,GAAJ,CAAQwI,QAAR,EAAkBZ,SAAS,CAAC,CAAD,CAA3B,CAAX;MACH;;MACDc,CAAC,GAAGvN,GAAG,CAAC4E,GAAJ,CAAQ2I,CAAR,EAAWjN,CAAC,CAACoM,GAAF,CAAMW,QAAN,EAAgB,KAAKrB,eAAL,CAAqBW,IAArB,EAAhB,CAAX,CAAJ;;MACA,IAAI,KAAK9B,OAAT,EAAkB;QACd0C,CAAC,GAAGjN,CAAC,CAACsM,OAAF,CAAUW,CAAV,EAAa,KAAKtB,IAAL,CAAUU,IAAV,EAAb,CAAJ;MACH;;MACD,MAAM,CAAC4C,EAAD,EAAKC,EAAL,EAASC,EAAT,EAAaC,EAAb,IAAmB1P,GAAG,CAAC8N,KAAJ,CAAUP,CAAV,EAAa,CAAb,EAAgBA,CAAC,CAAC9J,IAAF,GAAS,CAAzB,CAAzB;MACAwB,CAAC,GAAG,KAAKkI,mBAAL,CAAyB/E,KAAzB,CAA+BmH,EAA/B,CAAJ;MACAH,CAAC,GAAG,KAAKjC,mBAAL,CAAyB/E,KAAzB,CAA+BoH,EAA/B,CAAJ;MACAH,CAAC,GAAGrP,GAAG,CAAC4E,GAAJ,CAAQ5E,GAAG,CAAC6E,GAAJ,CAAQuK,CAAR,EAAWD,QAAX,CAAR,EAA8BnP,GAAG,CAAC6E,GAAJ,CAAQI,CAAR,EAAW,KAAK2F,UAAL,CAAgBxC,KAAhB,CAAsBqH,EAAtB,CAAX,CAA9B,CAAJ;MACAH,CAAC,GAAG,KAAKnC,mBAAL,CAAyB/E,KAAzB,CAA+BsH,EAA/B,CAAJ;MACA,MAAMnD,CAAC,GAAGvM,GAAG,CAAC6E,GAAJ,CAAQyK,CAAR,EAAW,KAAK1E,UAAL,CAAgBxC,KAAhB,CAAsBiH,CAAtB,CAAX,CAAV,CAtDc,CAuDd;;MACA,OAAO,CAAC9C,CAAD,EAAIA,CAAJ,EAAO8C,CAAP,CAAP;IACH,CAzDU,CAAX;EA0DH;;EACD5F,SAAS,GAAG;IACR,MAAMC,UAAU,GAAG,MAAMD,SAAN,EAAnB;IACA,MAAME,MAAM,GAAG;MACXgB,KAAK,EAAE,KAAKA,KADD;MAEXC,UAAU,EAAEvK,mBAAmB,CAAC,KAAKuK,UAAN,CAFpB;MAGXuC,mBAAmB,EAAE9M,mBAAmB,CAAC,KAAK8M,mBAAN,CAH7B;MAIXtC,OAAO,EAAE,KAAKA,OAJH;MAKXC,iBAAiB,EAAE3J,oBAAoB,CAAC,KAAK2J,iBAAN,CAL5B;MAMXC,oBAAoB,EAAE5J,oBAAoB,CAAC,KAAK4J,oBAAN,CAN/B;MAOXC,eAAe,EAAE7J,oBAAoB,CAAC,KAAK6J,eAAN,CAP1B;MAQXyD,cAAc,EAAE,KAAKA,cARV;MASXxD,iBAAiB,EAAE5J,oBAAoB,CAAC,KAAK4J,iBAAN,CAT5B;MAUXC,oBAAoB,EAAE7J,oBAAoB,CAAC,KAAK6J,oBAAN,CAV/B;MAWXC,eAAe,EAAE9J,oBAAoB,CAAC,KAAK8J,eAAN,CAX1B;MAYX0B,mBAAmB,EAAExL,oBAAoB,CAAC,KAAKwL,mBAAN,CAZ9B;MAaXzB,gBAAgB,EAAE3K,mBAAmB,CAAC,KAAK2K,gBAAN,CAb1B;MAcXC,mBAAmB,EAAE5K,mBAAmB,CAAC,KAAK4K,mBAAN,CAd7B;MAeXC,cAAc,EAAE7K,mBAAmB,CAAC,KAAK6K,cAAN,CAfxB;MAgBXC,OAAO,EAAE,KAAKA,OAhBH;MAiBXG,gBAAgB,EAAE,KAAKA,gBAjBZ;MAkBX0B,cAAc,EAAE,KAAKA;IAlBV,CAAf;IAoBA,OAAOrD,MAAM,CAACC,MAAP,CAAc,EAAd,EAAkBN,UAAlB,EAA8BC,MAA9B,CAAP;EACH;;AA7JiC;AA+JtC;;AACA6E,QAAQ,CAAC1E,SAAT,GAAqB,UAArB;AACA7J,aAAa,CAACmK,aAAd,CAA4BoE,QAA5B;AACA,OAAO,MAAMmB,IAAN,SAAmBrK,GAAnB,CAAuB;EAC1BC,WAAW,CAACC,IAAD,EAAO;IACd,IAAIA,IAAI,CAAC4H,cAAL,KAAwB,CAA5B,EAA+B;MAC3B9J,OAAO,CAACC,IAAR,CAAa,iEACT,oDADJ;IAEH;;IACDiC,IAAI,CAACC,IAAL,GAAY,IAAI+I,QAAJ,CAAahJ,IAAb,CAAZ;IACA,MAAMA,IAAN,EANc,CAOd;EACH;;EACDqD,IAAI,CAAC9G,MAAD,EAASsG,MAAT,EAAiB;IACjB,OAAOnI,IAAI,CAAC,MAAM;MACd,IAAI,KAAKuF,IAAL,CAAUmG,WAAV,IAAyB,IAA7B,EAAmC;QAC/B5L,GAAG,CAAC6H,OAAJ,CAAY,KAAKpC,IAAL,CAAUmG,WAAtB;QACA,KAAKnG,IAAL,CAAUmG,WAAV,GAAwB,IAAxB;MACH;;MACD,IAAI,KAAKnG,IAAL,CAAUoG,oBAAV,IAAkC,IAAtC,EAA4C;QACxC7L,GAAG,CAAC6H,OAAJ,CAAY,KAAKpC,IAAL,CAAUoG,oBAAtB;QACA,KAAKpG,IAAL,CAAUoG,oBAAV,GAAiC,IAAjC;MACH;;MACD,MAAMhJ,IAAI,GAAGwF,MAAM,IAAI,IAAV,GAAiB,IAAjB,GAAwBA,MAAM,CAAC,MAAD,CAA3C;MACA,MAAMV,QAAQ,GAAGU,MAAM,IAAI,IAAV,GAAiB,IAAjB,GAAwBA,MAAM,CAAC,UAAD,CAA/C;MACA,MAAMrG,YAAY,GAAGqG,MAAM,IAAI,IAAV,GAAiB,IAAjB,GAAwBA,MAAM,CAAC,cAAD,CAAnD;MACA,OAAO,MAAMQ,IAAN,CAAW9G,MAAX,EAAmB;QAAEc,IAAF;QAAQ8E,QAAR;QAAkB3F;MAAlB,CAAnB,CAAP;IACH,CAbU,CAAX;EAcH;EACD;;;EACiB,OAAViI,UAAU,CAACC,GAAD,EAAMP,MAAN,EAAc;IAC3B,IAAIA,MAAM,CAAC,eAAD,CAAN,KAA4B,CAAhC,EAAmC;MAC/BA,MAAM,CAAC,gBAAD,CAAN,GAA2B,CAA3B;IACH;;IACD,OAAO,IAAIO,GAAJ,CAAQP,MAAR,CAAP;EACH;;AAhCyB;AAkC9B;;AACAgG,IAAI,CAAC7F,SAAL,GAAiB,MAAjB;AACA7J,aAAa,CAACmK,aAAd,CAA4BuF,IAA5B;AACA,OAAO,MAAMjK,eAAN,SAA8B2E,OAA9B,CAAsC;EACzC9E,WAAW,CAACC,IAAD,EAAO;IACd,MAAMA,IAAN;IACA,KAAKG,KAAL,GAAaH,IAAI,CAACG,KAAlB;EACH;;EACY,IAATC,SAAS,GAAG;IACZ;IACA;IACA;IACA;IACA,MAAMA,SAAS,GAAG,EAAlB;;IACA,KAAK,MAAMH,IAAX,IAAmB,KAAKE,KAAL,CAAWtD,KAAX,GAAmBsB,OAAnB,EAAnB,EAAiD;MAC7C,IAAIxB,KAAK,CAACC,OAAN,CAAcqD,IAAI,CAACG,SAAnB,CAAJ,EAAmC;QAC/BA,SAAS,CAACV,IAAV,CAAe,GAAGO,IAAI,CAACG,SAAvB;MACH,CAFD,MAGK;QACDA,SAAS,CAACV,IAAV,CAAeO,IAAI,CAACG,SAApB;MACH;IACJ;;IACD,OAAOA,SAAP;EACH;;EACDiD,IAAI,CAAC9G,MAAD,EAASsG,MAAT,EAAiB;IACjB,OAAOnI,IAAI,CAAC,MAAM;MACd6B,MAAM,GAAGA,MAAT;MACA,IAAI+B,MAAM,GAAG/B,MAAM,CAACM,KAAP,CAAa,CAAb,CAAb,CAFc,CAGd;;MACA,MAAMuN,YAAY,GAAG,EAArB;;MACA,KAAK,MAAMnK,IAAX,IAAmB,KAAKE,KAAL,CAAWtD,KAAX,GAAmBsB,OAAnB,EAAnB,EAAiD;QAC7C,IAAIxB,KAAK,CAACC,OAAN,CAAcqD,IAAI,CAACG,SAAnB,CAAJ,EAAmC;UAC/BgK,YAAY,CAAC1K,IAAb,CAAkBpB,MAAM,CAAC+L,MAAP,CAAc,CAAd,EAAiBpK,IAAI,CAACG,SAAL,CAAetD,MAAhC,CAAlB;QACH,CAFD,MAGK;UACDsN,YAAY,CAAC1K,IAAb,CAAkBpB,MAAM,CAAC+L,MAAP,CAAc,CAAd,EAAiB,CAAjB,CAAlB;QACH;MACJ;;MACDD,YAAY,CAACjM,OAAb,GAbc,CAcd;;MACA,MAAMmM,eAAe,GAAG,EAAxB;MACA,IAAIC,UAAJ;;MACA,KAAK,IAAI9K,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAG,KAAKU,KAAL,CAAWrD,MAA/B,EAAuC,EAAE2C,CAAzC,EAA4C;QACxC,MAAMQ,IAAI,GAAG,KAAKE,KAAL,CAAWV,CAAX,CAAb;QACAnB,MAAM,GAAG8L,YAAY,CAAC3K,CAAD,CAArB,CAFwC,CAGxC;;QACA,IAAIA,CAAC,KAAK,CAAV,EAAa;UACT8K,UAAU,GAAG,CAAChO,MAAM,CAAC,CAAD,CAAP,EAAYoB,MAAZ,CAAmBW,MAAnB,CAAb;QACH,CAFD,MAGK;UACDiM,UAAU,GAAG,CAACA,UAAU,CAAC,CAAD,CAAX,EAAgB5M,MAAhB,CAAuBW,MAAvB,CAAb;QACH;;QACDiM,UAAU,GAAGtK,IAAI,CAACoD,IAAL,CAAUkH,UAAV,EAAsB1H,MAAtB,CAAb;QACAyH,eAAe,CAAC5K,IAAhB,CAAqB6K,UAAU,CAAC1N,KAAX,CAAiB,CAAjB,CAArB;MACH,CA7Ba,CA8Bd;;;MACAyB,MAAM,GAAG,EAAT;;MACA,KAAK,MAAMkM,UAAX,IAAyBF,eAAe,CAACzN,KAAhB,GAAwBsB,OAAxB,EAAzB,EAA4D;QACxDG,MAAM,CAACoB,IAAP,CAAY,GAAG8K,UAAf;MACH;;MACD,OAAO,CAACD,UAAU,CAAC,CAAD,CAAX,EAAgB5M,MAAhB,CAAuBW,MAAvB,CAAP;IACH,CApCU,CAAX;EAqCH;;EACDqD,KAAK,CAACT,UAAD,EAAa;IACd,IAAIhF,eAAe,CAACgF,UAAD,CAAnB,EAAiC;MAC7B;MACA;MACAA,UAAU,GAAGA,UAAU,CAAC,CAAD,CAAvB;IACH;;IACDA,UAAU,GAAGA,UAAb;IACA,IAAIC,SAAJ;IACA,KAAKhB,KAAL,CAAWsK,OAAX,CAAmB,CAACxK,IAAD,EAAOR,CAAP,KAAa;MAC5B1E,SAAS,CAAE,WAAU0E,CAAE,EAAd,EAAiB,MAAM;QAC5B;QACAQ,IAAI,CAAC0B,KAAL,CAAWT,UAAX;;QACA,IAAIvE,KAAK,CAACC,OAAN,CAAcqD,IAAI,CAACG,SAAnB,CAAJ,EAAmC;UAC/Be,SAAS,GAAGlB,IAAI,CAACG,SAAL,CAAe,CAAf,CAAZ;QACH,CAFD,MAGK;UACDe,SAAS,GAAGlB,IAAI,CAACG,SAAjB;QACH;;QACDc,UAAU,GAAG,CAACA,UAAU,CAAC,CAAD,CAAX,EAAgBC,SAAhB,CAAb;MACH,CAVQ,CAAT;IAWH,CAZD;IAaA,KAAKuF,KAAL,GAAa,IAAb;EACH;;EACDzC,SAAS,GAAG;IACR,MAAMC,UAAU,GAAG,MAAMD,SAAN,EAAnB;;IACA,MAAMyG,aAAa,GAAIzK,IAAD,IAAU;MAC5B,OAAO;QACH,aAAaA,IAAI,CAACoE,YAAL,EADV;QAEH,UAAUpE,IAAI,CAACgE,SAAL;MAFP,CAAP;IAIH,CALD;;IAMA,MAAM0G,WAAW,GAAG,KAAKxK,KAAL,CAAWZ,GAAX,CAAemL,aAAf,CAApB;IACA,MAAMvG,MAAM,GAAG;MAAE,SAASwG;IAAX,CAAf;IACA,OAAOpG,MAAM,CAACC,MAAP,CAAc,EAAd,EAAkBN,UAAlB,EAA8BC,MAA9B,CAAP;EACH;EACD;;;EACiB,OAAVM,UAAU,CAACC,GAAD,EAAMP,MAAN,EAAcQ,aAAa,GAAG,EAA9B,EAAkC;IAC/C,MAAMxE,KAAK,GAAG,EAAd;;IACA,KAAK,MAAMiE,UAAX,IAAyBD,MAAM,CAAC,OAAD,CAA/B,EAA0C;MACtChE,KAAK,CAACT,IAAN,CAAWrD,WAAW,CAAC+H,UAAD,EAAaO,aAAb,CAAtB;IACH;;IACD,OAAO,IAAID,GAAJ,CAAQ;MAAEvE;IAAF,CAAR,CAAP;EACH;;EACmB,IAAhByD,gBAAgB,GAAG;IACnB,IAAI,CAAC,KAAKC,SAAV,EAAqB;MACjB,OAAO,EAAP;IACH;;IACD,MAAME,OAAO,GAAG,EAAhB;;IACA,KAAK,MAAM9D,IAAX,IAAmB,KAAKE,KAAxB,EAA+B;MAC3B4D,OAAO,CAACrE,IAAR,CAAa,GAAGO,IAAI,CAAC2D,gBAArB;IACH;;IACD,OAAOG,OAAP;EACH;;EACsB,IAAnBD,mBAAmB,GAAG;IACtB,MAAMC,OAAO,GAAG,EAAhB;;IACA,KAAK,MAAM9D,IAAX,IAAmB,KAAKE,KAAxB,EAA+B;MAC3B4D,OAAO,CAACrE,IAAR,CAAa,GAAGO,IAAI,CAAC6D,mBAArB;IACH;;IACD,IAAI,CAAC,KAAKD,SAAV,EAAqB;MACjB,MAAMD,gBAAgB,GAAG,EAAzB;;MACA,KAAK,MAAM3D,IAAX,IAAmB,KAAKE,KAAxB,EAA+B;QAC3ByD,gBAAgB,CAAClE,IAAjB,CAAsB,GAAGO,IAAI,CAAC2D,gBAA9B;MACH;;MACD,OAAOA,gBAAgB,CAACjG,MAAjB,CAAwBoG,OAAxB,CAAP;IACH;;IACD,OAAOA,OAAP;EACH;EACD;AACJ;AACA;AACA;AACA;;;EACI6G,UAAU,GAAG;IACT,MAAM7G,OAAO,GAAG,EAAhB;;IACA,KAAK,MAAM9D,IAAX,IAAmB,KAAKE,KAAxB,EAA+B;MAC3B4D,OAAO,CAACrE,IAAR,CAAa,GAAGO,IAAI,CAAC8D,OAArB;IACH;;IACD,OAAO5H,aAAa,CAAC4H,OAAD,CAApB;EACH;EACD;AACJ;AACA;AACA;AACA;AACA;;;EACI8G,UAAU,CAAC9G,OAAD,EAAU;IAChB,MAAM+G,MAAM,GAAG,EAAf;;IACA,KAAK,MAAM7K,IAAX,IAAmB,KAAKE,KAAxB,EAA+B;MAC3B,MAAM4K,SAAS,GAAG9K,IAAI,CAAC8D,OAAL,CAAajH,MAA/B;MACA,MAAMkO,YAAY,GAAGjH,OAAO,CAACsG,MAAR,CAAeU,SAAf,CAArB;;MACA,KAAK,IAAItL,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGQ,IAAI,CAAC8D,OAAL,CAAajH,MAAjC,EAAyC,EAAE2C,CAA3C,EAA8C;QAC1CqL,MAAM,CAACpL,IAAP,CAAY,CAACO,IAAI,CAAC8D,OAAL,CAAatE,CAAb,CAAD,EAAkBuL,YAAY,CAACvL,CAAD,CAA9B,CAAZ;MACH;IACJ;;IACDrD,aAAa,CAAC0O,MAAD,CAAb;EACH;;AA3JwC;AA6J7C;;AACA5K,eAAe,CAACoE,SAAhB,GAA4B,iBAA5B;AACA7J,aAAa,CAACmK,aAAd,CAA4B1E,eAA5B;AACA,OAAO,SAAS0G,mBAAT,CAA6B5G,IAA7B,EAAmC;EACtC,MAAM;IAAE6G,IAAF;IAAQC,IAAR;IAAc3E,QAAQ,GAAG,KAAzB;IAAgC2F,KAAK,GAAG,CAAxC;IAA2C3B;EAA3C,IAA2DnG,IAAjE;;EACA,MAAMiL,aAAa,GAAG,MAAM9E,WAAW,IAAI,IAAf,GAAsBA,WAAW,CAACU,IAAI,EAAL,EAASC,IAAT,CAAjC,GAAkDhM,CAAC,CAACiL,OAAF,CAAUc,IAAI,EAAd,EAAkBC,IAAlB,CAA9E;;EACA,MAAMoE,UAAU,GAAG,MAAMpQ,CAAC,CAACqQ,YAAF,CAAeF,aAAf,EAA8BpE,IAA9B,EAAoC1E,QAApC,CAAzB,CAHsC,CAItC;;;EACA,IAAI,CAAC2F,KAAD,IAAUA,KAAK,IAAI,CAAvB,EAA0B;IACtB,OAAOtN,GAAG,CAACkI,IAAJ,CAASwI,UAAU,GAAGvI,KAAb,EAAT,CAAP;EACH;;EACD,MAAMyI,KAAK,GAAGzO,KAAK,CAACmL,KAAD,CAAL,CAAauD,IAAb,CAAkB3D,SAAlB,EAA6BnI,GAA7B,CAAiC2L,UAAjC,CAAd;EACA,OAAOE,KAAK,CAAC7L,GAAN,CAAU+L,CAAC,IAAI9Q,GAAG,CAACkI,IAAJ,CAAS4I,CAAC,CAAC3I,KAAF,EAAT,CAAf,CAAP;AACH"},"metadata":{},"sourceType":"module"}