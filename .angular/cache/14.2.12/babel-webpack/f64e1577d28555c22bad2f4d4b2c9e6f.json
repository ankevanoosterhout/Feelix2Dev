{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * Executor: Evaluates SymbolicTensor based on feeds.\n */\nimport { cast, dispose, memory, util } from '@tensorflow/tfjs-core';\nimport { ValueError } from '../errors';\nimport { LruCache } from '../utils/executor_utils';\nimport { toList } from '../utils/generic_utils';\nimport { InputLayer } from './input_layer';\nimport { SymbolicTensor } from './topology';\n/**\n * Helper function to check the dtype and shape compatibility of a feed value.\n */\n\nfunction assertFeedCompatibility(key, val) {\n  // Check dtype compatibility.\n  if (key.dtype == null || key.dtype === val.dtype) {\n    //  a.  If types match, return val tensor as is.\n    return val;\n  }\n\n  try {\n    //  b. Attempt to convert to expected type.\n    return cast(val, key.dtype);\n  } catch (err) {\n    //  c. If conversion fails, return helpful error.\n    throw new ValueError(`The dtype of the feed (${val.dtype}) can not be cast to the dtype ` + `of the key '${key.name}' (${key.dtype}).`);\n  }\n}\n/**\n * FeedDict: A mapping from unique SymbolicTensors to feed values for them.\n * A feed value is a concrete value represented as an `Tensor`.\n */\n\n\nexport class FeedDict {\n  /**\n   * Constructor, optionally does copy-construction.\n   * @param feeds An Array of `Feed`s, or another `FeedDict`, in which case\n   *   copy-construction will be performed.\n   */\n  constructor(feeds) {\n    this.id2Value = {};\n    this.id2Mask = {};\n    this.name2Id = {};\n\n    if (feeds instanceof FeedDict) {\n      for (const id in feeds.id2Value) {\n        this.id2Value[id] = feeds.id2Value[id];\n\n        if (id in feeds.id2Mask) {\n          this.id2Mask[id] = feeds.id2Mask[id];\n        }\n      }\n    } else {\n      if (feeds == null) {\n        return;\n      }\n\n      for (const feed of feeds) {\n        this.add(feed.key, feed.value);\n      }\n    }\n  }\n  /**\n   * Add a key-value pair to the FeedDict.\n   *\n   * @param key The key of the feed.\n   * @param value The value of the tensor feed.\n   * @param mask The value of the mask feed (optional).\n   * @returns This `FeedDict`.\n   * @throws ValueError: If the key `SymbolicTensor` already exists in the\n   *   `FeedDict`.\n   */\n\n\n  add(key, value, mask) {\n    if (this.id2Value[key.id] == null) {\n      this.id2Value[key.id] = assertFeedCompatibility(key, value);\n      this.name2Id[key.name] = key.id;\n\n      if (mask != null) {\n        this.id2Mask[key.id] = mask;\n      }\n    } else {\n      throw new ValueError(`Duplicate key: name=${key.name}, id=${key.id}`);\n    }\n\n    return this;\n  }\n  /**\n   * Add a Feed to the FeedDict.\n   * @param feed The new `Feed` to add.\n   * @returns This `FeedDict`.\n   */\n\n\n  addFeed(feed) {\n    this.add(feed.key, feed.value);\n  }\n  /**\n   * Probe whether a key already exists in the FeedDict.\n   * @param key\n   */\n\n\n  hasKey(key) {\n    return this.id2Value[key.id] != null;\n  }\n  /**\n   * Get all the SymbolicTensor available in this FeedDict.\n   */\n\n\n  names() {\n    return Object.keys(this.name2Id);\n  }\n  /**\n   * Get the feed value for given key.\n   * @param key The SymbolicTensor, or its name (as a string), of which the\n   *     value is sought.\n   * @returns If `key` exists, the corresponding feed value.\n   * @throws ValueError: If `key` does not exist in this `FeedDict`.\n   */\n\n\n  getValue(key) {\n    if (key instanceof SymbolicTensor) {\n      if (this.id2Value[key.id] == null) {\n        throw new ValueError(`Nonexistent key: ${key.name}`);\n      } else {\n        return this.id2Value[key.id];\n      }\n    } else {\n      const id = this.name2Id[key];\n\n      if (id == null) {\n        throw new ValueError(`Feed dict has no SymbolicTensor name: ${key}`);\n      }\n\n      return this.id2Value[id];\n    }\n  }\n  /**\n   * Get the feed mask for given key.\n   * @param key The SymbolicTensor, or its name (as a string), of which the\n   *     value is sought.\n   * @returns If `key` exists, the corresponding feed mask.\n   * @throws ValueError: If `key` does not exist in this `FeedDict`.\n   */\n\n\n  getMask(key) {\n    if (key instanceof SymbolicTensor) {\n      if (this.id2Value[key.id] == null) {\n        throw new ValueError(`Nonexistent key: ${key.name}`);\n      } else {\n        return this.id2Mask[key.id];\n      }\n    } else {\n      const id = this.name2Id[key];\n\n      if (id == null) {\n        throw new ValueError(`Feed dict has no SymbolicTensor name: ${key}`);\n      }\n\n      return this.id2Mask[id];\n    }\n  }\n  /** Dispose all mask Tensors held by this object. */\n\n\n  disposeMasks() {\n    if (this.id2Mask != null) {\n      dispose(this.id2Mask);\n    }\n  }\n\n} // Cache for topologically sorted SymbolicTensors for given execution\n// targets (i.e., fetches).\n\nexport const cachedSorted = new LruCache(); // Cache for recipient count maps for given execution targets (i.e., fetches).\n\nexport const cachedRecipientCounts = new LruCache();\nexport function updateCacheMaxEntries(maxEntries) {\n  if (cachedSorted != null) {\n    cachedSorted.setMaxEntries(maxEntries);\n  }\n\n  if (cachedRecipientCounts != null) {\n    cachedRecipientCounts.setMaxEntries(maxEntries);\n  }\n}\n/**\n * Execute a SymbolicTensor by using concrete feed values.\n *\n * A `SymbolicTensor` object is a node in a computation graph of TF.js\n * Layers. The object is backed by a source layer and input\n * `SymbolicTensor`s to the source layer. This method evaluates\n * the `call()` method of the source layer, using concrete values of the\n * inputs obtained from either\n * * `feedDict`, if the input key exists in `feedDict`, or else,\n * * a recursive call to `execute()` itself.\n *\n * @param x: The `SymbolicTensor` to execute.\n * @param feedDict: The feed values, as base condition of the recursion.\n *   execution.\n * @param kwargs: Optional keyword arguments.\n * @param probe: A probe object (of interface `ExecutionProbe`) used for\n *   testing memory footprint of `execute` calls.\n * @returns Result of the execution.\n * @throws ValueError: If any `SymbolicTensor`s from `InputLayer`s\n *   encountered during the execution lacks a feed value in `feedDict`.\n */\n\nexport function execute(fetches, feedDict, kwargs, probe) {\n  const training = kwargs == null ? false : kwargs['training'];\n  const arrayFetches = Array.isArray(fetches);\n  const fetchArray = arrayFetches ? fetches : [fetches];\n  const outputNames = fetchArray.map(t => t.name);\n  const finalOutputs = [];\n  const feedNames = feedDict.names();\n\n  for (const outputName of outputNames) {\n    if (feedNames.indexOf(outputName) !== -1) {\n      finalOutputs.push(feedDict.getValue(outputName));\n    } else {\n      finalOutputs.push(null);\n    }\n  }\n\n  if (probe != null) {\n    // For optional probing of memory footprint during execution.\n    probe.maxNumTensors = -Infinity;\n    probe.minNumTensors = Infinity;\n  } // Check cache.\n\n\n  const fetchAndFeedKey = outputNames.join(',') + '|' + feedDict.names().sort().join(',');\n  let sorted = cachedSorted.get(fetchAndFeedKey);\n  let recipientCounts;\n\n  if (sorted == null) {\n    // Cache doesn't contain the desired combination of fetches. Compute\n    // topological sort for the combination for the first time.\n    const out = getTopologicalSortAndRecipientCounts(fetchArray, feedDict);\n    sorted = out.sorted;\n    recipientCounts = out.recipientCounts; // Store results in cache for future use.\n\n    cachedSorted.put(fetchAndFeedKey, sorted);\n    cachedRecipientCounts.put(fetchAndFeedKey, recipientCounts);\n  }\n\n  recipientCounts = {};\n\n  if (!training) {\n    Object.assign(recipientCounts, cachedRecipientCounts.get(fetchAndFeedKey));\n  }\n\n  const internalFeedDict = new FeedDict(feedDict); // Start iterative execution on the topologically-sorted SymbolicTensors.\n\n  for (let i = 0; i < sorted.length; ++i) {\n    if (probe != null) {\n      // For optional probing of memory usage during execution.\n      const numTensors = memory().numTensors;\n\n      if (numTensors > probe.maxNumTensors) {\n        probe.maxNumTensors = numTensors;\n      }\n\n      if (numTensors < probe.minNumTensors) {\n        probe.minNumTensors = numTensors;\n      }\n    }\n\n    const symbolic = sorted[i];\n    const srcLayer = symbolic.sourceLayer;\n\n    if (srcLayer instanceof InputLayer) {\n      continue;\n    }\n\n    const inputValues = [];\n    const inputMasks = [];\n    const tensorsToDispose = [];\n    let maskExists = false;\n\n    for (const input of symbolic.inputs) {\n      const value = internalFeedDict.getValue(input);\n      const mask = internalFeedDict.getMask(input);\n      inputValues.push(value);\n      inputMasks.push(mask);\n\n      if (mask != null) {\n        maskExists = true;\n      }\n\n      if (!training) {\n        recipientCounts[input.name]--;\n\n        if (recipientCounts[input.name] === 0 && !feedDict.hasKey(input) && outputNames.indexOf(input.name) === -1 && !value.isDisposed && input.sourceLayer.stateful !== true) {\n          tensorsToDispose.push(value);\n        }\n      }\n    }\n\n    if (maskExists) {\n      kwargs = kwargs || {};\n      kwargs['mask'] = inputMasks[0];\n    }\n\n    const outputTensors = toList(srcLayer.apply(inputValues, kwargs));\n    let outputMask = null;\n\n    if (srcLayer.supportsMasking) {\n      outputMask = srcLayer.computeMask(inputValues, inputMasks);\n    }\n\n    const layerOutputs = getNodeOutputs(symbolic);\n    const outputSymbolicTensors = Array.isArray(layerOutputs) ? layerOutputs : [layerOutputs];\n\n    for (let i = 0; i < outputSymbolicTensors.length; ++i) {\n      if (!internalFeedDict.hasKey(outputSymbolicTensors[i])) {\n        internalFeedDict.add(outputSymbolicTensors[i], outputTensors[i], Array.isArray(outputMask) ? outputMask[0] : outputMask);\n      }\n\n      const index = outputNames.indexOf(outputSymbolicTensors[i].name);\n\n      if (index !== -1) {\n        finalOutputs[index] = outputTensors[i];\n      }\n    }\n\n    if (!training) {\n      // Clean up Tensors that are no longer needed.\n      dispose(tensorsToDispose);\n    }\n  } // NOTE(cais): Unlike intermediate tensors, we don't discard mask\n  // tensors as we go, because these tensors are sometimes passed over a\n  // series of mutliple layers, i.e., not obeying the immediate input\n  // relations in the graph. If this becomes a memory-usage concern,\n  // we can improve this in the future.\n\n\n  internalFeedDict.disposeMasks();\n  return arrayFetches ? finalOutputs : finalOutputs[0];\n}\n/**\n * Sort the `SymbolicTensor`s topologically, for an array of fetches.\n *\n * This function calls getTopologicalSortAndRecipientCountsForOneFetch and\n * merges their results.\n *\n * @param fetch The array of fetches requested. Must be a non-empty array.\n * @param feedDict The dictionary of fed values.\n * @returns sorted: Topologically-sorted array of SymbolicTensors.\n *   recipientCounts: Recipient counts for all SymbolicTensors in `sorted`.\n */\n\nfunction getTopologicalSortAndRecipientCounts(fetches, feedDict) {\n  util.assert(fetches != null && fetches.length > 0, () => `Expected at least one fetch, got none`);\n  let finalSorted = [];\n  let finalRecipientMap = {};\n\n  if (fetches.length === 1) {\n    // Special-casing 1 fetch for efficiency.\n    const out = getTopologicalSortAndRecipientCountsForOneFetch(fetches[0], feedDict);\n    finalSorted = out.sorted;\n    finalRecipientMap = out.recipientMap;\n  } else {\n    const visited = new Set();\n\n    for (const fetch of fetches) {\n      const {\n        sorted,\n        recipientMap\n      } = getTopologicalSortAndRecipientCountsForOneFetch(fetch, feedDict); // Merge sorted SymbolicTensor Arrays.\n\n      for (const symbolicTensor of sorted) {\n        if (!visited.has(symbolicTensor.name)) {\n          finalSorted.push(symbolicTensor);\n          visited.add(symbolicTensor.name);\n        }\n      } // Merge recipient maps.\n\n\n      for (const name in recipientMap) {\n        if (finalRecipientMap[name] == null) {\n          finalRecipientMap[name] = new Set();\n        }\n\n        recipientMap[name].forEach(recipient => finalRecipientMap[name].add(recipient));\n      }\n    }\n  }\n\n  return {\n    sorted: finalSorted,\n    recipientCounts: recipientMap2Counts(finalRecipientMap)\n  };\n}\n\nfunction recipientMap2Counts(recipientMap) {\n  const recipientCounts = {};\n\n  for (const name in recipientMap) {\n    recipientCounts[name] = recipientMap[name].size;\n  }\n\n  return recipientCounts;\n}\n/**\n * Sort the `SymbolicTensor`s topologically, for a single fetch.\n *\n * This helper function processes the upstream SymbolicTensors of a single\n * fetch.\n *\n * @param fetch The single fetch requested.\n * @param feedDict The dictionary of fed values.\n * @returns sorted: Topologically-sorted array of SymbolicTensors.\n *   recipientMap: Recipient names for all SymbolicTensors in `sorted`.\n */\n\n\nexport function getTopologicalSortAndRecipientCountsForOneFetch(fetch, feedDict) {\n  const visited = new Set();\n  const sorted = [];\n  const recipientMap = {}; // Put keys of the feedDict into visited first, so they don't have to be\n  // walked. This is needed in case where there are feeds for intermediate\n  // SymbolicTensors of the graph.\n\n  for (const key of feedDict.names()) {\n    visited.add(key);\n  }\n\n  const stack = [];\n  const marks = []; // Initial population of stack and marks.\n\n  stack.push(fetch);\n\n  while (stack.length > 0) {\n    const top = stack[stack.length - 1];\n\n    if (visited.has(top.name)) {\n      stack.pop();\n      continue;\n    }\n\n    const topIsMarked = marks[marks.length - 1] === stack.length - 1;\n\n    if (top.inputs.length === 0 || topIsMarked) {\n      // Input SymbolicTensor or all children have been visited.\n      stack.pop();\n      sorted.push(top);\n      visited.add(top.name);\n\n      if (topIsMarked) {\n        marks.pop();\n      }\n    } else {\n      // A non-input SymbolicTensor whose upstream SymbolicTensors haven't\n      // been visited yet. Push them onto the stack.\n      marks.push(stack.length - 1);\n\n      for (const input of top.inputs) {\n        // Increment the recipient count. Note that this needs to happen\n        // regardless of whether the SymbolicTensor has been visited before.\n        if (recipientMap[input.name] == null) {\n          recipientMap[input.name] = new Set();\n        }\n\n        recipientMap[input.name].add(top.name);\n\n        if (visited.has(input.name)) {\n          continue; // Avoid repeated visits to the same SymbolicTensor.\n        }\n\n        stack.push(input);\n      }\n    }\n  }\n\n  return {\n    sorted,\n    recipientMap\n  };\n}\n/**\n * Get the symbolic output tensors of the node to which a given fetch belongs.\n * @param fetch The fetched symbolic tensor.\n * @returns The Array of symbolic tensors output by the node to which `fetch`\n *   belongs.\n */\n\nfunction getNodeOutputs(fetch) {\n  let layerOutputs;\n\n  if (fetch.sourceLayer.inboundNodes.length === 1) {\n    layerOutputs = fetch.sourceLayer.output;\n  } else {\n    let nodeIndex = null;\n\n    for (let i = 0; i < fetch.sourceLayer.inboundNodes.length; ++i) {\n      for (const outputTensor of fetch.sourceLayer.inboundNodes[i].outputTensors) {\n        if (outputTensor.id === fetch.id) {\n          nodeIndex = i;\n          break;\n        }\n      }\n    }\n\n    layerOutputs = fetch.sourceLayer.getOutputAt(nodeIndex);\n  }\n\n  return layerOutputs;\n}","map":{"version":3,"names":["cast","dispose","memory","util","ValueError","LruCache","toList","InputLayer","SymbolicTensor","assertFeedCompatibility","key","val","dtype","err","name","FeedDict","constructor","feeds","id2Value","id2Mask","name2Id","id","feed","add","value","mask","addFeed","hasKey","names","Object","keys","getValue","getMask","disposeMasks","cachedSorted","cachedRecipientCounts","updateCacheMaxEntries","maxEntries","setMaxEntries","execute","fetches","feedDict","kwargs","probe","training","arrayFetches","Array","isArray","fetchArray","outputNames","map","t","finalOutputs","feedNames","outputName","indexOf","push","maxNumTensors","Infinity","minNumTensors","fetchAndFeedKey","join","sort","sorted","get","recipientCounts","out","getTopologicalSortAndRecipientCounts","put","assign","internalFeedDict","i","length","numTensors","symbolic","srcLayer","sourceLayer","inputValues","inputMasks","tensorsToDispose","maskExists","input","inputs","isDisposed","stateful","outputTensors","apply","outputMask","supportsMasking","computeMask","layerOutputs","getNodeOutputs","outputSymbolicTensors","index","assert","finalSorted","finalRecipientMap","getTopologicalSortAndRecipientCountsForOneFetch","recipientMap","visited","Set","fetch","symbolicTensor","has","forEach","recipient","recipientMap2Counts","size","stack","marks","top","pop","topIsMarked","inboundNodes","output","nodeIndex","outputTensor","getOutputAt"],"sources":["C:/Users/Anke/Documents/Feelix documents/Feelix2.0-dev/Feelix v2/node_modules/@tensorflow/tfjs-layers/dist/engine/executor.js"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n * Executor: Evaluates SymbolicTensor based on feeds.\n */\nimport { cast, dispose, memory, util } from '@tensorflow/tfjs-core';\nimport { ValueError } from '../errors';\nimport { LruCache } from '../utils/executor_utils';\nimport { toList } from '../utils/generic_utils';\nimport { InputLayer } from './input_layer';\nimport { SymbolicTensor } from './topology';\n/**\n * Helper function to check the dtype and shape compatibility of a feed value.\n */\nfunction assertFeedCompatibility(key, val) {\n    // Check dtype compatibility.\n    if (key.dtype == null || key.dtype === val.dtype) {\n        //  a.  If types match, return val tensor as is.\n        return val;\n    }\n    try {\n        //  b. Attempt to convert to expected type.\n        return cast(val, key.dtype);\n    }\n    catch (err) {\n        //  c. If conversion fails, return helpful error.\n        throw new ValueError(`The dtype of the feed (${val.dtype}) can not be cast to the dtype ` +\n            `of the key '${key.name}' (${key.dtype}).`);\n    }\n}\n/**\n * FeedDict: A mapping from unique SymbolicTensors to feed values for them.\n * A feed value is a concrete value represented as an `Tensor`.\n */\nexport class FeedDict {\n    /**\n     * Constructor, optionally does copy-construction.\n     * @param feeds An Array of `Feed`s, or another `FeedDict`, in which case\n     *   copy-construction will be performed.\n     */\n    constructor(feeds) {\n        this.id2Value = {};\n        this.id2Mask = {};\n        this.name2Id = {};\n        if (feeds instanceof FeedDict) {\n            for (const id in feeds.id2Value) {\n                this.id2Value[id] = feeds.id2Value[id];\n                if (id in feeds.id2Mask) {\n                    this.id2Mask[id] = feeds.id2Mask[id];\n                }\n            }\n        }\n        else {\n            if (feeds == null) {\n                return;\n            }\n            for (const feed of feeds) {\n                this.add(feed.key, feed.value);\n            }\n        }\n    }\n    /**\n     * Add a key-value pair to the FeedDict.\n     *\n     * @param key The key of the feed.\n     * @param value The value of the tensor feed.\n     * @param mask The value of the mask feed (optional).\n     * @returns This `FeedDict`.\n     * @throws ValueError: If the key `SymbolicTensor` already exists in the\n     *   `FeedDict`.\n     */\n    add(key, value, mask) {\n        if (this.id2Value[key.id] == null) {\n            this.id2Value[key.id] = assertFeedCompatibility(key, value);\n            this.name2Id[key.name] = key.id;\n            if (mask != null) {\n                this.id2Mask[key.id] = mask;\n            }\n        }\n        else {\n            throw new ValueError(`Duplicate key: name=${key.name}, id=${key.id}`);\n        }\n        return this;\n    }\n    /**\n     * Add a Feed to the FeedDict.\n     * @param feed The new `Feed` to add.\n     * @returns This `FeedDict`.\n     */\n    addFeed(feed) {\n        this.add(feed.key, feed.value);\n    }\n    /**\n     * Probe whether a key already exists in the FeedDict.\n     * @param key\n     */\n    hasKey(key) {\n        return this.id2Value[key.id] != null;\n    }\n    /**\n     * Get all the SymbolicTensor available in this FeedDict.\n     */\n    names() {\n        return Object.keys(this.name2Id);\n    }\n    /**\n     * Get the feed value for given key.\n     * @param key The SymbolicTensor, or its name (as a string), of which the\n     *     value is sought.\n     * @returns If `key` exists, the corresponding feed value.\n     * @throws ValueError: If `key` does not exist in this `FeedDict`.\n     */\n    getValue(key) {\n        if (key instanceof SymbolicTensor) {\n            if (this.id2Value[key.id] == null) {\n                throw new ValueError(`Nonexistent key: ${key.name}`);\n            }\n            else {\n                return this.id2Value[key.id];\n            }\n        }\n        else {\n            const id = this.name2Id[key];\n            if (id == null) {\n                throw new ValueError(`Feed dict has no SymbolicTensor name: ${key}`);\n            }\n            return this.id2Value[id];\n        }\n    }\n    /**\n     * Get the feed mask for given key.\n     * @param key The SymbolicTensor, or its name (as a string), of which the\n     *     value is sought.\n     * @returns If `key` exists, the corresponding feed mask.\n     * @throws ValueError: If `key` does not exist in this `FeedDict`.\n     */\n    getMask(key) {\n        if (key instanceof SymbolicTensor) {\n            if (this.id2Value[key.id] == null) {\n                throw new ValueError(`Nonexistent key: ${key.name}`);\n            }\n            else {\n                return this.id2Mask[key.id];\n            }\n        }\n        else {\n            const id = this.name2Id[key];\n            if (id == null) {\n                throw new ValueError(`Feed dict has no SymbolicTensor name: ${key}`);\n            }\n            return this.id2Mask[id];\n        }\n    }\n    /** Dispose all mask Tensors held by this object. */\n    disposeMasks() {\n        if (this.id2Mask != null) {\n            dispose(this.id2Mask);\n        }\n    }\n}\n// Cache for topologically sorted SymbolicTensors for given execution\n// targets (i.e., fetches).\nexport const cachedSorted = new LruCache();\n// Cache for recipient count maps for given execution targets (i.e., fetches).\nexport const cachedRecipientCounts = new LruCache();\nexport function updateCacheMaxEntries(maxEntries) {\n    if (cachedSorted != null) {\n        cachedSorted.setMaxEntries(maxEntries);\n    }\n    if (cachedRecipientCounts != null) {\n        cachedRecipientCounts.setMaxEntries(maxEntries);\n    }\n}\n/**\n * Execute a SymbolicTensor by using concrete feed values.\n *\n * A `SymbolicTensor` object is a node in a computation graph of TF.js\n * Layers. The object is backed by a source layer and input\n * `SymbolicTensor`s to the source layer. This method evaluates\n * the `call()` method of the source layer, using concrete values of the\n * inputs obtained from either\n * * `feedDict`, if the input key exists in `feedDict`, or else,\n * * a recursive call to `execute()` itself.\n *\n * @param x: The `SymbolicTensor` to execute.\n * @param feedDict: The feed values, as base condition of the recursion.\n *   execution.\n * @param kwargs: Optional keyword arguments.\n * @param probe: A probe object (of interface `ExecutionProbe`) used for\n *   testing memory footprint of `execute` calls.\n * @returns Result of the execution.\n * @throws ValueError: If any `SymbolicTensor`s from `InputLayer`s\n *   encountered during the execution lacks a feed value in `feedDict`.\n */\nexport function execute(fetches, feedDict, kwargs, probe) {\n    const training = kwargs == null ? false : kwargs['training'];\n    const arrayFetches = Array.isArray(fetches);\n    const fetchArray = arrayFetches ? fetches : [fetches];\n    const outputNames = fetchArray.map(t => t.name);\n    const finalOutputs = [];\n    const feedNames = feedDict.names();\n    for (const outputName of outputNames) {\n        if (feedNames.indexOf(outputName) !== -1) {\n            finalOutputs.push(feedDict.getValue(outputName));\n        }\n        else {\n            finalOutputs.push(null);\n        }\n    }\n    if (probe != null) {\n        // For optional probing of memory footprint during execution.\n        probe.maxNumTensors = -Infinity;\n        probe.minNumTensors = Infinity;\n    }\n    // Check cache.\n    const fetchAndFeedKey = outputNames.join(',') + '|' + feedDict.names().sort().join(',');\n    let sorted = cachedSorted.get(fetchAndFeedKey);\n    let recipientCounts;\n    if (sorted == null) {\n        // Cache doesn't contain the desired combination of fetches. Compute\n        // topological sort for the combination for the first time.\n        const out = getTopologicalSortAndRecipientCounts(fetchArray, feedDict);\n        sorted = out.sorted;\n        recipientCounts = out.recipientCounts;\n        // Store results in cache for future use.\n        cachedSorted.put(fetchAndFeedKey, sorted);\n        cachedRecipientCounts.put(fetchAndFeedKey, recipientCounts);\n    }\n    recipientCounts = {};\n    if (!training) {\n        Object.assign(recipientCounts, cachedRecipientCounts.get(fetchAndFeedKey));\n    }\n    const internalFeedDict = new FeedDict(feedDict);\n    // Start iterative execution on the topologically-sorted SymbolicTensors.\n    for (let i = 0; i < sorted.length; ++i) {\n        if (probe != null) {\n            // For optional probing of memory usage during execution.\n            const numTensors = memory().numTensors;\n            if (numTensors > probe.maxNumTensors) {\n                probe.maxNumTensors = numTensors;\n            }\n            if (numTensors < probe.minNumTensors) {\n                probe.minNumTensors = numTensors;\n            }\n        }\n        const symbolic = sorted[i];\n        const srcLayer = symbolic.sourceLayer;\n        if (srcLayer instanceof InputLayer) {\n            continue;\n        }\n        const inputValues = [];\n        const inputMasks = [];\n        const tensorsToDispose = [];\n        let maskExists = false;\n        for (const input of symbolic.inputs) {\n            const value = internalFeedDict.getValue(input);\n            const mask = internalFeedDict.getMask(input);\n            inputValues.push(value);\n            inputMasks.push(mask);\n            if (mask != null) {\n                maskExists = true;\n            }\n            if (!training) {\n                recipientCounts[input.name]--;\n                if (recipientCounts[input.name] === 0 && !feedDict.hasKey(input) &&\n                    outputNames.indexOf(input.name) === -1 && !value.isDisposed &&\n                    input.sourceLayer.stateful !== true) {\n                    tensorsToDispose.push(value);\n                }\n            }\n        }\n        if (maskExists) {\n            kwargs = kwargs || {};\n            kwargs['mask'] = inputMasks[0];\n        }\n        const outputTensors = toList(srcLayer.apply(inputValues, kwargs));\n        let outputMask = null;\n        if (srcLayer.supportsMasking) {\n            outputMask = srcLayer.computeMask(inputValues, inputMasks);\n        }\n        const layerOutputs = getNodeOutputs(symbolic);\n        const outputSymbolicTensors = Array.isArray(layerOutputs) ? layerOutputs : [layerOutputs];\n        for (let i = 0; i < outputSymbolicTensors.length; ++i) {\n            if (!internalFeedDict.hasKey(outputSymbolicTensors[i])) {\n                internalFeedDict.add(outputSymbolicTensors[i], outputTensors[i], Array.isArray(outputMask) ? outputMask[0] : outputMask);\n            }\n            const index = outputNames.indexOf(outputSymbolicTensors[i].name);\n            if (index !== -1) {\n                finalOutputs[index] = outputTensors[i];\n            }\n        }\n        if (!training) {\n            // Clean up Tensors that are no longer needed.\n            dispose(tensorsToDispose);\n        }\n    }\n    // NOTE(cais): Unlike intermediate tensors, we don't discard mask\n    // tensors as we go, because these tensors are sometimes passed over a\n    // series of mutliple layers, i.e., not obeying the immediate input\n    // relations in the graph. If this becomes a memory-usage concern,\n    // we can improve this in the future.\n    internalFeedDict.disposeMasks();\n    return arrayFetches ? finalOutputs : finalOutputs[0];\n}\n/**\n * Sort the `SymbolicTensor`s topologically, for an array of fetches.\n *\n * This function calls getTopologicalSortAndRecipientCountsForOneFetch and\n * merges their results.\n *\n * @param fetch The array of fetches requested. Must be a non-empty array.\n * @param feedDict The dictionary of fed values.\n * @returns sorted: Topologically-sorted array of SymbolicTensors.\n *   recipientCounts: Recipient counts for all SymbolicTensors in `sorted`.\n */\nfunction getTopologicalSortAndRecipientCounts(fetches, feedDict) {\n    util.assert(fetches != null && fetches.length > 0, () => `Expected at least one fetch, got none`);\n    let finalSorted = [];\n    let finalRecipientMap = {};\n    if (fetches.length === 1) {\n        // Special-casing 1 fetch for efficiency.\n        const out = getTopologicalSortAndRecipientCountsForOneFetch(fetches[0], feedDict);\n        finalSorted = out.sorted;\n        finalRecipientMap = out.recipientMap;\n    }\n    else {\n        const visited = new Set();\n        for (const fetch of fetches) {\n            const { sorted, recipientMap } = getTopologicalSortAndRecipientCountsForOneFetch(fetch, feedDict);\n            // Merge sorted SymbolicTensor Arrays.\n            for (const symbolicTensor of sorted) {\n                if (!visited.has(symbolicTensor.name)) {\n                    finalSorted.push(symbolicTensor);\n                    visited.add(symbolicTensor.name);\n                }\n            }\n            // Merge recipient maps.\n            for (const name in recipientMap) {\n                if (finalRecipientMap[name] == null) {\n                    finalRecipientMap[name] = new Set();\n                }\n                recipientMap[name].forEach(recipient => finalRecipientMap[name].add(recipient));\n            }\n        }\n    }\n    return {\n        sorted: finalSorted,\n        recipientCounts: recipientMap2Counts(finalRecipientMap)\n    };\n}\nfunction recipientMap2Counts(recipientMap) {\n    const recipientCounts = {};\n    for (const name in recipientMap) {\n        recipientCounts[name] = recipientMap[name].size;\n    }\n    return recipientCounts;\n}\n/**\n * Sort the `SymbolicTensor`s topologically, for a single fetch.\n *\n * This helper function processes the upstream SymbolicTensors of a single\n * fetch.\n *\n * @param fetch The single fetch requested.\n * @param feedDict The dictionary of fed values.\n * @returns sorted: Topologically-sorted array of SymbolicTensors.\n *   recipientMap: Recipient names for all SymbolicTensors in `sorted`.\n */\nexport function getTopologicalSortAndRecipientCountsForOneFetch(fetch, feedDict) {\n    const visited = new Set();\n    const sorted = [];\n    const recipientMap = {};\n    // Put keys of the feedDict into visited first, so they don't have to be\n    // walked. This is needed in case where there are feeds for intermediate\n    // SymbolicTensors of the graph.\n    for (const key of feedDict.names()) {\n        visited.add(key);\n    }\n    const stack = [];\n    const marks = [];\n    // Initial population of stack and marks.\n    stack.push(fetch);\n    while (stack.length > 0) {\n        const top = stack[stack.length - 1];\n        if (visited.has(top.name)) {\n            stack.pop();\n            continue;\n        }\n        const topIsMarked = marks[marks.length - 1] === stack.length - 1;\n        if (top.inputs.length === 0 || topIsMarked) {\n            // Input SymbolicTensor or all children have been visited.\n            stack.pop();\n            sorted.push(top);\n            visited.add(top.name);\n            if (topIsMarked) {\n                marks.pop();\n            }\n        }\n        else {\n            // A non-input SymbolicTensor whose upstream SymbolicTensors haven't\n            // been visited yet. Push them onto the stack.\n            marks.push(stack.length - 1);\n            for (const input of top.inputs) {\n                // Increment the recipient count. Note that this needs to happen\n                // regardless of whether the SymbolicTensor has been visited before.\n                if (recipientMap[input.name] == null) {\n                    recipientMap[input.name] = new Set();\n                }\n                recipientMap[input.name].add(top.name);\n                if (visited.has(input.name)) {\n                    continue; // Avoid repeated visits to the same SymbolicTensor.\n                }\n                stack.push(input);\n            }\n        }\n    }\n    return { sorted, recipientMap };\n}\n/**\n * Get the symbolic output tensors of the node to which a given fetch belongs.\n * @param fetch The fetched symbolic tensor.\n * @returns The Array of symbolic tensors output by the node to which `fetch`\n *   belongs.\n */\nfunction getNodeOutputs(fetch) {\n    let layerOutputs;\n    if (fetch.sourceLayer.inboundNodes.length === 1) {\n        layerOutputs = fetch.sourceLayer.output;\n    }\n    else {\n        let nodeIndex = null;\n        for (let i = 0; i < fetch.sourceLayer.inboundNodes.length; ++i) {\n            for (const outputTensor of fetch.sourceLayer.inboundNodes[i]\n                .outputTensors) {\n                if (outputTensor.id === fetch.id) {\n                    nodeIndex = i;\n                    break;\n                }\n            }\n        }\n        layerOutputs = fetch.sourceLayer.getOutputAt(nodeIndex);\n    }\n    return layerOutputs;\n}\n"],"mappings":"AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AACA;AACA;AACA;AACA,SAASA,IAAT,EAAeC,OAAf,EAAwBC,MAAxB,EAAgCC,IAAhC,QAA4C,uBAA5C;AACA,SAASC,UAAT,QAA2B,WAA3B;AACA,SAASC,QAAT,QAAyB,yBAAzB;AACA,SAASC,MAAT,QAAuB,wBAAvB;AACA,SAASC,UAAT,QAA2B,eAA3B;AACA,SAASC,cAAT,QAA+B,YAA/B;AACA;AACA;AACA;;AACA,SAASC,uBAAT,CAAiCC,GAAjC,EAAsCC,GAAtC,EAA2C;EACvC;EACA,IAAID,GAAG,CAACE,KAAJ,IAAa,IAAb,IAAqBF,GAAG,CAACE,KAAJ,KAAcD,GAAG,CAACC,KAA3C,EAAkD;IAC9C;IACA,OAAOD,GAAP;EACH;;EACD,IAAI;IACA;IACA,OAAOX,IAAI,CAACW,GAAD,EAAMD,GAAG,CAACE,KAAV,CAAX;EACH,CAHD,CAIA,OAAOC,GAAP,EAAY;IACR;IACA,MAAM,IAAIT,UAAJ,CAAgB,0BAAyBO,GAAG,CAACC,KAAM,iCAApC,GAChB,eAAcF,GAAG,CAACI,IAAK,MAAKJ,GAAG,CAACE,KAAM,IADrC,CAAN;EAEH;AACJ;AACD;AACA;AACA;AACA;;;AACA,OAAO,MAAMG,QAAN,CAAe;EAClB;AACJ;AACA;AACA;AACA;EACIC,WAAW,CAACC,KAAD,EAAQ;IACf,KAAKC,QAAL,GAAgB,EAAhB;IACA,KAAKC,OAAL,GAAe,EAAf;IACA,KAAKC,OAAL,GAAe,EAAf;;IACA,IAAIH,KAAK,YAAYF,QAArB,EAA+B;MAC3B,KAAK,MAAMM,EAAX,IAAiBJ,KAAK,CAACC,QAAvB,EAAiC;QAC7B,KAAKA,QAAL,CAAcG,EAAd,IAAoBJ,KAAK,CAACC,QAAN,CAAeG,EAAf,CAApB;;QACA,IAAIA,EAAE,IAAIJ,KAAK,CAACE,OAAhB,EAAyB;UACrB,KAAKA,OAAL,CAAaE,EAAb,IAAmBJ,KAAK,CAACE,OAAN,CAAcE,EAAd,CAAnB;QACH;MACJ;IACJ,CAPD,MAQK;MACD,IAAIJ,KAAK,IAAI,IAAb,EAAmB;QACf;MACH;;MACD,KAAK,MAAMK,IAAX,IAAmBL,KAAnB,EAA0B;QACtB,KAAKM,GAAL,CAASD,IAAI,CAACZ,GAAd,EAAmBY,IAAI,CAACE,KAAxB;MACH;IACJ;EACJ;EACD;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;EACID,GAAG,CAACb,GAAD,EAAMc,KAAN,EAAaC,IAAb,EAAmB;IAClB,IAAI,KAAKP,QAAL,CAAcR,GAAG,CAACW,EAAlB,KAAyB,IAA7B,EAAmC;MAC/B,KAAKH,QAAL,CAAcR,GAAG,CAACW,EAAlB,IAAwBZ,uBAAuB,CAACC,GAAD,EAAMc,KAAN,CAA/C;MACA,KAAKJ,OAAL,CAAaV,GAAG,CAACI,IAAjB,IAAyBJ,GAAG,CAACW,EAA7B;;MACA,IAAII,IAAI,IAAI,IAAZ,EAAkB;QACd,KAAKN,OAAL,CAAaT,GAAG,CAACW,EAAjB,IAAuBI,IAAvB;MACH;IACJ,CAND,MAOK;MACD,MAAM,IAAIrB,UAAJ,CAAgB,uBAAsBM,GAAG,CAACI,IAAK,QAAOJ,GAAG,CAACW,EAAG,EAA7D,CAAN;IACH;;IACD,OAAO,IAAP;EACH;EACD;AACJ;AACA;AACA;AACA;;;EACIK,OAAO,CAACJ,IAAD,EAAO;IACV,KAAKC,GAAL,CAASD,IAAI,CAACZ,GAAd,EAAmBY,IAAI,CAACE,KAAxB;EACH;EACD;AACJ;AACA;AACA;;;EACIG,MAAM,CAACjB,GAAD,EAAM;IACR,OAAO,KAAKQ,QAAL,CAAcR,GAAG,CAACW,EAAlB,KAAyB,IAAhC;EACH;EACD;AACJ;AACA;;;EACIO,KAAK,GAAG;IACJ,OAAOC,MAAM,CAACC,IAAP,CAAY,KAAKV,OAAjB,CAAP;EACH;EACD;AACJ;AACA;AACA;AACA;AACA;AACA;;;EACIW,QAAQ,CAACrB,GAAD,EAAM;IACV,IAAIA,GAAG,YAAYF,cAAnB,EAAmC;MAC/B,IAAI,KAAKU,QAAL,CAAcR,GAAG,CAACW,EAAlB,KAAyB,IAA7B,EAAmC;QAC/B,MAAM,IAAIjB,UAAJ,CAAgB,oBAAmBM,GAAG,CAACI,IAAK,EAA5C,CAAN;MACH,CAFD,MAGK;QACD,OAAO,KAAKI,QAAL,CAAcR,GAAG,CAACW,EAAlB,CAAP;MACH;IACJ,CAPD,MAQK;MACD,MAAMA,EAAE,GAAG,KAAKD,OAAL,CAAaV,GAAb,CAAX;;MACA,IAAIW,EAAE,IAAI,IAAV,EAAgB;QACZ,MAAM,IAAIjB,UAAJ,CAAgB,yCAAwCM,GAAI,EAA5D,CAAN;MACH;;MACD,OAAO,KAAKQ,QAAL,CAAcG,EAAd,CAAP;IACH;EACJ;EACD;AACJ;AACA;AACA;AACA;AACA;AACA;;;EACIW,OAAO,CAACtB,GAAD,EAAM;IACT,IAAIA,GAAG,YAAYF,cAAnB,EAAmC;MAC/B,IAAI,KAAKU,QAAL,CAAcR,GAAG,CAACW,EAAlB,KAAyB,IAA7B,EAAmC;QAC/B,MAAM,IAAIjB,UAAJ,CAAgB,oBAAmBM,GAAG,CAACI,IAAK,EAA5C,CAAN;MACH,CAFD,MAGK;QACD,OAAO,KAAKK,OAAL,CAAaT,GAAG,CAACW,EAAjB,CAAP;MACH;IACJ,CAPD,MAQK;MACD,MAAMA,EAAE,GAAG,KAAKD,OAAL,CAAaV,GAAb,CAAX;;MACA,IAAIW,EAAE,IAAI,IAAV,EAAgB;QACZ,MAAM,IAAIjB,UAAJ,CAAgB,yCAAwCM,GAAI,EAA5D,CAAN;MACH;;MACD,OAAO,KAAKS,OAAL,CAAaE,EAAb,CAAP;IACH;EACJ;EACD;;;EACAY,YAAY,GAAG;IACX,IAAI,KAAKd,OAAL,IAAgB,IAApB,EAA0B;MACtBlB,OAAO,CAAC,KAAKkB,OAAN,CAAP;IACH;EACJ;;AA5HiB,C,CA8HtB;AACA;;AACA,OAAO,MAAMe,YAAY,GAAG,IAAI7B,QAAJ,EAArB,C,CACP;;AACA,OAAO,MAAM8B,qBAAqB,GAAG,IAAI9B,QAAJ,EAA9B;AACP,OAAO,SAAS+B,qBAAT,CAA+BC,UAA/B,EAA2C;EAC9C,IAAIH,YAAY,IAAI,IAApB,EAA0B;IACtBA,YAAY,CAACI,aAAb,CAA2BD,UAA3B;EACH;;EACD,IAAIF,qBAAqB,IAAI,IAA7B,EAAmC;IAC/BA,qBAAqB,CAACG,aAAtB,CAAoCD,UAApC;EACH;AACJ;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AACA,OAAO,SAASE,OAAT,CAAiBC,OAAjB,EAA0BC,QAA1B,EAAoCC,MAApC,EAA4CC,KAA5C,EAAmD;EACtD,MAAMC,QAAQ,GAAGF,MAAM,IAAI,IAAV,GAAiB,KAAjB,GAAyBA,MAAM,CAAC,UAAD,CAAhD;EACA,MAAMG,YAAY,GAAGC,KAAK,CAACC,OAAN,CAAcP,OAAd,CAArB;EACA,MAAMQ,UAAU,GAAGH,YAAY,GAAGL,OAAH,GAAa,CAACA,OAAD,CAA5C;EACA,MAAMS,WAAW,GAAGD,UAAU,CAACE,GAAX,CAAeC,CAAC,IAAIA,CAAC,CAACrC,IAAtB,CAApB;EACA,MAAMsC,YAAY,GAAG,EAArB;EACA,MAAMC,SAAS,GAAGZ,QAAQ,CAACb,KAAT,EAAlB;;EACA,KAAK,MAAM0B,UAAX,IAAyBL,WAAzB,EAAsC;IAClC,IAAII,SAAS,CAACE,OAAV,CAAkBD,UAAlB,MAAkC,CAAC,CAAvC,EAA0C;MACtCF,YAAY,CAACI,IAAb,CAAkBf,QAAQ,CAACV,QAAT,CAAkBuB,UAAlB,CAAlB;IACH,CAFD,MAGK;MACDF,YAAY,CAACI,IAAb,CAAkB,IAAlB;IACH;EACJ;;EACD,IAAIb,KAAK,IAAI,IAAb,EAAmB;IACf;IACAA,KAAK,CAACc,aAAN,GAAsB,CAACC,QAAvB;IACAf,KAAK,CAACgB,aAAN,GAAsBD,QAAtB;EACH,CAnBqD,CAoBtD;;;EACA,MAAME,eAAe,GAAGX,WAAW,CAACY,IAAZ,CAAiB,GAAjB,IAAwB,GAAxB,GAA8BpB,QAAQ,CAACb,KAAT,GAAiBkC,IAAjB,GAAwBD,IAAxB,CAA6B,GAA7B,CAAtD;EACA,IAAIE,MAAM,GAAG7B,YAAY,CAAC8B,GAAb,CAAiBJ,eAAjB,CAAb;EACA,IAAIK,eAAJ;;EACA,IAAIF,MAAM,IAAI,IAAd,EAAoB;IAChB;IACA;IACA,MAAMG,GAAG,GAAGC,oCAAoC,CAACnB,UAAD,EAAaP,QAAb,CAAhD;IACAsB,MAAM,GAAGG,GAAG,CAACH,MAAb;IACAE,eAAe,GAAGC,GAAG,CAACD,eAAtB,CALgB,CAMhB;;IACA/B,YAAY,CAACkC,GAAb,CAAiBR,eAAjB,EAAkCG,MAAlC;IACA5B,qBAAqB,CAACiC,GAAtB,CAA0BR,eAA1B,EAA2CK,eAA3C;EACH;;EACDA,eAAe,GAAG,EAAlB;;EACA,IAAI,CAACrB,QAAL,EAAe;IACXf,MAAM,CAACwC,MAAP,CAAcJ,eAAd,EAA+B9B,qBAAqB,CAAC6B,GAAtB,CAA0BJ,eAA1B,CAA/B;EACH;;EACD,MAAMU,gBAAgB,GAAG,IAAIvD,QAAJ,CAAa0B,QAAb,CAAzB,CAtCsD,CAuCtD;;EACA,KAAK,IAAI8B,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGR,MAAM,CAACS,MAA3B,EAAmC,EAAED,CAArC,EAAwC;IACpC,IAAI5B,KAAK,IAAI,IAAb,EAAmB;MACf;MACA,MAAM8B,UAAU,GAAGvE,MAAM,GAAGuE,UAA5B;;MACA,IAAIA,UAAU,GAAG9B,KAAK,CAACc,aAAvB,EAAsC;QAClCd,KAAK,CAACc,aAAN,GAAsBgB,UAAtB;MACH;;MACD,IAAIA,UAAU,GAAG9B,KAAK,CAACgB,aAAvB,EAAsC;QAClChB,KAAK,CAACgB,aAAN,GAAsBc,UAAtB;MACH;IACJ;;IACD,MAAMC,QAAQ,GAAGX,MAAM,CAACQ,CAAD,CAAvB;IACA,MAAMI,QAAQ,GAAGD,QAAQ,CAACE,WAA1B;;IACA,IAAID,QAAQ,YAAYpE,UAAxB,EAAoC;MAChC;IACH;;IACD,MAAMsE,WAAW,GAAG,EAApB;IACA,MAAMC,UAAU,GAAG,EAAnB;IACA,MAAMC,gBAAgB,GAAG,EAAzB;IACA,IAAIC,UAAU,GAAG,KAAjB;;IACA,KAAK,MAAMC,KAAX,IAAoBP,QAAQ,CAACQ,MAA7B,EAAqC;MACjC,MAAM1D,KAAK,GAAG8C,gBAAgB,CAACvC,QAAjB,CAA0BkD,KAA1B,CAAd;MACA,MAAMxD,IAAI,GAAG6C,gBAAgB,CAACtC,OAAjB,CAAyBiD,KAAzB,CAAb;MACAJ,WAAW,CAACrB,IAAZ,CAAiBhC,KAAjB;MACAsD,UAAU,CAACtB,IAAX,CAAgB/B,IAAhB;;MACA,IAAIA,IAAI,IAAI,IAAZ,EAAkB;QACduD,UAAU,GAAG,IAAb;MACH;;MACD,IAAI,CAACpC,QAAL,EAAe;QACXqB,eAAe,CAACgB,KAAK,CAACnE,IAAP,CAAf;;QACA,IAAImD,eAAe,CAACgB,KAAK,CAACnE,IAAP,CAAf,KAAgC,CAAhC,IAAqC,CAAC2B,QAAQ,CAACd,MAAT,CAAgBsD,KAAhB,CAAtC,IACAhC,WAAW,CAACM,OAAZ,CAAoB0B,KAAK,CAACnE,IAA1B,MAAoC,CAAC,CADrC,IAC0C,CAACU,KAAK,CAAC2D,UADjD,IAEAF,KAAK,CAACL,WAAN,CAAkBQ,QAAlB,KAA+B,IAFnC,EAEyC;UACrCL,gBAAgB,CAACvB,IAAjB,CAAsBhC,KAAtB;QACH;MACJ;IACJ;;IACD,IAAIwD,UAAJ,EAAgB;MACZtC,MAAM,GAAGA,MAAM,IAAI,EAAnB;MACAA,MAAM,CAAC,MAAD,CAAN,GAAiBoC,UAAU,CAAC,CAAD,CAA3B;IACH;;IACD,MAAMO,aAAa,GAAG/E,MAAM,CAACqE,QAAQ,CAACW,KAAT,CAAeT,WAAf,EAA4BnC,MAA5B,CAAD,CAA5B;IACA,IAAI6C,UAAU,GAAG,IAAjB;;IACA,IAAIZ,QAAQ,CAACa,eAAb,EAA8B;MAC1BD,UAAU,GAAGZ,QAAQ,CAACc,WAAT,CAAqBZ,WAArB,EAAkCC,UAAlC,CAAb;IACH;;IACD,MAAMY,YAAY,GAAGC,cAAc,CAACjB,QAAD,CAAnC;IACA,MAAMkB,qBAAqB,GAAG9C,KAAK,CAACC,OAAN,CAAc2C,YAAd,IAA8BA,YAA9B,GAA6C,CAACA,YAAD,CAA3E;;IACA,KAAK,IAAInB,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGqB,qBAAqB,CAACpB,MAA1C,EAAkD,EAAED,CAApD,EAAuD;MACnD,IAAI,CAACD,gBAAgB,CAAC3C,MAAjB,CAAwBiE,qBAAqB,CAACrB,CAAD,CAA7C,CAAL,EAAwD;QACpDD,gBAAgB,CAAC/C,GAAjB,CAAqBqE,qBAAqB,CAACrB,CAAD,CAA1C,EAA+Cc,aAAa,CAACd,CAAD,CAA5D,EAAiEzB,KAAK,CAACC,OAAN,CAAcwC,UAAd,IAA4BA,UAAU,CAAC,CAAD,CAAtC,GAA4CA,UAA7G;MACH;;MACD,MAAMM,KAAK,GAAG5C,WAAW,CAACM,OAAZ,CAAoBqC,qBAAqB,CAACrB,CAAD,CAArB,CAAyBzD,IAA7C,CAAd;;MACA,IAAI+E,KAAK,KAAK,CAAC,CAAf,EAAkB;QACdzC,YAAY,CAACyC,KAAD,CAAZ,GAAsBR,aAAa,CAACd,CAAD,CAAnC;MACH;IACJ;;IACD,IAAI,CAAC3B,QAAL,EAAe;MACX;MACA3C,OAAO,CAAC8E,gBAAD,CAAP;IACH;EACJ,CArGqD,CAsGtD;EACA;EACA;EACA;EACA;;;EACAT,gBAAgB,CAACrC,YAAjB;EACA,OAAOY,YAAY,GAAGO,YAAH,GAAkBA,YAAY,CAAC,CAAD,CAAjD;AACH;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AACA,SAASe,oCAAT,CAA8C3B,OAA9C,EAAuDC,QAAvD,EAAiE;EAC7DtC,IAAI,CAAC2F,MAAL,CAAYtD,OAAO,IAAI,IAAX,IAAmBA,OAAO,CAACgC,MAAR,GAAiB,CAAhD,EAAmD,MAAO,uCAA1D;EACA,IAAIuB,WAAW,GAAG,EAAlB;EACA,IAAIC,iBAAiB,GAAG,EAAxB;;EACA,IAAIxD,OAAO,CAACgC,MAAR,KAAmB,CAAvB,EAA0B;IACtB;IACA,MAAMN,GAAG,GAAG+B,+CAA+C,CAACzD,OAAO,CAAC,CAAD,CAAR,EAAaC,QAAb,CAA3D;IACAsD,WAAW,GAAG7B,GAAG,CAACH,MAAlB;IACAiC,iBAAiB,GAAG9B,GAAG,CAACgC,YAAxB;EACH,CALD,MAMK;IACD,MAAMC,OAAO,GAAG,IAAIC,GAAJ,EAAhB;;IACA,KAAK,MAAMC,KAAX,IAAoB7D,OAApB,EAA6B;MACzB,MAAM;QAAEuB,MAAF;QAAUmC;MAAV,IAA2BD,+CAA+C,CAACI,KAAD,EAAQ5D,QAAR,CAAhF,CADyB,CAEzB;;MACA,KAAK,MAAM6D,cAAX,IAA6BvC,MAA7B,EAAqC;QACjC,IAAI,CAACoC,OAAO,CAACI,GAAR,CAAYD,cAAc,CAACxF,IAA3B,CAAL,EAAuC;UACnCiF,WAAW,CAACvC,IAAZ,CAAiB8C,cAAjB;UACAH,OAAO,CAAC5E,GAAR,CAAY+E,cAAc,CAACxF,IAA3B;QACH;MACJ,CARwB,CASzB;;;MACA,KAAK,MAAMA,IAAX,IAAmBoF,YAAnB,EAAiC;QAC7B,IAAIF,iBAAiB,CAAClF,IAAD,CAAjB,IAA2B,IAA/B,EAAqC;UACjCkF,iBAAiB,CAAClF,IAAD,CAAjB,GAA0B,IAAIsF,GAAJ,EAA1B;QACH;;QACDF,YAAY,CAACpF,IAAD,CAAZ,CAAmB0F,OAAnB,CAA2BC,SAAS,IAAIT,iBAAiB,CAAClF,IAAD,CAAjB,CAAwBS,GAAxB,CAA4BkF,SAA5B,CAAxC;MACH;IACJ;EACJ;;EACD,OAAO;IACH1C,MAAM,EAAEgC,WADL;IAEH9B,eAAe,EAAEyC,mBAAmB,CAACV,iBAAD;EAFjC,CAAP;AAIH;;AACD,SAASU,mBAAT,CAA6BR,YAA7B,EAA2C;EACvC,MAAMjC,eAAe,GAAG,EAAxB;;EACA,KAAK,MAAMnD,IAAX,IAAmBoF,YAAnB,EAAiC;IAC7BjC,eAAe,CAACnD,IAAD,CAAf,GAAwBoF,YAAY,CAACpF,IAAD,CAAZ,CAAmB6F,IAA3C;EACH;;EACD,OAAO1C,eAAP;AACH;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;AACA,OAAO,SAASgC,+CAAT,CAAyDI,KAAzD,EAAgE5D,QAAhE,EAA0E;EAC7E,MAAM0D,OAAO,GAAG,IAAIC,GAAJ,EAAhB;EACA,MAAMrC,MAAM,GAAG,EAAf;EACA,MAAMmC,YAAY,GAAG,EAArB,CAH6E,CAI7E;EACA;EACA;;EACA,KAAK,MAAMxF,GAAX,IAAkB+B,QAAQ,CAACb,KAAT,EAAlB,EAAoC;IAChCuE,OAAO,CAAC5E,GAAR,CAAYb,GAAZ;EACH;;EACD,MAAMkG,KAAK,GAAG,EAAd;EACA,MAAMC,KAAK,GAAG,EAAd,CAX6E,CAY7E;;EACAD,KAAK,CAACpD,IAAN,CAAW6C,KAAX;;EACA,OAAOO,KAAK,CAACpC,MAAN,GAAe,CAAtB,EAAyB;IACrB,MAAMsC,GAAG,GAAGF,KAAK,CAACA,KAAK,CAACpC,MAAN,GAAe,CAAhB,CAAjB;;IACA,IAAI2B,OAAO,CAACI,GAAR,CAAYO,GAAG,CAAChG,IAAhB,CAAJ,EAA2B;MACvB8F,KAAK,CAACG,GAAN;MACA;IACH;;IACD,MAAMC,WAAW,GAAGH,KAAK,CAACA,KAAK,CAACrC,MAAN,GAAe,CAAhB,CAAL,KAA4BoC,KAAK,CAACpC,MAAN,GAAe,CAA/D;;IACA,IAAIsC,GAAG,CAAC5B,MAAJ,CAAWV,MAAX,KAAsB,CAAtB,IAA2BwC,WAA/B,EAA4C;MACxC;MACAJ,KAAK,CAACG,GAAN;MACAhD,MAAM,CAACP,IAAP,CAAYsD,GAAZ;MACAX,OAAO,CAAC5E,GAAR,CAAYuF,GAAG,CAAChG,IAAhB;;MACA,IAAIkG,WAAJ,EAAiB;QACbH,KAAK,CAACE,GAAN;MACH;IACJ,CARD,MASK;MACD;MACA;MACAF,KAAK,CAACrD,IAAN,CAAWoD,KAAK,CAACpC,MAAN,GAAe,CAA1B;;MACA,KAAK,MAAMS,KAAX,IAAoB6B,GAAG,CAAC5B,MAAxB,EAAgC;QAC5B;QACA;QACA,IAAIgB,YAAY,CAACjB,KAAK,CAACnE,IAAP,CAAZ,IAA4B,IAAhC,EAAsC;UAClCoF,YAAY,CAACjB,KAAK,CAACnE,IAAP,CAAZ,GAA2B,IAAIsF,GAAJ,EAA3B;QACH;;QACDF,YAAY,CAACjB,KAAK,CAACnE,IAAP,CAAZ,CAAyBS,GAAzB,CAA6BuF,GAAG,CAAChG,IAAjC;;QACA,IAAIqF,OAAO,CAACI,GAAR,CAAYtB,KAAK,CAACnE,IAAlB,CAAJ,EAA6B;UACzB,SADyB,CACf;QACb;;QACD8F,KAAK,CAACpD,IAAN,CAAWyB,KAAX;MACH;IACJ;EACJ;;EACD,OAAO;IAAElB,MAAF;IAAUmC;EAAV,CAAP;AACH;AACD;AACA;AACA;AACA;AACA;AACA;;AACA,SAASP,cAAT,CAAwBU,KAAxB,EAA+B;EAC3B,IAAIX,YAAJ;;EACA,IAAIW,KAAK,CAACzB,WAAN,CAAkBqC,YAAlB,CAA+BzC,MAA/B,KAA0C,CAA9C,EAAiD;IAC7CkB,YAAY,GAAGW,KAAK,CAACzB,WAAN,CAAkBsC,MAAjC;EACH,CAFD,MAGK;IACD,IAAIC,SAAS,GAAG,IAAhB;;IACA,KAAK,IAAI5C,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAG8B,KAAK,CAACzB,WAAN,CAAkBqC,YAAlB,CAA+BzC,MAAnD,EAA2D,EAAED,CAA7D,EAAgE;MAC5D,KAAK,MAAM6C,YAAX,IAA2Bf,KAAK,CAACzB,WAAN,CAAkBqC,YAAlB,CAA+B1C,CAA/B,EACtBc,aADL,EACoB;QAChB,IAAI+B,YAAY,CAAC/F,EAAb,KAAoBgF,KAAK,CAAChF,EAA9B,EAAkC;UAC9B8F,SAAS,GAAG5C,CAAZ;UACA;QACH;MACJ;IACJ;;IACDmB,YAAY,GAAGW,KAAK,CAACzB,WAAN,CAAkByC,WAAlB,CAA8BF,SAA9B,CAAf;EACH;;EACD,OAAOzB,YAAP;AACH"},"metadata":{},"sourceType":"module"}