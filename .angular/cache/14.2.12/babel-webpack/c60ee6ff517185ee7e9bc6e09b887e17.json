{"ast":null,"code":"/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { convertToTensor } from '../../tensor_util_env';\nimport { assertShapesMatch } from '../../util';\nimport { abs } from '../abs';\nimport { add } from '../add';\nimport { exp } from '../exp';\nimport { log1p } from '../log1p';\nimport { Reduction } from '../loss_ops_utils';\nimport { mul } from '../mul';\nimport { neg } from '../neg';\nimport { op } from '../operation';\nimport { relu } from '../relu';\nimport { scalar } from '../scalar';\nimport { sub } from '../sub';\nimport { computeWeightedLoss } from './compute_weighted_loss';\n\nfunction sigmoidCrossEntropyWithLogits_(labels, logits) {\n  const $labels = convertToTensor(labels, 'labels', 'sigmoidCrossEntropyWithLogits');\n  const $logits = convertToTensor(logits, 'logits', 'sigmoidCrossEntropyWithLogits');\n  assertShapesMatch($labels.shape, $logits.shape, 'Error in sigmoidCrossEntropyWithLogits: ');\n  /**\n   * Implementation Details:\n   *\n   * For brevity, let `x = logits`, `z = labels`.  The logistic loss is\n   *     z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n   *   = z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))\n   *   = z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))\n   *   = z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))\n   *   = (1 - z) * x + log(1 + exp(-x))\n   *   = x - x * z + log(1 + exp(-x))\n   *\n   *   For x < 0, to avoid overflow in exp(-x), we reformulate the above\n   *     x - x * z + log(1 + exp(-x))\n   *   = log(exp(x)) - x * z + log(1 + exp(-x))\n   *   = - x * z + log(1 + exp(x))\n   *\n   * Hence, to ensure stability and avoid overflow, the implementation uses\n   * this equivalent formulation:\n   *     max(x, 0) - x * z + log(1 + exp(-abs(x)))\n   */\n\n  const maxOutput = relu($logits);\n  const outputXTarget = mul($logits, $labels);\n  const sigmoidOutput = log1p(exp(neg(abs($logits))));\n  return add(sub(maxOutput, outputXTarget), sigmoidOutput);\n}\n/**\n * Computes the sigmoid cross entropy loss between two tensors.\n *\n * If labelSmoothing is nonzero, smooth the labels towards 1/2:\n *\n *   newMulticlassLabels = multiclassLabels * (1 - labelSmoothing)\n *                         + 0.5 * labelSmoothing\n *\n * @param multiClassLabels The ground truth output tensor of shape\n * [batch_size, num_classes], same dimensions as 'predictions'.\n * @param logits The predicted outputs.\n * @param weights Tensor whose rank is either 0, or the same rank as\n *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions\n *    must be either `1`, or the same as the corresponding `losses`\n *    dimension).\n * @param labelSmoothing If greater than 0, then smooth the labels.\n * @param reduction Type of reduction to apply to loss. Should be of type\n *    `Reduction`\n *\n * @doc { heading: 'Training', subheading: 'Losses', namespace: 'losses' }\n */\n\n\nfunction sigmoidCrossEntropy_(multiClassLabels, logits, weights, labelSmoothing = 0, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\n  let $multiClassLabels = convertToTensor(multiClassLabels, 'multiClassLabels', 'sigmoidCrossEntropy');\n  const $logits = convertToTensor(logits, 'logits', 'sigmoidCrossEntropy');\n  let $weights = null;\n\n  if (weights != null) {\n    $weights = convertToTensor(weights, 'weights', 'sigmoidCrossEntropy');\n  }\n\n  assertShapesMatch($multiClassLabels.shape, $logits.shape, 'Error in sigmoidCrossEntropy: ');\n\n  if (labelSmoothing > 0) {\n    const labelSmoothingScalar = scalar(labelSmoothing);\n    const one = scalar(1);\n    const half = scalar(0.5);\n    $multiClassLabels = add(mul($multiClassLabels, sub(one, labelSmoothingScalar)), mul(half, labelSmoothingScalar));\n  }\n\n  const losses = sigmoidCrossEntropyWithLogits_($multiClassLabels, $logits);\n  return computeWeightedLoss(losses, $weights, reduction);\n}\n\nexport const sigmoidCrossEntropy = op({\n  sigmoidCrossEntropy_\n});","map":{"version":3,"names":["convertToTensor","assertShapesMatch","abs","add","exp","log1p","Reduction","mul","neg","op","relu","scalar","sub","computeWeightedLoss","sigmoidCrossEntropyWithLogits_","labels","logits","$labels","$logits","shape","maxOutput","outputXTarget","sigmoidOutput","sigmoidCrossEntropy_","multiClassLabels","weights","labelSmoothing","reduction","SUM_BY_NONZERO_WEIGHTS","$multiClassLabels","$weights","labelSmoothingScalar","one","half","losses","sigmoidCrossEntropy"],"sources":["C:/Users/Anke/Documents/Feelix documents/Feelix2.0-dev/Feelix v2/node_modules/@tensorflow/tfjs-core/dist/ops/losses/sigmoid_cross_entropy.js"],"sourcesContent":["/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { convertToTensor } from '../../tensor_util_env';\nimport { assertShapesMatch } from '../../util';\nimport { abs } from '../abs';\nimport { add } from '../add';\nimport { exp } from '../exp';\nimport { log1p } from '../log1p';\nimport { Reduction } from '../loss_ops_utils';\nimport { mul } from '../mul';\nimport { neg } from '../neg';\nimport { op } from '../operation';\nimport { relu } from '../relu';\nimport { scalar } from '../scalar';\nimport { sub } from '../sub';\nimport { computeWeightedLoss } from './compute_weighted_loss';\nfunction sigmoidCrossEntropyWithLogits_(labels, logits) {\n    const $labels = convertToTensor(labels, 'labels', 'sigmoidCrossEntropyWithLogits');\n    const $logits = convertToTensor(logits, 'logits', 'sigmoidCrossEntropyWithLogits');\n    assertShapesMatch($labels.shape, $logits.shape, 'Error in sigmoidCrossEntropyWithLogits: ');\n    /**\n     * Implementation Details:\n     *\n     * For brevity, let `x = logits`, `z = labels`.  The logistic loss is\n     *     z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n     *   = z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))\n     *   = z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))\n     *   = z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))\n     *   = (1 - z) * x + log(1 + exp(-x))\n     *   = x - x * z + log(1 + exp(-x))\n     *\n     *   For x < 0, to avoid overflow in exp(-x), we reformulate the above\n     *     x - x * z + log(1 + exp(-x))\n     *   = log(exp(x)) - x * z + log(1 + exp(-x))\n     *   = - x * z + log(1 + exp(x))\n     *\n     * Hence, to ensure stability and avoid overflow, the implementation uses\n     * this equivalent formulation:\n     *     max(x, 0) - x * z + log(1 + exp(-abs(x)))\n     */\n    const maxOutput = relu($logits);\n    const outputXTarget = mul($logits, $labels);\n    const sigmoidOutput = log1p(exp(neg(abs($logits))));\n    return add(sub(maxOutput, outputXTarget), sigmoidOutput);\n}\n/**\n * Computes the sigmoid cross entropy loss between two tensors.\n *\n * If labelSmoothing is nonzero, smooth the labels towards 1/2:\n *\n *   newMulticlassLabels = multiclassLabels * (1 - labelSmoothing)\n *                         + 0.5 * labelSmoothing\n *\n * @param multiClassLabels The ground truth output tensor of shape\n * [batch_size, num_classes], same dimensions as 'predictions'.\n * @param logits The predicted outputs.\n * @param weights Tensor whose rank is either 0, or the same rank as\n *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions\n *    must be either `1`, or the same as the corresponding `losses`\n *    dimension).\n * @param labelSmoothing If greater than 0, then smooth the labels.\n * @param reduction Type of reduction to apply to loss. Should be of type\n *    `Reduction`\n *\n * @doc { heading: 'Training', subheading: 'Losses', namespace: 'losses' }\n */\nfunction sigmoidCrossEntropy_(multiClassLabels, logits, weights, labelSmoothing = 0, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\n    let $multiClassLabels = convertToTensor(multiClassLabels, 'multiClassLabels', 'sigmoidCrossEntropy');\n    const $logits = convertToTensor(logits, 'logits', 'sigmoidCrossEntropy');\n    let $weights = null;\n    if (weights != null) {\n        $weights = convertToTensor(weights, 'weights', 'sigmoidCrossEntropy');\n    }\n    assertShapesMatch($multiClassLabels.shape, $logits.shape, 'Error in sigmoidCrossEntropy: ');\n    if (labelSmoothing > 0) {\n        const labelSmoothingScalar = scalar(labelSmoothing);\n        const one = scalar(1);\n        const half = scalar(0.5);\n        $multiClassLabels =\n            add(mul($multiClassLabels, sub(one, labelSmoothingScalar)), mul(half, labelSmoothingScalar));\n    }\n    const losses = sigmoidCrossEntropyWithLogits_($multiClassLabels, $logits);\n    return computeWeightedLoss(losses, $weights, reduction);\n}\nexport const sigmoidCrossEntropy = op({ sigmoidCrossEntropy_ });\n"],"mappings":"AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAASA,eAAT,QAAgC,uBAAhC;AACA,SAASC,iBAAT,QAAkC,YAAlC;AACA,SAASC,GAAT,QAAoB,QAApB;AACA,SAASC,GAAT,QAAoB,QAApB;AACA,SAASC,GAAT,QAAoB,QAApB;AACA,SAASC,KAAT,QAAsB,UAAtB;AACA,SAASC,SAAT,QAA0B,mBAA1B;AACA,SAASC,GAAT,QAAoB,QAApB;AACA,SAASC,GAAT,QAAoB,QAApB;AACA,SAASC,EAAT,QAAmB,cAAnB;AACA,SAASC,IAAT,QAAqB,SAArB;AACA,SAASC,MAAT,QAAuB,WAAvB;AACA,SAASC,GAAT,QAAoB,QAApB;AACA,SAASC,mBAAT,QAAoC,yBAApC;;AACA,SAASC,8BAAT,CAAwCC,MAAxC,EAAgDC,MAAhD,EAAwD;EACpD,MAAMC,OAAO,GAAGjB,eAAe,CAACe,MAAD,EAAS,QAAT,EAAmB,+BAAnB,CAA/B;EACA,MAAMG,OAAO,GAAGlB,eAAe,CAACgB,MAAD,EAAS,QAAT,EAAmB,+BAAnB,CAA/B;EACAf,iBAAiB,CAACgB,OAAO,CAACE,KAAT,EAAgBD,OAAO,CAACC,KAAxB,EAA+B,0CAA/B,CAAjB;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;EACI,MAAMC,SAAS,GAAGV,IAAI,CAACQ,OAAD,CAAtB;EACA,MAAMG,aAAa,GAAGd,GAAG,CAACW,OAAD,EAAUD,OAAV,CAAzB;EACA,MAAMK,aAAa,GAAGjB,KAAK,CAACD,GAAG,CAACI,GAAG,CAACN,GAAG,CAACgB,OAAD,CAAJ,CAAJ,CAAJ,CAA3B;EACA,OAAOf,GAAG,CAACS,GAAG,CAACQ,SAAD,EAAYC,aAAZ,CAAJ,EAAgCC,aAAhC,CAAV;AACH;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;AACA,SAASC,oBAAT,CAA8BC,gBAA9B,EAAgDR,MAAhD,EAAwDS,OAAxD,EAAiEC,cAAc,GAAG,CAAlF,EAAqFC,SAAS,GAAGrB,SAAS,CAACsB,sBAA3G,EAAmI;EAC/H,IAAIC,iBAAiB,GAAG7B,eAAe,CAACwB,gBAAD,EAAmB,kBAAnB,EAAuC,qBAAvC,CAAvC;EACA,MAAMN,OAAO,GAAGlB,eAAe,CAACgB,MAAD,EAAS,QAAT,EAAmB,qBAAnB,CAA/B;EACA,IAAIc,QAAQ,GAAG,IAAf;;EACA,IAAIL,OAAO,IAAI,IAAf,EAAqB;IACjBK,QAAQ,GAAG9B,eAAe,CAACyB,OAAD,EAAU,SAAV,EAAqB,qBAArB,CAA1B;EACH;;EACDxB,iBAAiB,CAAC4B,iBAAiB,CAACV,KAAnB,EAA0BD,OAAO,CAACC,KAAlC,EAAyC,gCAAzC,CAAjB;;EACA,IAAIO,cAAc,GAAG,CAArB,EAAwB;IACpB,MAAMK,oBAAoB,GAAGpB,MAAM,CAACe,cAAD,CAAnC;IACA,MAAMM,GAAG,GAAGrB,MAAM,CAAC,CAAD,CAAlB;IACA,MAAMsB,IAAI,GAAGtB,MAAM,CAAC,GAAD,CAAnB;IACAkB,iBAAiB,GACb1B,GAAG,CAACI,GAAG,CAACsB,iBAAD,EAAoBjB,GAAG,CAACoB,GAAD,EAAMD,oBAAN,CAAvB,CAAJ,EAAyDxB,GAAG,CAAC0B,IAAD,EAAOF,oBAAP,CAA5D,CADP;EAEH;;EACD,MAAMG,MAAM,GAAGpB,8BAA8B,CAACe,iBAAD,EAAoBX,OAApB,CAA7C;EACA,OAAOL,mBAAmB,CAACqB,MAAD,EAASJ,QAAT,EAAmBH,SAAnB,CAA1B;AACH;;AACD,OAAO,MAAMQ,mBAAmB,GAAG1B,EAAE,CAAC;EAAEc;AAAF,CAAD,CAA9B"},"metadata":{},"sourceType":"module"}