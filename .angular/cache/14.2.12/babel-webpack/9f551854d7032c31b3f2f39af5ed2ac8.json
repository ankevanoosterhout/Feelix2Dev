{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/* Original Source: losses.py */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { tidy, util } from '@tensorflow/tfjs-core';\nimport { epsilon } from './backend/common';\nimport * as K from './backend/tfjs_backend';\nimport { ValueError } from './errors';\n/**\n * Normalizes a tensor wrt the L2 norm alongside the specified axis.\n * @param x\n * @param axis Axis along which to perform normalization.\n */\n\nexport function l2Normalize(x, axis) {\n  return tidy(() => {\n    if (x.dtype !== 'float32') {\n      x = tfc.cast(x, 'float32');\n    }\n\n    const squareSum = tfc.sum(K.square(x), axis, true);\n    const epsilonTensor = tfc.fill(squareSum.shape, epsilon());\n    const norm = tfc.sqrt(tfc.maximum(squareSum, epsilonTensor));\n    return tfc.div(x, norm);\n  });\n}\nexport function meanSquaredError(yTrue, yPred) {\n  return tidy(() => tfc.mean(K.square(tfc.sub(yPred, yTrue)), -1));\n}\nexport function meanAbsoluteError(yTrue, yPred) {\n  return tidy(() => tfc.mean(tfc.abs(tfc.sub(yPred, yTrue)), -1));\n}\nexport function meanAbsolutePercentageError(yTrue, yPred) {\n  return tidy(() => {\n    const diff = tfc.sub(yTrue, yPred);\n    const clippedTrue = tfc.clipByValue(tfc.abs(yTrue), epsilon(), Number.MAX_VALUE);\n    const absResult = tfc.abs(tfc.div(diff, clippedTrue));\n    return tfc.mul(100, tfc.mean(absResult, -1));\n  });\n}\nexport function meanSquaredLogarithmicError(yTrue, yPred) {\n  return tidy(() => {\n    const clippedPred = tfc.clipByValue(yPred, epsilon(), Number.MAX_VALUE);\n    const firstLog = tfc.log(tfc.add(1, clippedPred));\n    const clippedTrue = tfc.clipByValue(yTrue, epsilon(), Number.MAX_VALUE);\n    const secondLog = tfc.log(tfc.add(1, clippedTrue));\n    return tfc.mean(K.square(tfc.sub(firstLog, secondLog)), -1);\n  });\n}\nexport function squaredHinge(yTrue, yPred) {\n  return tidy(() => {\n    const maxResult = tfc.maximum(0, tfc.sub(1, tfc.mul(yTrue, yPred)));\n    return tfc.mean(K.square(maxResult), -1);\n  });\n}\nexport function hinge(yTrue, yPred) {\n  return tidy(() => {\n    const maxResult = tfc.maximum(0, tfc.sub(1, tfc.mul(yTrue, yPred)));\n    return tfc.mean(maxResult, -1);\n  });\n}\nexport function categoricalHinge(yTrue, yPred) {\n  return tidy(() => {\n    const pos = tfc.sum(tfc.mul(yTrue, yPred), -1);\n    const neg = tfc.max(tfc.mul(tfc.sub(1, yTrue), yPred), -1);\n    return tfc.maximum(0, tfc.add(1, tfc.sub(neg, pos)));\n  });\n}\n/**\n * Logarithm of the hyperbolic cosine of the prediction error.\n *\n * `log(cosh(x))` is approximately equal to `(x ** 2) / 2` for small `x` and\n * to `abs(x) - log(2)` for large `x`. This means that 'logcosh' works mostly\n * like the mean squared error, but will not be so strongly affected by the\n * occasional wildly incorrect prediction.\n */\n\nexport function logcosh(yTrue, yPred) {\n  return tidy(() => {\n    const log2 = Math.log(2);\n    const predictionDiff = tfc.sub(yPred, yTrue);\n    const logcoshResult = tfc.sub(tfc.add(predictionDiff, tfc.softplus(tfc.mul(-2, predictionDiff))), log2);\n    return tfc.mean(logcoshResult, -1);\n  });\n}\nexport function categoricalCrossentropy(target, output, fromLogits = false) {\n  return tidy(() => {\n    if (fromLogits) {\n      output = tfc.softmax(output);\n    } else {\n      // scale preds so that the class probabilities of each sample sum to 1.\n      const outputSum = tfc.sum(output, output.shape.length - 1, true);\n      output = tfc.div(output, outputSum);\n    }\n\n    output = tfc.clipByValue(output, epsilon(), 1 - epsilon());\n    return tfc.neg(tfc.sum(tfc.mul(tfc.cast(target, 'float32'), tfc.log(output)), output.shape.length - 1));\n  });\n}\n/**\n * Categorical crossentropy with integer targets.\n *\n * @param target An integer tensor.\n * @param output A tensor resulting from a softmax (unless `fromLogits` is\n *  `true`, in which case `output` is expected to be the logits).\n * @param fromLogits Boolean, whether `output` is the result of a softmax, or is\n *   a tensor of logits.\n */\n\nexport function sparseCategoricalCrossentropy(target, output, fromLogits = false) {\n  return tidy(() => {\n    const flatTarget = tfc.cast(tfc.floor(K.flatten(target)), 'int32');\n    output = tfc.clipByValue(output, epsilon(), 1 - epsilon());\n    const outputShape = output.shape;\n    const oneHotTarget = tfc.reshape(tfc.oneHot(flatTarget, outputShape[outputShape.length - 1]), outputShape);\n    return categoricalCrossentropy(oneHotTarget, output, fromLogits);\n  });\n}\n/**\n * From TensorFlow's implementation in nn_impl.py:\n *\n * For brevity, let `x = logits`, `z = labels`.  The logistic loss is\n *      z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n *    = z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))\n *    = z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))\n *    = z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))\n *    = (1 - z) * x + log(1 + exp(-x))\n *    = x - x * z + log(1 + exp(-x))\n * For x < 0, to avoid overflow in exp(-x), we reformulate the above\n *      x - x * z + log(1 + exp(-x))\n *    = log(exp(x)) - x * z + log(1 + exp(-x))\n *    = - x * z + log(1 + exp(x))\n * Hence, to ensure stability and avoid overflow, the implementation uses this\n * equivalent formulation\n *    max(x, 0) - x * z + log(1 + exp(-abs(x)))\n *\n * @param labels The labels.\n * @param logits The logits.\n */\n\nexport function sigmoidCrossEntropyWithLogits(labels, logits) {\n  if (!util.arraysEqual(labels.shape, logits.shape)) {\n    throw new ValueError(`logits and labels must have the same shape, but got shapes ` + `${JSON.stringify(labels.shape)} and ${JSON.stringify(logits.shape)}`);\n  }\n\n  return tidy(() => {\n    // The logistic loss formula from above is\n    //   x - x * z + log(1 + exp(-x))\n    // For x < 0, a more numerically stable formula is\n    //   -x * z + log(1 + exp(x))\n    // Note that these two expressions can be combined into the following:\n    //   max(x, 0) - x * z + log(1 + exp(-abs(x)))\n    const reluLogits = tfc.relu(logits);\n    const negAbsLogits = tfc.neg(tfc.abs(logits));\n    return tfc.add(tfc.sub(reluLogits, tfc.mul(logits, labels)), tfc.log1p(tfc.exp(negAbsLogits)));\n  });\n}\nexport function binaryCrossentropy(yTrue, yPred) {\n  return tidy(() => {\n    let y;\n    y = tfc.clipByValue(yPred, epsilon(), 1 - epsilon());\n    y = tfc.log(tfc.div(y, tfc.sub(1, y)));\n    return tfc.mean(sigmoidCrossEntropyWithLogits(yTrue, y), -1);\n  });\n}\nexport function kullbackLeiblerDivergence(yTrue, yPred) {\n  return tidy(() => {\n    const clippedTrue = tfc.clipByValue(yTrue, epsilon(), 1);\n    const clippedPred = tfc.clipByValue(yPred, epsilon(), 1);\n    return tfc.sum(tfc.mul(yTrue, tfc.log(tfc.div(clippedTrue, clippedPred))), -1);\n  });\n}\nexport function poisson(yTrue, yPred) {\n  return tidy(() => {\n    const logPred = tfc.log(tfc.add(epsilon(), yPred));\n    return tfc.mean(tfc.sub(yPred, tfc.mul(yTrue, logPred)), -1);\n  });\n}\nexport function cosineProximity(yTrue, yPred) {\n  return tidy(() => {\n    const trueNormalized = l2Normalize(yTrue, -1);\n    const predNormalized = l2Normalize(yPred, -1);\n    const trueXPred = tfc.mul(trueNormalized, predNormalized);\n    return tfc.neg(tfc.sum(trueXPred, -1));\n  });\n}\nexport const mse = meanSquaredError;\nexport const MSE = meanSquaredError;\nexport const mae = meanAbsoluteError;\nexport const MAE = meanAbsoluteError;\nexport const mape = meanAbsolutePercentageError;\nexport const MAPE = meanAbsolutePercentageError;\nexport const msle = meanSquaredLogarithmicError;\nexport const MSLE = meanSquaredLogarithmicError;\nexport const kld = kullbackLeiblerDivergence;\nexport const KLD = kullbackLeiblerDivergence;\nexport const cosine = cosineProximity; // TODO(michaelterry): Add deserialize() function.\n\nexport const lossesMap = {\n  meanSquaredError,\n  meanAbsoluteError,\n  meanAbsolutePercentageError,\n  meanSquaredLogarithmicError,\n  squaredHinge,\n  hinge,\n  categoricalHinge,\n  logcosh,\n  categoricalCrossentropy,\n  sparseCategoricalCrossentropy,\n  binaryCrossentropy,\n  kullbackLeiblerDivergence,\n  poisson,\n  cosineProximity\n}; // Porting note: This diverges from the PyKeras implementation and may need to\n// change based on (de)serialization requirements.\n\nexport function get(identifierOrFn) {\n  if (typeof identifierOrFn === 'string') {\n    if (identifierOrFn in lossesMap) {\n      return lossesMap[identifierOrFn];\n    }\n\n    let errMsg = `Unknown loss ${identifierOrFn}`;\n\n    if (identifierOrFn.toLowerCase().includes('softmaxcrossentropy')) {\n      errMsg = `Unknown loss ${identifierOrFn}. ` + 'Use \"categoricalCrossentropy\" as the string name for ' + 'tf.losses.softmaxCrossEntropy';\n    }\n\n    throw new ValueError(errMsg);\n  } else {\n    return identifierOrFn;\n  }\n}","map":{"version":3,"names":["tfc","tidy","util","epsilon","K","ValueError","l2Normalize","x","axis","dtype","cast","squareSum","sum","square","epsilonTensor","fill","shape","norm","sqrt","maximum","div","meanSquaredError","yTrue","yPred","mean","sub","meanAbsoluteError","abs","meanAbsolutePercentageError","diff","clippedTrue","clipByValue","Number","MAX_VALUE","absResult","mul","meanSquaredLogarithmicError","clippedPred","firstLog","log","add","secondLog","squaredHinge","maxResult","hinge","categoricalHinge","pos","neg","max","logcosh","log2","Math","predictionDiff","logcoshResult","softplus","categoricalCrossentropy","target","output","fromLogits","softmax","outputSum","length","sparseCategoricalCrossentropy","flatTarget","floor","flatten","outputShape","oneHotTarget","reshape","oneHot","sigmoidCrossEntropyWithLogits","labels","logits","arraysEqual","JSON","stringify","reluLogits","relu","negAbsLogits","log1p","exp","binaryCrossentropy","y","kullbackLeiblerDivergence","poisson","logPred","cosineProximity","trueNormalized","predNormalized","trueXPred","mse","MSE","mae","MAE","mape","MAPE","msle","MSLE","kld","KLD","cosine","lossesMap","get","identifierOrFn","errMsg","toLowerCase","includes"],"sources":["C:/Users/Anke/Documents/Feelix documents/Feelix2.0-dev/Feelix v2/node_modules/@tensorflow/tfjs-layers/dist/losses.js"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/* Original Source: losses.py */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { tidy, util } from '@tensorflow/tfjs-core';\nimport { epsilon } from './backend/common';\nimport * as K from './backend/tfjs_backend';\nimport { ValueError } from './errors';\n/**\n * Normalizes a tensor wrt the L2 norm alongside the specified axis.\n * @param x\n * @param axis Axis along which to perform normalization.\n */\nexport function l2Normalize(x, axis) {\n    return tidy(() => {\n        if (x.dtype !== 'float32') {\n            x = tfc.cast(x, 'float32');\n        }\n        const squareSum = tfc.sum(K.square(x), axis, true);\n        const epsilonTensor = tfc.fill(squareSum.shape, epsilon());\n        const norm = tfc.sqrt(tfc.maximum(squareSum, epsilonTensor));\n        return tfc.div(x, norm);\n    });\n}\nexport function meanSquaredError(yTrue, yPred) {\n    return tidy(() => tfc.mean(K.square(tfc.sub(yPred, yTrue)), -1));\n}\nexport function meanAbsoluteError(yTrue, yPred) {\n    return tidy(() => tfc.mean(tfc.abs(tfc.sub(yPred, yTrue)), -1));\n}\nexport function meanAbsolutePercentageError(yTrue, yPred) {\n    return tidy(() => {\n        const diff = tfc.sub(yTrue, yPred);\n        const clippedTrue = tfc.clipByValue(tfc.abs(yTrue), epsilon(), Number.MAX_VALUE);\n        const absResult = tfc.abs(tfc.div(diff, clippedTrue));\n        return tfc.mul(100, tfc.mean(absResult, -1));\n    });\n}\nexport function meanSquaredLogarithmicError(yTrue, yPred) {\n    return tidy(() => {\n        const clippedPred = tfc.clipByValue(yPred, epsilon(), Number.MAX_VALUE);\n        const firstLog = tfc.log(tfc.add(1, clippedPred));\n        const clippedTrue = tfc.clipByValue(yTrue, epsilon(), Number.MAX_VALUE);\n        const secondLog = tfc.log(tfc.add(1, clippedTrue));\n        return tfc.mean(K.square(tfc.sub(firstLog, secondLog)), -1);\n    });\n}\nexport function squaredHinge(yTrue, yPred) {\n    return tidy(() => {\n        const maxResult = tfc.maximum(0, tfc.sub(1, tfc.mul(yTrue, yPred)));\n        return tfc.mean(K.square(maxResult), -1);\n    });\n}\nexport function hinge(yTrue, yPred) {\n    return tidy(() => {\n        const maxResult = tfc.maximum(0, tfc.sub(1, tfc.mul(yTrue, yPred)));\n        return tfc.mean(maxResult, -1);\n    });\n}\nexport function categoricalHinge(yTrue, yPred) {\n    return tidy(() => {\n        const pos = tfc.sum(tfc.mul(yTrue, yPred), -1);\n        const neg = tfc.max(tfc.mul(tfc.sub(1, yTrue), yPred), -1);\n        return tfc.maximum(0, tfc.add(1, tfc.sub(neg, pos)));\n    });\n}\n/**\n * Logarithm of the hyperbolic cosine of the prediction error.\n *\n * `log(cosh(x))` is approximately equal to `(x ** 2) / 2` for small `x` and\n * to `abs(x) - log(2)` for large `x`. This means that 'logcosh' works mostly\n * like the mean squared error, but will not be so strongly affected by the\n * occasional wildly incorrect prediction.\n */\nexport function logcosh(yTrue, yPred) {\n    return tidy(() => {\n        const log2 = Math.log(2);\n        const predictionDiff = tfc.sub(yPred, yTrue);\n        const logcoshResult = tfc.sub(tfc.add(predictionDiff, tfc.softplus(tfc.mul(-2, predictionDiff))), log2);\n        return tfc.mean(logcoshResult, -1);\n    });\n}\nexport function categoricalCrossentropy(target, output, fromLogits = false) {\n    return tidy(() => {\n        if (fromLogits) {\n            output = tfc.softmax(output);\n        }\n        else {\n            // scale preds so that the class probabilities of each sample sum to 1.\n            const outputSum = tfc.sum(output, output.shape.length - 1, true);\n            output = tfc.div(output, outputSum);\n        }\n        output = tfc.clipByValue(output, epsilon(), 1 - epsilon());\n        return tfc.neg(tfc.sum(tfc.mul(tfc.cast(target, 'float32'), tfc.log(output)), output.shape.length - 1));\n    });\n}\n/**\n * Categorical crossentropy with integer targets.\n *\n * @param target An integer tensor.\n * @param output A tensor resulting from a softmax (unless `fromLogits` is\n *  `true`, in which case `output` is expected to be the logits).\n * @param fromLogits Boolean, whether `output` is the result of a softmax, or is\n *   a tensor of logits.\n */\nexport function sparseCategoricalCrossentropy(target, output, fromLogits = false) {\n    return tidy(() => {\n        const flatTarget = tfc.cast(tfc.floor(K.flatten(target)), 'int32');\n        output = tfc.clipByValue(output, epsilon(), 1 - epsilon());\n        const outputShape = output.shape;\n        const oneHotTarget = tfc.reshape(tfc.oneHot(flatTarget, outputShape[outputShape.length - 1]), outputShape);\n        return categoricalCrossentropy(oneHotTarget, output, fromLogits);\n    });\n}\n/**\n * From TensorFlow's implementation in nn_impl.py:\n *\n * For brevity, let `x = logits`, `z = labels`.  The logistic loss is\n *      z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n *    = z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))\n *    = z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))\n *    = z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))\n *    = (1 - z) * x + log(1 + exp(-x))\n *    = x - x * z + log(1 + exp(-x))\n * For x < 0, to avoid overflow in exp(-x), we reformulate the above\n *      x - x * z + log(1 + exp(-x))\n *    = log(exp(x)) - x * z + log(1 + exp(-x))\n *    = - x * z + log(1 + exp(x))\n * Hence, to ensure stability and avoid overflow, the implementation uses this\n * equivalent formulation\n *    max(x, 0) - x * z + log(1 + exp(-abs(x)))\n *\n * @param labels The labels.\n * @param logits The logits.\n */\nexport function sigmoidCrossEntropyWithLogits(labels, logits) {\n    if (!util.arraysEqual(labels.shape, logits.shape)) {\n        throw new ValueError(`logits and labels must have the same shape, but got shapes ` +\n            `${JSON.stringify(labels.shape)} and ${JSON.stringify(logits.shape)}`);\n    }\n    return tidy(() => {\n        // The logistic loss formula from above is\n        //   x - x * z + log(1 + exp(-x))\n        // For x < 0, a more numerically stable formula is\n        //   -x * z + log(1 + exp(x))\n        // Note that these two expressions can be combined into the following:\n        //   max(x, 0) - x * z + log(1 + exp(-abs(x)))\n        const reluLogits = tfc.relu(logits);\n        const negAbsLogits = tfc.neg(tfc.abs(logits));\n        return tfc.add(tfc.sub(reluLogits, tfc.mul(logits, labels)), tfc.log1p(tfc.exp(negAbsLogits)));\n    });\n}\nexport function binaryCrossentropy(yTrue, yPred) {\n    return tidy(() => {\n        let y;\n        y = tfc.clipByValue(yPred, epsilon(), 1 - epsilon());\n        y = tfc.log(tfc.div(y, tfc.sub(1, y)));\n        return tfc.mean(sigmoidCrossEntropyWithLogits(yTrue, y), -1);\n    });\n}\nexport function kullbackLeiblerDivergence(yTrue, yPred) {\n    return tidy(() => {\n        const clippedTrue = tfc.clipByValue(yTrue, epsilon(), 1);\n        const clippedPred = tfc.clipByValue(yPred, epsilon(), 1);\n        return tfc.sum(tfc.mul(yTrue, tfc.log(tfc.div(clippedTrue, clippedPred))), -1);\n    });\n}\nexport function poisson(yTrue, yPred) {\n    return tidy(() => {\n        const logPred = tfc.log(tfc.add(epsilon(), yPred));\n        return tfc.mean(tfc.sub(yPred, tfc.mul(yTrue, logPred)), -1);\n    });\n}\nexport function cosineProximity(yTrue, yPred) {\n    return tidy(() => {\n        const trueNormalized = l2Normalize(yTrue, -1);\n        const predNormalized = l2Normalize(yPred, -1);\n        const trueXPred = tfc.mul(trueNormalized, predNormalized);\n        return tfc.neg(tfc.sum(trueXPred, -1));\n    });\n}\nexport const mse = meanSquaredError;\nexport const MSE = meanSquaredError;\nexport const mae = meanAbsoluteError;\nexport const MAE = meanAbsoluteError;\nexport const mape = meanAbsolutePercentageError;\nexport const MAPE = meanAbsolutePercentageError;\nexport const msle = meanSquaredLogarithmicError;\nexport const MSLE = meanSquaredLogarithmicError;\nexport const kld = kullbackLeiblerDivergence;\nexport const KLD = kullbackLeiblerDivergence;\nexport const cosine = cosineProximity;\n// TODO(michaelterry): Add deserialize() function.\nexport const lossesMap = {\n    meanSquaredError,\n    meanAbsoluteError,\n    meanAbsolutePercentageError,\n    meanSquaredLogarithmicError,\n    squaredHinge,\n    hinge,\n    categoricalHinge,\n    logcosh,\n    categoricalCrossentropy,\n    sparseCategoricalCrossentropy,\n    binaryCrossentropy,\n    kullbackLeiblerDivergence,\n    poisson,\n    cosineProximity\n};\n// Porting note: This diverges from the PyKeras implementation and may need to\n// change based on (de)serialization requirements.\nexport function get(identifierOrFn) {\n    if (typeof identifierOrFn === 'string') {\n        if (identifierOrFn in lossesMap) {\n            return lossesMap[identifierOrFn];\n        }\n        let errMsg = `Unknown loss ${identifierOrFn}`;\n        if (identifierOrFn.toLowerCase().includes('softmaxcrossentropy')) {\n            errMsg = `Unknown loss ${identifierOrFn}. ` +\n                'Use \"categoricalCrossentropy\" as the string name for ' +\n                'tf.losses.softmaxCrossEntropy';\n        }\n        throw new ValueError(errMsg);\n    }\n    else {\n        return identifierOrFn;\n    }\n}\n"],"mappings":"AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AACA;AACA,OAAO,KAAKA,GAAZ,MAAqB,uBAArB;AACA,SAASC,IAAT,EAAeC,IAAf,QAA2B,uBAA3B;AACA,SAASC,OAAT,QAAwB,kBAAxB;AACA,OAAO,KAAKC,CAAZ,MAAmB,wBAAnB;AACA,SAASC,UAAT,QAA2B,UAA3B;AACA;AACA;AACA;AACA;AACA;;AACA,OAAO,SAASC,WAAT,CAAqBC,CAArB,EAAwBC,IAAxB,EAA8B;EACjC,OAAOP,IAAI,CAAC,MAAM;IACd,IAAIM,CAAC,CAACE,KAAF,KAAY,SAAhB,EAA2B;MACvBF,CAAC,GAAGP,GAAG,CAACU,IAAJ,CAASH,CAAT,EAAY,SAAZ,CAAJ;IACH;;IACD,MAAMI,SAAS,GAAGX,GAAG,CAACY,GAAJ,CAAQR,CAAC,CAACS,MAAF,CAASN,CAAT,CAAR,EAAqBC,IAArB,EAA2B,IAA3B,CAAlB;IACA,MAAMM,aAAa,GAAGd,GAAG,CAACe,IAAJ,CAASJ,SAAS,CAACK,KAAnB,EAA0Bb,OAAO,EAAjC,CAAtB;IACA,MAAMc,IAAI,GAAGjB,GAAG,CAACkB,IAAJ,CAASlB,GAAG,CAACmB,OAAJ,CAAYR,SAAZ,EAAuBG,aAAvB,CAAT,CAAb;IACA,OAAOd,GAAG,CAACoB,GAAJ,CAAQb,CAAR,EAAWU,IAAX,CAAP;EACH,CARU,CAAX;AASH;AACD,OAAO,SAASI,gBAAT,CAA0BC,KAA1B,EAAiCC,KAAjC,EAAwC;EAC3C,OAAOtB,IAAI,CAAC,MAAMD,GAAG,CAACwB,IAAJ,CAASpB,CAAC,CAACS,MAAF,CAASb,GAAG,CAACyB,GAAJ,CAAQF,KAAR,EAAeD,KAAf,CAAT,CAAT,EAA0C,CAAC,CAA3C,CAAP,CAAX;AACH;AACD,OAAO,SAASI,iBAAT,CAA2BJ,KAA3B,EAAkCC,KAAlC,EAAyC;EAC5C,OAAOtB,IAAI,CAAC,MAAMD,GAAG,CAACwB,IAAJ,CAASxB,GAAG,CAAC2B,GAAJ,CAAQ3B,GAAG,CAACyB,GAAJ,CAAQF,KAAR,EAAeD,KAAf,CAAR,CAAT,EAAyC,CAAC,CAA1C,CAAP,CAAX;AACH;AACD,OAAO,SAASM,2BAAT,CAAqCN,KAArC,EAA4CC,KAA5C,EAAmD;EACtD,OAAOtB,IAAI,CAAC,MAAM;IACd,MAAM4B,IAAI,GAAG7B,GAAG,CAACyB,GAAJ,CAAQH,KAAR,EAAeC,KAAf,CAAb;IACA,MAAMO,WAAW,GAAG9B,GAAG,CAAC+B,WAAJ,CAAgB/B,GAAG,CAAC2B,GAAJ,CAAQL,KAAR,CAAhB,EAAgCnB,OAAO,EAAvC,EAA2C6B,MAAM,CAACC,SAAlD,CAApB;IACA,MAAMC,SAAS,GAAGlC,GAAG,CAAC2B,GAAJ,CAAQ3B,GAAG,CAACoB,GAAJ,CAAQS,IAAR,EAAcC,WAAd,CAAR,CAAlB;IACA,OAAO9B,GAAG,CAACmC,GAAJ,CAAQ,GAAR,EAAanC,GAAG,CAACwB,IAAJ,CAASU,SAAT,EAAoB,CAAC,CAArB,CAAb,CAAP;EACH,CALU,CAAX;AAMH;AACD,OAAO,SAASE,2BAAT,CAAqCd,KAArC,EAA4CC,KAA5C,EAAmD;EACtD,OAAOtB,IAAI,CAAC,MAAM;IACd,MAAMoC,WAAW,GAAGrC,GAAG,CAAC+B,WAAJ,CAAgBR,KAAhB,EAAuBpB,OAAO,EAA9B,EAAkC6B,MAAM,CAACC,SAAzC,CAApB;IACA,MAAMK,QAAQ,GAAGtC,GAAG,CAACuC,GAAJ,CAAQvC,GAAG,CAACwC,GAAJ,CAAQ,CAAR,EAAWH,WAAX,CAAR,CAAjB;IACA,MAAMP,WAAW,GAAG9B,GAAG,CAAC+B,WAAJ,CAAgBT,KAAhB,EAAuBnB,OAAO,EAA9B,EAAkC6B,MAAM,CAACC,SAAzC,CAApB;IACA,MAAMQ,SAAS,GAAGzC,GAAG,CAACuC,GAAJ,CAAQvC,GAAG,CAACwC,GAAJ,CAAQ,CAAR,EAAWV,WAAX,CAAR,CAAlB;IACA,OAAO9B,GAAG,CAACwB,IAAJ,CAASpB,CAAC,CAACS,MAAF,CAASb,GAAG,CAACyB,GAAJ,CAAQa,QAAR,EAAkBG,SAAlB,CAAT,CAAT,EAAiD,CAAC,CAAlD,CAAP;EACH,CANU,CAAX;AAOH;AACD,OAAO,SAASC,YAAT,CAAsBpB,KAAtB,EAA6BC,KAA7B,EAAoC;EACvC,OAAOtB,IAAI,CAAC,MAAM;IACd,MAAM0C,SAAS,GAAG3C,GAAG,CAACmB,OAAJ,CAAY,CAAZ,EAAenB,GAAG,CAACyB,GAAJ,CAAQ,CAAR,EAAWzB,GAAG,CAACmC,GAAJ,CAAQb,KAAR,EAAeC,KAAf,CAAX,CAAf,CAAlB;IACA,OAAOvB,GAAG,CAACwB,IAAJ,CAASpB,CAAC,CAACS,MAAF,CAAS8B,SAAT,CAAT,EAA8B,CAAC,CAA/B,CAAP;EACH,CAHU,CAAX;AAIH;AACD,OAAO,SAASC,KAAT,CAAetB,KAAf,EAAsBC,KAAtB,EAA6B;EAChC,OAAOtB,IAAI,CAAC,MAAM;IACd,MAAM0C,SAAS,GAAG3C,GAAG,CAACmB,OAAJ,CAAY,CAAZ,EAAenB,GAAG,CAACyB,GAAJ,CAAQ,CAAR,EAAWzB,GAAG,CAACmC,GAAJ,CAAQb,KAAR,EAAeC,KAAf,CAAX,CAAf,CAAlB;IACA,OAAOvB,GAAG,CAACwB,IAAJ,CAASmB,SAAT,EAAoB,CAAC,CAArB,CAAP;EACH,CAHU,CAAX;AAIH;AACD,OAAO,SAASE,gBAAT,CAA0BvB,KAA1B,EAAiCC,KAAjC,EAAwC;EAC3C,OAAOtB,IAAI,CAAC,MAAM;IACd,MAAM6C,GAAG,GAAG9C,GAAG,CAACY,GAAJ,CAAQZ,GAAG,CAACmC,GAAJ,CAAQb,KAAR,EAAeC,KAAf,CAAR,EAA+B,CAAC,CAAhC,CAAZ;IACA,MAAMwB,GAAG,GAAG/C,GAAG,CAACgD,GAAJ,CAAQhD,GAAG,CAACmC,GAAJ,CAAQnC,GAAG,CAACyB,GAAJ,CAAQ,CAAR,EAAWH,KAAX,CAAR,EAA2BC,KAA3B,CAAR,EAA2C,CAAC,CAA5C,CAAZ;IACA,OAAOvB,GAAG,CAACmB,OAAJ,CAAY,CAAZ,EAAenB,GAAG,CAACwC,GAAJ,CAAQ,CAAR,EAAWxC,GAAG,CAACyB,GAAJ,CAAQsB,GAAR,EAAaD,GAAb,CAAX,CAAf,CAAP;EACH,CAJU,CAAX;AAKH;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AACA,OAAO,SAASG,OAAT,CAAiB3B,KAAjB,EAAwBC,KAAxB,EAA+B;EAClC,OAAOtB,IAAI,CAAC,MAAM;IACd,MAAMiD,IAAI,GAAGC,IAAI,CAACZ,GAAL,CAAS,CAAT,CAAb;IACA,MAAMa,cAAc,GAAGpD,GAAG,CAACyB,GAAJ,CAAQF,KAAR,EAAeD,KAAf,CAAvB;IACA,MAAM+B,aAAa,GAAGrD,GAAG,CAACyB,GAAJ,CAAQzB,GAAG,CAACwC,GAAJ,CAAQY,cAAR,EAAwBpD,GAAG,CAACsD,QAAJ,CAAatD,GAAG,CAACmC,GAAJ,CAAQ,CAAC,CAAT,EAAYiB,cAAZ,CAAb,CAAxB,CAAR,EAA4EF,IAA5E,CAAtB;IACA,OAAOlD,GAAG,CAACwB,IAAJ,CAAS6B,aAAT,EAAwB,CAAC,CAAzB,CAAP;EACH,CALU,CAAX;AAMH;AACD,OAAO,SAASE,uBAAT,CAAiCC,MAAjC,EAAyCC,MAAzC,EAAiDC,UAAU,GAAG,KAA9D,EAAqE;EACxE,OAAOzD,IAAI,CAAC,MAAM;IACd,IAAIyD,UAAJ,EAAgB;MACZD,MAAM,GAAGzD,GAAG,CAAC2D,OAAJ,CAAYF,MAAZ,CAAT;IACH,CAFD,MAGK;MACD;MACA,MAAMG,SAAS,GAAG5D,GAAG,CAACY,GAAJ,CAAQ6C,MAAR,EAAgBA,MAAM,CAACzC,KAAP,CAAa6C,MAAb,GAAsB,CAAtC,EAAyC,IAAzC,CAAlB;MACAJ,MAAM,GAAGzD,GAAG,CAACoB,GAAJ,CAAQqC,MAAR,EAAgBG,SAAhB,CAAT;IACH;;IACDH,MAAM,GAAGzD,GAAG,CAAC+B,WAAJ,CAAgB0B,MAAhB,EAAwBtD,OAAO,EAA/B,EAAmC,IAAIA,OAAO,EAA9C,CAAT;IACA,OAAOH,GAAG,CAAC+C,GAAJ,CAAQ/C,GAAG,CAACY,GAAJ,CAAQZ,GAAG,CAACmC,GAAJ,CAAQnC,GAAG,CAACU,IAAJ,CAAS8C,MAAT,EAAiB,SAAjB,CAAR,EAAqCxD,GAAG,CAACuC,GAAJ,CAAQkB,MAAR,CAArC,CAAR,EAA+DA,MAAM,CAACzC,KAAP,CAAa6C,MAAb,GAAsB,CAArF,CAAR,CAAP;EACH,CAXU,CAAX;AAYH;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AACA,OAAO,SAASC,6BAAT,CAAuCN,MAAvC,EAA+CC,MAA/C,EAAuDC,UAAU,GAAG,KAApE,EAA2E;EAC9E,OAAOzD,IAAI,CAAC,MAAM;IACd,MAAM8D,UAAU,GAAG/D,GAAG,CAACU,IAAJ,CAASV,GAAG,CAACgE,KAAJ,CAAU5D,CAAC,CAAC6D,OAAF,CAAUT,MAAV,CAAV,CAAT,EAAuC,OAAvC,CAAnB;IACAC,MAAM,GAAGzD,GAAG,CAAC+B,WAAJ,CAAgB0B,MAAhB,EAAwBtD,OAAO,EAA/B,EAAmC,IAAIA,OAAO,EAA9C,CAAT;IACA,MAAM+D,WAAW,GAAGT,MAAM,CAACzC,KAA3B;IACA,MAAMmD,YAAY,GAAGnE,GAAG,CAACoE,OAAJ,CAAYpE,GAAG,CAACqE,MAAJ,CAAWN,UAAX,EAAuBG,WAAW,CAACA,WAAW,CAACL,MAAZ,GAAqB,CAAtB,CAAlC,CAAZ,EAAyEK,WAAzE,CAArB;IACA,OAAOX,uBAAuB,CAACY,YAAD,EAAeV,MAAf,EAAuBC,UAAvB,CAA9B;EACH,CANU,CAAX;AAOH;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AACA,OAAO,SAASY,6BAAT,CAAuCC,MAAvC,EAA+CC,MAA/C,EAAuD;EAC1D,IAAI,CAACtE,IAAI,CAACuE,WAAL,CAAiBF,MAAM,CAACvD,KAAxB,EAA+BwD,MAAM,CAACxD,KAAtC,CAAL,EAAmD;IAC/C,MAAM,IAAIX,UAAJ,CAAgB,6DAAD,GAChB,GAAEqE,IAAI,CAACC,SAAL,CAAeJ,MAAM,CAACvD,KAAtB,CAA6B,QAAO0D,IAAI,CAACC,SAAL,CAAeH,MAAM,CAACxD,KAAtB,CAA6B,EADlE,CAAN;EAEH;;EACD,OAAOf,IAAI,CAAC,MAAM;IACd;IACA;IACA;IACA;IACA;IACA;IACA,MAAM2E,UAAU,GAAG5E,GAAG,CAAC6E,IAAJ,CAASL,MAAT,CAAnB;IACA,MAAMM,YAAY,GAAG9E,GAAG,CAAC+C,GAAJ,CAAQ/C,GAAG,CAAC2B,GAAJ,CAAQ6C,MAAR,CAAR,CAArB;IACA,OAAOxE,GAAG,CAACwC,GAAJ,CAAQxC,GAAG,CAACyB,GAAJ,CAAQmD,UAAR,EAAoB5E,GAAG,CAACmC,GAAJ,CAAQqC,MAAR,EAAgBD,MAAhB,CAApB,CAAR,EAAsDvE,GAAG,CAAC+E,KAAJ,CAAU/E,GAAG,CAACgF,GAAJ,CAAQF,YAAR,CAAV,CAAtD,CAAP;EACH,CAVU,CAAX;AAWH;AACD,OAAO,SAASG,kBAAT,CAA4B3D,KAA5B,EAAmCC,KAAnC,EAA0C;EAC7C,OAAOtB,IAAI,CAAC,MAAM;IACd,IAAIiF,CAAJ;IACAA,CAAC,GAAGlF,GAAG,CAAC+B,WAAJ,CAAgBR,KAAhB,EAAuBpB,OAAO,EAA9B,EAAkC,IAAIA,OAAO,EAA7C,CAAJ;IACA+E,CAAC,GAAGlF,GAAG,CAACuC,GAAJ,CAAQvC,GAAG,CAACoB,GAAJ,CAAQ8D,CAAR,EAAWlF,GAAG,CAACyB,GAAJ,CAAQ,CAAR,EAAWyD,CAAX,CAAX,CAAR,CAAJ;IACA,OAAOlF,GAAG,CAACwB,IAAJ,CAAS8C,6BAA6B,CAAChD,KAAD,EAAQ4D,CAAR,CAAtC,EAAkD,CAAC,CAAnD,CAAP;EACH,CALU,CAAX;AAMH;AACD,OAAO,SAASC,yBAAT,CAAmC7D,KAAnC,EAA0CC,KAA1C,EAAiD;EACpD,OAAOtB,IAAI,CAAC,MAAM;IACd,MAAM6B,WAAW,GAAG9B,GAAG,CAAC+B,WAAJ,CAAgBT,KAAhB,EAAuBnB,OAAO,EAA9B,EAAkC,CAAlC,CAApB;IACA,MAAMkC,WAAW,GAAGrC,GAAG,CAAC+B,WAAJ,CAAgBR,KAAhB,EAAuBpB,OAAO,EAA9B,EAAkC,CAAlC,CAApB;IACA,OAAOH,GAAG,CAACY,GAAJ,CAAQZ,GAAG,CAACmC,GAAJ,CAAQb,KAAR,EAAetB,GAAG,CAACuC,GAAJ,CAAQvC,GAAG,CAACoB,GAAJ,CAAQU,WAAR,EAAqBO,WAArB,CAAR,CAAf,CAAR,EAAoE,CAAC,CAArE,CAAP;EACH,CAJU,CAAX;AAKH;AACD,OAAO,SAAS+C,OAAT,CAAiB9D,KAAjB,EAAwBC,KAAxB,EAA+B;EAClC,OAAOtB,IAAI,CAAC,MAAM;IACd,MAAMoF,OAAO,GAAGrF,GAAG,CAACuC,GAAJ,CAAQvC,GAAG,CAACwC,GAAJ,CAAQrC,OAAO,EAAf,EAAmBoB,KAAnB,CAAR,CAAhB;IACA,OAAOvB,GAAG,CAACwB,IAAJ,CAASxB,GAAG,CAACyB,GAAJ,CAAQF,KAAR,EAAevB,GAAG,CAACmC,GAAJ,CAAQb,KAAR,EAAe+D,OAAf,CAAf,CAAT,EAAkD,CAAC,CAAnD,CAAP;EACH,CAHU,CAAX;AAIH;AACD,OAAO,SAASC,eAAT,CAAyBhE,KAAzB,EAAgCC,KAAhC,EAAuC;EAC1C,OAAOtB,IAAI,CAAC,MAAM;IACd,MAAMsF,cAAc,GAAGjF,WAAW,CAACgB,KAAD,EAAQ,CAAC,CAAT,CAAlC;IACA,MAAMkE,cAAc,GAAGlF,WAAW,CAACiB,KAAD,EAAQ,CAAC,CAAT,CAAlC;IACA,MAAMkE,SAAS,GAAGzF,GAAG,CAACmC,GAAJ,CAAQoD,cAAR,EAAwBC,cAAxB,CAAlB;IACA,OAAOxF,GAAG,CAAC+C,GAAJ,CAAQ/C,GAAG,CAACY,GAAJ,CAAQ6E,SAAR,EAAmB,CAAC,CAApB,CAAR,CAAP;EACH,CALU,CAAX;AAMH;AACD,OAAO,MAAMC,GAAG,GAAGrE,gBAAZ;AACP,OAAO,MAAMsE,GAAG,GAAGtE,gBAAZ;AACP,OAAO,MAAMuE,GAAG,GAAGlE,iBAAZ;AACP,OAAO,MAAMmE,GAAG,GAAGnE,iBAAZ;AACP,OAAO,MAAMoE,IAAI,GAAGlE,2BAAb;AACP,OAAO,MAAMmE,IAAI,GAAGnE,2BAAb;AACP,OAAO,MAAMoE,IAAI,GAAG5D,2BAAb;AACP,OAAO,MAAM6D,IAAI,GAAG7D,2BAAb;AACP,OAAO,MAAM8D,GAAG,GAAGf,yBAAZ;AACP,OAAO,MAAMgB,GAAG,GAAGhB,yBAAZ;AACP,OAAO,MAAMiB,MAAM,GAAGd,eAAf,C,CACP;;AACA,OAAO,MAAMe,SAAS,GAAG;EACrBhF,gBADqB;EAErBK,iBAFqB;EAGrBE,2BAHqB;EAIrBQ,2BAJqB;EAKrBM,YALqB;EAMrBE,KANqB;EAOrBC,gBAPqB;EAQrBI,OARqB;EASrBM,uBATqB;EAUrBO,6BAVqB;EAWrBmB,kBAXqB;EAYrBE,yBAZqB;EAarBC,OAbqB;EAcrBE;AAdqB,CAAlB,C,CAgBP;AACA;;AACA,OAAO,SAASgB,GAAT,CAAaC,cAAb,EAA6B;EAChC,IAAI,OAAOA,cAAP,KAA0B,QAA9B,EAAwC;IACpC,IAAIA,cAAc,IAAIF,SAAtB,EAAiC;MAC7B,OAAOA,SAAS,CAACE,cAAD,CAAhB;IACH;;IACD,IAAIC,MAAM,GAAI,gBAAeD,cAAe,EAA5C;;IACA,IAAIA,cAAc,CAACE,WAAf,GAA6BC,QAA7B,CAAsC,qBAAtC,CAAJ,EAAkE;MAC9DF,MAAM,GAAI,gBAAeD,cAAe,IAA/B,GACL,uDADK,GAEL,+BAFJ;IAGH;;IACD,MAAM,IAAIlG,UAAJ,CAAemG,MAAf,CAAN;EACH,CAXD,MAYK;IACD,OAAOD,cAAP;EACH;AACJ"},"metadata":{},"sourceType":"module"}