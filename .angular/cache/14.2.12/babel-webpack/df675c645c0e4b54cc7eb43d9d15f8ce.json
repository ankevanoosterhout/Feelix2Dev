{"ast":null,"code":"/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { util } from '@tensorflow/tfjs-core';\nimport { Im2ColPackedProgram } from '../im2col_packed_gpu';\nimport { mapActivationToShaderProgram } from '../kernel_utils/kernel_funcs_utils';\nimport { MatMulPackedProgram } from '../mulmat_packed_gpu';\nimport * as webgl_util from '../webgl_util';\nimport { batchMatMulImpl, MATMUL_SHARED_DIM_THRESHOLD } from './BatchMatMul_impl';\nimport { identity } from './Identity';\nimport { reshape } from './Reshape'; // Both conv2dByMatMul and conv2dWithIm2Row fuse height and width into one\n// dimension to compute batchMatMul, so bias and activation weights are also\n// supposed to fuse the two dimensions into one.\n//\n// This function computes the target shape for fusing height and width\n// dimensions. Returning null means the shape is already compatible.\n//\n// Even though the bias is not supposed to be a 3-D or a 4-D (including\n// batch) tensor and PReLU activiation weights is not supposed to be a 4-D\n// tensor, we still need to support them, because we haven't disabled\n// them for NHWC format.\n// https://github.com/tensorflow/tfjs/blob/b53bd47e880367ae57493f0ea628abaf08db2d5d/tfjs-core/src/ops/fused/conv2d.ts#L181-L196\n\nfunction getShapeForBatchMatMul(shape, isChannelsLast) {\n  const length = shape.length;\n\n  if (length >= 3) {\n    return isChannelsLast ? [...shape.slice(0, -3)\n    /* batch */\n    , shape[length - 3] * shape[length - 2]\n    /* height * width */\n    , shape[length - 1]\n    /* channel */\n    ] : [...shape.slice(0, -3)\n    /* batch */\n    , shape[length - 3]\n    /* channel */\n    , shape[length - 2] * shape[length - 1]\n    /* height * width */\n    ];\n  } else if (!isChannelsLast && length === 1 && shape[0] > 1) {\n    return [shape[0], 1];\n  } else {\n    return null;\n  }\n} // For 1x1 kernels that iterate through every point in the input, convolution\n// can be expressed as matrix multiplication (without need for memory\n// remapping).\n\n\nexport function conv2dByMatMul({\n  x,\n  filter,\n  convInfo,\n  backend,\n  bias = null,\n  preluActivationWeights = null,\n  leakyreluAlpha = 0,\n  activation = null\n}) {\n  // Reshapes conv2D input to 2D tensors, uses matMul and then reshape the\n  // result from 2D to 4D.\n  const xShape = x.shape;\n  const xTexData = backend.texData.get(x.dataId);\n  const sharedMatMulDim = convInfo.inChannels;\n  const outerShapeX = xShape[0] * xShape[1] * xShape[2];\n  const outerShapeFilter = convInfo.outChannels;\n  const isChannelsLast = convInfo.dataFormat === 'channelsLast';\n  const transposeA = false;\n  const transposeB = false;\n  let out;\n  const intermediates = [];\n\n  if (preluActivationWeights != null) {\n    const targetShape = getShapeForBatchMatMul(preluActivationWeights.shape, isChannelsLast);\n\n    if (targetShape != null) {\n      preluActivationWeights = reshape({\n        inputs: {\n          x: preluActivationWeights\n        },\n        backend,\n        attrs: {\n          shape: targetShape\n        }\n      });\n      intermediates.push(preluActivationWeights);\n    }\n  }\n\n  if (bias != null) {\n    const targetShape = getShapeForBatchMatMul(bias.shape, isChannelsLast);\n\n    if (targetShape != null) {\n      bias = reshape({\n        inputs: {\n          x: bias\n        },\n        backend,\n        attrs: {\n          shape: targetShape\n        }\n      });\n      intermediates.push(bias);\n    }\n  } // TODO: Once reduction ops are packed, batchMatMul will always be packed\n  // and we can remove this condition.\n\n\n  const batchMatMulWillBeUnpacked = (outerShapeX === 1 || outerShapeFilter === 1) && sharedMatMulDim > MATMUL_SHARED_DIM_THRESHOLD; // The algorithm in the if condition assumes (1) the output will be packed,\n  // (2) x is packed, (3) x isChannelsLast, (4)  x's packed texture is already\n  // on GPU, (5) col is odd, (6) the width, height and inChannels are the same\n  // for xTexData.shape and xShape.\n\n  const canOptimize = !batchMatMulWillBeUnpacked && xTexData.isPacked && isChannelsLast && xTexData.texture != null && xShape[2] % 2 !== 0 && util.arraysEqual(xTexData.shape.slice(-3), xShape.slice(-3));\n\n  if (canOptimize) {\n    // We avoid expensive packed 2x2 reshape by padding col count to next,\n    // even number. When col is odd, the result of packed batchMatMul is\n    // the same (has the same texture layout and and values in the texture) as\n    // it is for next even col. We make the odd-cols tensor to look like\n    // even-cols tensor before the operation and, after the batchMatMul,\n    // fix the even-cols result to have odd number of cols.\n    const targetShape = xShape[0] * xShape[1] * (xShape[2] + 1);\n    const xReshaped = {\n      dataId: x.dataId,\n      shape: [1, targetShape, convInfo.inChannels],\n      dtype: x.dtype\n    }; // xTexData.shape gets referenced from GPGPUBinary.inShapeInfos.\n    // Decrementing col count, after batchMatMul->...->compileProgram leads to\n    // invalid col count within the reference in GPGPUBinary.inShapeInfos.\n    // Alternative fix would be to provide a copy to GPGPUBinary.inShapeInfos\n    // in compileProgram method, but that would affect compilation of all\n    // programs - instead, provide a copy here, with even col count, before\n    // calling batchMatMul->...->compileProgram and after that, the original\n    // xTexData.shape is restored.\n\n    const originalXTexDataShape = xTexData.shape;\n    xTexData.shape = xTexData.shape.slice();\n    xTexData.shape[xTexData.shape.length - 2]++;\n    util.assert(webgl_util.isReshapeFree(xTexData.shape, xReshaped.shape), () => `packed reshape ${xTexData.shape} to ${xReshaped.shape} isn't free`);\n    const filterReshaped = reshape({\n      inputs: {\n        x: filter\n      },\n      backend,\n      attrs: {\n        shape: [1, convInfo.inChannels, convInfo.outChannels]\n      }\n    });\n    intermediates.push(filterReshaped);\n    const pointwiseConv = batchMatMulImpl({\n      a: xReshaped,\n      b: filterReshaped,\n      backend,\n      transposeA,\n      transposeB,\n      bias,\n      activation,\n      preluActivationWeights,\n      leakyreluAlpha\n    });\n    const pointwiseConvTexData = backend.texData.get(pointwiseConv.dataId);\n    util.assert(pointwiseConvTexData.isPacked, () => 'batchMatMul result is expected to be packed'); // Restore the input shape to original.\n\n    xTexData.shape = originalXTexDataShape; // Set the output shape - there is no need for expensive reshape as data\n    // layout is already correct.\n\n    pointwiseConvTexData.shape = convInfo.outShape;\n    out = identity({\n      inputs: {\n        x: pointwiseConv\n      },\n      backend\n    });\n    out.shape = convInfo.outShape;\n    intermediates.push(pointwiseConv);\n  } else {\n    const numCols = convInfo.outHeight * convInfo.outWidth;\n    const xReshaped = reshape({\n      inputs: {\n        x\n      },\n      backend,\n      attrs: {\n        shape: isChannelsLast ? [convInfo.batchSize, numCols, convInfo.inChannels] : [convInfo.batchSize, convInfo.inChannels, numCols]\n      }\n    });\n    const filterReshaped = reshape({\n      inputs: {\n        x: filter\n      },\n      backend,\n      attrs: {\n        shape: [1, convInfo.inChannels, convInfo.outChannels]\n      }\n    });\n    const result = batchMatMulImpl({\n      a: isChannelsLast ? xReshaped : filterReshaped,\n      b: isChannelsLast ? filterReshaped : xReshaped,\n      transposeA: !isChannelsLast,\n      transposeB,\n      backend,\n      bias,\n      activation,\n      preluActivationWeights,\n      leakyreluAlpha\n    });\n    out = reshape({\n      inputs: {\n        x: result\n      },\n      backend,\n      attrs: {\n        shape: convInfo.outShape\n      }\n    });\n    intermediates.push(xReshaped);\n    intermediates.push(filterReshaped);\n    intermediates.push(result);\n  }\n\n  for (const i of intermediates) {\n    backend.disposeIntermediateTensorInfo(i);\n  }\n\n  return out;\n} // Implements the im2row algorithm as outlined in \"High Performance\n// Convolutional Neural Networks for Document Processing\" (Suvisoft, 2006)\n\nexport function conv2dWithIm2Row({\n  x,\n  filter,\n  convInfo,\n  backend,\n  bias = null,\n  preluActivationWeights = null,\n  leakyreluAlpha = 0,\n  activation = null\n}) {\n  // Rearranges conv2d input so each block to be convolved over forms the\n  // column of a new matrix with shape [filterWidth * filterHeight *\n  // inChannels, outHeight * outWidth]. The filter is also rearranged so each\n  // output channel forms a row of a new matrix with shape [outChannels,\n  // filterWidth * filterHeight * inChannels]. The convolution is then\n  // computed by multiplying these matrices and reshaping the result.\n  const {\n    filterWidth,\n    filterHeight,\n    inChannels,\n    outWidth,\n    outHeight,\n    dataFormat\n  } = convInfo;\n  const isChannelsLast = dataFormat === 'channelsLast';\n  const sharedDim = filterWidth * filterHeight * inChannels;\n  const numCols = outHeight * outWidth;\n  const x2ColShape = [convInfo.batchSize, sharedDim, numCols];\n  const transposeA = true;\n  const transposeB = false;\n  const intermediates = [];\n\n  if (preluActivationWeights != null) {\n    const targetShape = getShapeForBatchMatMul(preluActivationWeights.shape, isChannelsLast);\n\n    if (targetShape != null) {\n      preluActivationWeights = reshape({\n        inputs: {\n          x: preluActivationWeights\n        },\n        backend,\n        attrs: {\n          shape: targetShape\n        }\n      });\n      intermediates.push(preluActivationWeights);\n    }\n  }\n\n  if (bias != null) {\n    const targetShape = getShapeForBatchMatMul(bias.shape, isChannelsLast);\n\n    if (targetShape != null) {\n      bias = reshape({\n        inputs: {\n          x: bias\n        },\n        backend,\n        attrs: {\n          shape: targetShape\n        }\n      });\n      intermediates.push(bias);\n    }\n  }\n\n  const w2Row = reshape({\n    inputs: {\n      x: filter\n    },\n    backend,\n    attrs: {\n      shape: [1, sharedDim, util.sizeFromShape(filter.shape) / sharedDim]\n    }\n  });\n  intermediates.push(w2Row);\n  const im2ColProgram = new Im2ColPackedProgram(x2ColShape, convInfo);\n  const customValues = [x.shape, [convInfo.padInfo.top, convInfo.padInfo.left], [convInfo.strideHeight, convInfo.strideWidth], [convInfo.dilationHeight, convInfo.dilationWidth], [convInfo.inChannels], [convInfo.filterWidth * convInfo.inChannels], [convInfo.outWidth]];\n  const im2Col = backend.runWebGLProgram(im2ColProgram, [x], 'float32', customValues);\n  const im2ColReshaped = reshape({\n    inputs: {\n      x: im2Col\n    },\n    backend,\n    attrs: {\n      shape: x2ColShape\n    }\n  });\n  intermediates.push(im2Col);\n  intermediates.push(im2ColReshaped);\n  const hasBias = bias != null;\n  const hasPreluActivationWeights = preluActivationWeights != null;\n  const hasLeakyreluAlpha = activation === 'leakyrelu';\n  const fusedActivation = activation ? mapActivationToShaderProgram(activation, true) : null;\n  const matmulProgram = new MatMulPackedProgram(isChannelsLast ? im2ColReshaped.shape : w2Row.shape, isChannelsLast ? w2Row.shape : im2ColReshaped.shape, isChannelsLast ? [convInfo.batchSize, numCols, convInfo.outChannels] : [convInfo.batchSize, convInfo.outChannels, numCols], transposeA, transposeB, hasBias, fusedActivation, hasPreluActivationWeights, hasLeakyreluAlpha);\n  const inputs = isChannelsLast ? [im2ColReshaped, w2Row] : [w2Row, im2ColReshaped];\n\n  if (bias) {\n    inputs.push(bias);\n  }\n\n  if (hasPreluActivationWeights) {\n    inputs.push(preluActivationWeights);\n  }\n\n  if (hasLeakyreluAlpha) {\n    const $leakyreluAlpha = backend.makeTensorInfo([], 'float32', util.createScalarValue(leakyreluAlpha, 'float32'));\n    inputs.push($leakyreluAlpha);\n    intermediates.push($leakyreluAlpha);\n  }\n\n  const product = backend.runWebGLProgram(matmulProgram, inputs, 'float32');\n  const out = reshape({\n    inputs: {\n      x: product\n    },\n    backend,\n    attrs: {\n      shape: convInfo.outShape\n    }\n  });\n  intermediates.push(product);\n\n  for (const i of intermediates) {\n    backend.disposeIntermediateTensorInfo(i);\n  }\n\n  return out;\n}","map":{"version":3,"names":["util","Im2ColPackedProgram","mapActivationToShaderProgram","MatMulPackedProgram","webgl_util","batchMatMulImpl","MATMUL_SHARED_DIM_THRESHOLD","identity","reshape","getShapeForBatchMatMul","shape","isChannelsLast","length","slice","conv2dByMatMul","x","filter","convInfo","backend","bias","preluActivationWeights","leakyreluAlpha","activation","xShape","xTexData","texData","get","dataId","sharedMatMulDim","inChannels","outerShapeX","outerShapeFilter","outChannels","dataFormat","transposeA","transposeB","out","intermediates","targetShape","inputs","attrs","push","batchMatMulWillBeUnpacked","canOptimize","isPacked","texture","arraysEqual","xReshaped","dtype","originalXTexDataShape","assert","isReshapeFree","filterReshaped","pointwiseConv","a","b","pointwiseConvTexData","outShape","numCols","outHeight","outWidth","batchSize","result","i","disposeIntermediateTensorInfo","conv2dWithIm2Row","filterWidth","filterHeight","sharedDim","x2ColShape","w2Row","sizeFromShape","im2ColProgram","customValues","padInfo","top","left","strideHeight","strideWidth","dilationHeight","dilationWidth","im2Col","runWebGLProgram","im2ColReshaped","hasBias","hasPreluActivationWeights","hasLeakyreluAlpha","fusedActivation","matmulProgram","$leakyreluAlpha","makeTensorInfo","createScalarValue","product"],"sources":["C:/Users/Anke/Documents/Feelix documents/Feelix2.0-dev/Feelix v2/node_modules/@tensorflow/tfjs-backend-webgl/dist/kernels/Conv2D_impl.js"],"sourcesContent":["/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { util } from '@tensorflow/tfjs-core';\nimport { Im2ColPackedProgram } from '../im2col_packed_gpu';\nimport { mapActivationToShaderProgram } from '../kernel_utils/kernel_funcs_utils';\nimport { MatMulPackedProgram } from '../mulmat_packed_gpu';\nimport * as webgl_util from '../webgl_util';\nimport { batchMatMulImpl, MATMUL_SHARED_DIM_THRESHOLD } from './BatchMatMul_impl';\nimport { identity } from './Identity';\nimport { reshape } from './Reshape';\n// Both conv2dByMatMul and conv2dWithIm2Row fuse height and width into one\n// dimension to compute batchMatMul, so bias and activation weights are also\n// supposed to fuse the two dimensions into one.\n//\n// This function computes the target shape for fusing height and width\n// dimensions. Returning null means the shape is already compatible.\n//\n// Even though the bias is not supposed to be a 3-D or a 4-D (including\n// batch) tensor and PReLU activiation weights is not supposed to be a 4-D\n// tensor, we still need to support them, because we haven't disabled\n// them for NHWC format.\n// https://github.com/tensorflow/tfjs/blob/b53bd47e880367ae57493f0ea628abaf08db2d5d/tfjs-core/src/ops/fused/conv2d.ts#L181-L196\nfunction getShapeForBatchMatMul(shape, isChannelsLast) {\n    const length = shape.length;\n    if (length >= 3) {\n        return isChannelsLast ?\n            [\n                ...shape.slice(0, -3) /* batch */,\n                shape[length - 3] * shape[length - 2] /* height * width */,\n                shape[length - 1] /* channel */\n            ] :\n            [\n                ...shape.slice(0, -3) /* batch */, shape[length - 3] /* channel */,\n                shape[length - 2] * shape[length - 1] /* height * width */\n            ];\n    }\n    else if (!isChannelsLast && length === 1 && shape[0] > 1) {\n        return [shape[0], 1];\n    }\n    else {\n        return null;\n    }\n}\n// For 1x1 kernels that iterate through every point in the input, convolution\n// can be expressed as matrix multiplication (without need for memory\n// remapping).\nexport function conv2dByMatMul({ x, filter, convInfo, backend, bias = null, preluActivationWeights = null, leakyreluAlpha = 0, activation = null }) {\n    // Reshapes conv2D input to 2D tensors, uses matMul and then reshape the\n    // result from 2D to 4D.\n    const xShape = x.shape;\n    const xTexData = backend.texData.get(x.dataId);\n    const sharedMatMulDim = convInfo.inChannels;\n    const outerShapeX = xShape[0] * xShape[1] * xShape[2];\n    const outerShapeFilter = convInfo.outChannels;\n    const isChannelsLast = convInfo.dataFormat === 'channelsLast';\n    const transposeA = false;\n    const transposeB = false;\n    let out;\n    const intermediates = [];\n    if (preluActivationWeights != null) {\n        const targetShape = getShapeForBatchMatMul(preluActivationWeights.shape, isChannelsLast);\n        if (targetShape != null) {\n            preluActivationWeights = reshape({\n                inputs: { x: preluActivationWeights },\n                backend,\n                attrs: { shape: targetShape }\n            });\n            intermediates.push(preluActivationWeights);\n        }\n    }\n    if (bias != null) {\n        const targetShape = getShapeForBatchMatMul(bias.shape, isChannelsLast);\n        if (targetShape != null) {\n            bias = reshape({ inputs: { x: bias }, backend, attrs: { shape: targetShape } });\n            intermediates.push(bias);\n        }\n    }\n    // TODO: Once reduction ops are packed, batchMatMul will always be packed\n    // and we can remove this condition.\n    const batchMatMulWillBeUnpacked = (outerShapeX === 1 || outerShapeFilter === 1) &&\n        sharedMatMulDim > MATMUL_SHARED_DIM_THRESHOLD;\n    // The algorithm in the if condition assumes (1) the output will be packed,\n    // (2) x is packed, (3) x isChannelsLast, (4)  x's packed texture is already\n    // on GPU, (5) col is odd, (6) the width, height and inChannels are the same\n    // for xTexData.shape and xShape.\n    const canOptimize = !batchMatMulWillBeUnpacked && xTexData.isPacked &&\n        isChannelsLast && xTexData.texture != null && xShape[2] % 2 !== 0 &&\n        util.arraysEqual(xTexData.shape.slice(-3), xShape.slice(-3));\n    if (canOptimize) {\n        // We avoid expensive packed 2x2 reshape by padding col count to next,\n        // even number. When col is odd, the result of packed batchMatMul is\n        // the same (has the same texture layout and and values in the texture) as\n        // it is for next even col. We make the odd-cols tensor to look like\n        // even-cols tensor before the operation and, after the batchMatMul,\n        // fix the even-cols result to have odd number of cols.\n        const targetShape = xShape[0] * xShape[1] * (xShape[2] + 1);\n        const xReshaped = {\n            dataId: x.dataId,\n            shape: [1, targetShape, convInfo.inChannels],\n            dtype: x.dtype\n        };\n        // xTexData.shape gets referenced from GPGPUBinary.inShapeInfos.\n        // Decrementing col count, after batchMatMul->...->compileProgram leads to\n        // invalid col count within the reference in GPGPUBinary.inShapeInfos.\n        // Alternative fix would be to provide a copy to GPGPUBinary.inShapeInfos\n        // in compileProgram method, but that would affect compilation of all\n        // programs - instead, provide a copy here, with even col count, before\n        // calling batchMatMul->...->compileProgram and after that, the original\n        // xTexData.shape is restored.\n        const originalXTexDataShape = xTexData.shape;\n        xTexData.shape = xTexData.shape.slice();\n        xTexData.shape[xTexData.shape.length - 2]++;\n        util.assert(webgl_util.isReshapeFree(xTexData.shape, xReshaped.shape), () => `packed reshape ${xTexData.shape} to ${xReshaped.shape} isn't free`);\n        const filterReshaped = reshape({\n            inputs: { x: filter },\n            backend,\n            attrs: { shape: [1, convInfo.inChannels, convInfo.outChannels] }\n        });\n        intermediates.push(filterReshaped);\n        const pointwiseConv = batchMatMulImpl({\n            a: xReshaped,\n            b: filterReshaped,\n            backend,\n            transposeA,\n            transposeB,\n            bias,\n            activation,\n            preluActivationWeights,\n            leakyreluAlpha\n        });\n        const pointwiseConvTexData = backend.texData.get(pointwiseConv.dataId);\n        util.assert(pointwiseConvTexData.isPacked, () => 'batchMatMul result is expected to be packed');\n        // Restore the input shape to original.\n        xTexData.shape = originalXTexDataShape;\n        // Set the output shape - there is no need for expensive reshape as data\n        // layout is already correct.\n        pointwiseConvTexData.shape = convInfo.outShape;\n        out = identity({ inputs: { x: pointwiseConv }, backend });\n        out.shape = convInfo.outShape;\n        intermediates.push(pointwiseConv);\n    }\n    else {\n        const numCols = convInfo.outHeight * convInfo.outWidth;\n        const xReshaped = reshape({\n            inputs: { x },\n            backend,\n            attrs: {\n                shape: isChannelsLast ?\n                    [convInfo.batchSize, numCols, convInfo.inChannels] :\n                    [convInfo.batchSize, convInfo.inChannels, numCols]\n            }\n        });\n        const filterReshaped = reshape({\n            inputs: { x: filter },\n            backend,\n            attrs: { shape: [1, convInfo.inChannels, convInfo.outChannels] }\n        });\n        const result = batchMatMulImpl({\n            a: isChannelsLast ? xReshaped : filterReshaped,\n            b: isChannelsLast ? filterReshaped : xReshaped,\n            transposeA: !isChannelsLast,\n            transposeB,\n            backend,\n            bias,\n            activation,\n            preluActivationWeights,\n            leakyreluAlpha\n        });\n        out = reshape({ inputs: { x: result }, backend, attrs: { shape: convInfo.outShape } });\n        intermediates.push(xReshaped);\n        intermediates.push(filterReshaped);\n        intermediates.push(result);\n    }\n    for (const i of intermediates) {\n        backend.disposeIntermediateTensorInfo(i);\n    }\n    return out;\n}\n// Implements the im2row algorithm as outlined in \"High Performance\n// Convolutional Neural Networks for Document Processing\" (Suvisoft, 2006)\nexport function conv2dWithIm2Row({ x, filter, convInfo, backend, bias = null, preluActivationWeights = null, leakyreluAlpha = 0, activation = null }) {\n    // Rearranges conv2d input so each block to be convolved over forms the\n    // column of a new matrix with shape [filterWidth * filterHeight *\n    // inChannels, outHeight * outWidth]. The filter is also rearranged so each\n    // output channel forms a row of a new matrix with shape [outChannels,\n    // filterWidth * filterHeight * inChannels]. The convolution is then\n    // computed by multiplying these matrices and reshaping the result.\n    const { filterWidth, filterHeight, inChannels, outWidth, outHeight, dataFormat } = convInfo;\n    const isChannelsLast = dataFormat === 'channelsLast';\n    const sharedDim = filterWidth * filterHeight * inChannels;\n    const numCols = outHeight * outWidth;\n    const x2ColShape = [convInfo.batchSize, sharedDim, numCols];\n    const transposeA = true;\n    const transposeB = false;\n    const intermediates = [];\n    if (preluActivationWeights != null) {\n        const targetShape = getShapeForBatchMatMul(preluActivationWeights.shape, isChannelsLast);\n        if (targetShape != null) {\n            preluActivationWeights = reshape({\n                inputs: { x: preluActivationWeights },\n                backend,\n                attrs: { shape: targetShape }\n            });\n            intermediates.push(preluActivationWeights);\n        }\n    }\n    if (bias != null) {\n        const targetShape = getShapeForBatchMatMul(bias.shape, isChannelsLast);\n        if (targetShape != null) {\n            bias = reshape({ inputs: { x: bias }, backend, attrs: { shape: targetShape } });\n            intermediates.push(bias);\n        }\n    }\n    const w2Row = reshape({\n        inputs: { x: filter },\n        backend,\n        attrs: { shape: [1, sharedDim, util.sizeFromShape(filter.shape) / sharedDim] }\n    });\n    intermediates.push(w2Row);\n    const im2ColProgram = new Im2ColPackedProgram(x2ColShape, convInfo);\n    const customValues = [\n        x.shape, [convInfo.padInfo.top, convInfo.padInfo.left],\n        [convInfo.strideHeight, convInfo.strideWidth],\n        [convInfo.dilationHeight, convInfo.dilationWidth], [convInfo.inChannels],\n        [convInfo.filterWidth * convInfo.inChannels], [convInfo.outWidth]\n    ];\n    const im2Col = backend.runWebGLProgram(im2ColProgram, [x], 'float32', customValues);\n    const im2ColReshaped = reshape({ inputs: { x: im2Col }, backend, attrs: { shape: x2ColShape } });\n    intermediates.push(im2Col);\n    intermediates.push(im2ColReshaped);\n    const hasBias = bias != null;\n    const hasPreluActivationWeights = preluActivationWeights != null;\n    const hasLeakyreluAlpha = activation === 'leakyrelu';\n    const fusedActivation = activation ? mapActivationToShaderProgram(activation, true) : null;\n    const matmulProgram = new MatMulPackedProgram(isChannelsLast ? im2ColReshaped.shape :\n        w2Row.shape, isChannelsLast ? w2Row.shape :\n        im2ColReshaped.shape, isChannelsLast ? [convInfo.batchSize, numCols, convInfo.outChannels] :\n        [convInfo.batchSize, convInfo.outChannels, numCols], transposeA, transposeB, hasBias, fusedActivation, hasPreluActivationWeights, hasLeakyreluAlpha);\n    const inputs = isChannelsLast ? [im2ColReshaped, w2Row] : [w2Row, im2ColReshaped];\n    if (bias) {\n        inputs.push(bias);\n    }\n    if (hasPreluActivationWeights) {\n        inputs.push(preluActivationWeights);\n    }\n    if (hasLeakyreluAlpha) {\n        const $leakyreluAlpha = backend.makeTensorInfo([], 'float32', util.createScalarValue(leakyreluAlpha, 'float32'));\n        inputs.push($leakyreluAlpha);\n        intermediates.push($leakyreluAlpha);\n    }\n    const product = backend.runWebGLProgram(matmulProgram, inputs, 'float32');\n    const out = reshape({ inputs: { x: product }, backend, attrs: { shape: convInfo.outShape } });\n    intermediates.push(product);\n    for (const i of intermediates) {\n        backend.disposeIntermediateTensorInfo(i);\n    }\n    return out;\n}\n"],"mappings":"AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAASA,IAAT,QAAqB,uBAArB;AACA,SAASC,mBAAT,QAAoC,sBAApC;AACA,SAASC,4BAAT,QAA6C,oCAA7C;AACA,SAASC,mBAAT,QAAoC,sBAApC;AACA,OAAO,KAAKC,UAAZ,MAA4B,eAA5B;AACA,SAASC,eAAT,EAA0BC,2BAA1B,QAA6D,oBAA7D;AACA,SAASC,QAAT,QAAyB,YAAzB;AACA,SAASC,OAAT,QAAwB,WAAxB,C,CACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AACA,SAASC,sBAAT,CAAgCC,KAAhC,EAAuCC,cAAvC,EAAuD;EACnD,MAAMC,MAAM,GAAGF,KAAK,CAACE,MAArB;;EACA,IAAIA,MAAM,IAAI,CAAd,EAAiB;IACb,OAAOD,cAAc,GACjB,CACI,GAAGD,KAAK,CAACG,KAAN,CAAY,CAAZ,EAAe,CAAC,CAAhB;IAAmB;IAD1B,EAEIH,KAAK,CAACE,MAAM,GAAG,CAAV,CAAL,GAAoBF,KAAK,CAACE,MAAM,GAAG,CAAV;IAAa;IAF1C,EAGIF,KAAK,CAACE,MAAM,GAAG,CAAV;IAAa;IAHtB,CADiB,GAMjB,CACI,GAAGF,KAAK,CAACG,KAAN,CAAY,CAAZ,EAAe,CAAC,CAAhB;IAAmB;IAD1B,EACuCH,KAAK,CAACE,MAAM,GAAG,CAAV;IAAa;IADzD,EAEIF,KAAK,CAACE,MAAM,GAAG,CAAV,CAAL,GAAoBF,KAAK,CAACE,MAAM,GAAG,CAAV;IAAa;IAF1C,CANJ;EAUH,CAXD,MAYK,IAAI,CAACD,cAAD,IAAmBC,MAAM,KAAK,CAA9B,IAAmCF,KAAK,CAAC,CAAD,CAAL,GAAW,CAAlD,EAAqD;IACtD,OAAO,CAACA,KAAK,CAAC,CAAD,CAAN,EAAW,CAAX,CAAP;EACH,CAFI,MAGA;IACD,OAAO,IAAP;EACH;AACJ,C,CACD;AACA;AACA;;;AACA,OAAO,SAASI,cAAT,CAAwB;EAAEC,CAAF;EAAKC,MAAL;EAAaC,QAAb;EAAuBC,OAAvB;EAAgCC,IAAI,GAAG,IAAvC;EAA6CC,sBAAsB,GAAG,IAAtE;EAA4EC,cAAc,GAAG,CAA7F;EAAgGC,UAAU,GAAG;AAA7G,CAAxB,EAA6I;EAChJ;EACA;EACA,MAAMC,MAAM,GAAGR,CAAC,CAACL,KAAjB;EACA,MAAMc,QAAQ,GAAGN,OAAO,CAACO,OAAR,CAAgBC,GAAhB,CAAoBX,CAAC,CAACY,MAAtB,CAAjB;EACA,MAAMC,eAAe,GAAGX,QAAQ,CAACY,UAAjC;EACA,MAAMC,WAAW,GAAGP,MAAM,CAAC,CAAD,CAAN,GAAYA,MAAM,CAAC,CAAD,CAAlB,GAAwBA,MAAM,CAAC,CAAD,CAAlD;EACA,MAAMQ,gBAAgB,GAAGd,QAAQ,CAACe,WAAlC;EACA,MAAMrB,cAAc,GAAGM,QAAQ,CAACgB,UAAT,KAAwB,cAA/C;EACA,MAAMC,UAAU,GAAG,KAAnB;EACA,MAAMC,UAAU,GAAG,KAAnB;EACA,IAAIC,GAAJ;EACA,MAAMC,aAAa,GAAG,EAAtB;;EACA,IAAIjB,sBAAsB,IAAI,IAA9B,EAAoC;IAChC,MAAMkB,WAAW,GAAG7B,sBAAsB,CAACW,sBAAsB,CAACV,KAAxB,EAA+BC,cAA/B,CAA1C;;IACA,IAAI2B,WAAW,IAAI,IAAnB,EAAyB;MACrBlB,sBAAsB,GAAGZ,OAAO,CAAC;QAC7B+B,MAAM,EAAE;UAAExB,CAAC,EAAEK;QAAL,CADqB;QAE7BF,OAF6B;QAG7BsB,KAAK,EAAE;UAAE9B,KAAK,EAAE4B;QAAT;MAHsB,CAAD,CAAhC;MAKAD,aAAa,CAACI,IAAd,CAAmBrB,sBAAnB;IACH;EACJ;;EACD,IAAID,IAAI,IAAI,IAAZ,EAAkB;IACd,MAAMmB,WAAW,GAAG7B,sBAAsB,CAACU,IAAI,CAACT,KAAN,EAAaC,cAAb,CAA1C;;IACA,IAAI2B,WAAW,IAAI,IAAnB,EAAyB;MACrBnB,IAAI,GAAGX,OAAO,CAAC;QAAE+B,MAAM,EAAE;UAAExB,CAAC,EAAEI;QAAL,CAAV;QAAuBD,OAAvB;QAAgCsB,KAAK,EAAE;UAAE9B,KAAK,EAAE4B;QAAT;MAAvC,CAAD,CAAd;MACAD,aAAa,CAACI,IAAd,CAAmBtB,IAAnB;IACH;EACJ,CA9B+I,CA+BhJ;EACA;;;EACA,MAAMuB,yBAAyB,GAAG,CAACZ,WAAW,KAAK,CAAhB,IAAqBC,gBAAgB,KAAK,CAA3C,KAC9BH,eAAe,GAAGtB,2BADtB,CAjCgJ,CAmChJ;EACA;EACA;EACA;;EACA,MAAMqC,WAAW,GAAG,CAACD,yBAAD,IAA8BlB,QAAQ,CAACoB,QAAvC,IAChBjC,cADgB,IACEa,QAAQ,CAACqB,OAAT,IAAoB,IADtB,IAC8BtB,MAAM,CAAC,CAAD,CAAN,GAAY,CAAZ,KAAkB,CADhD,IAEhBvB,IAAI,CAAC8C,WAAL,CAAiBtB,QAAQ,CAACd,KAAT,CAAeG,KAAf,CAAqB,CAAC,CAAtB,CAAjB,EAA2CU,MAAM,CAACV,KAAP,CAAa,CAAC,CAAd,CAA3C,CAFJ;;EAGA,IAAI8B,WAAJ,EAAiB;IACb;IACA;IACA;IACA;IACA;IACA;IACA,MAAML,WAAW,GAAGf,MAAM,CAAC,CAAD,CAAN,GAAYA,MAAM,CAAC,CAAD,CAAlB,IAAyBA,MAAM,CAAC,CAAD,CAAN,GAAY,CAArC,CAApB;IACA,MAAMwB,SAAS,GAAG;MACdpB,MAAM,EAAEZ,CAAC,CAACY,MADI;MAEdjB,KAAK,EAAE,CAAC,CAAD,EAAI4B,WAAJ,EAAiBrB,QAAQ,CAACY,UAA1B,CAFO;MAGdmB,KAAK,EAAEjC,CAAC,CAACiC;IAHK,CAAlB,CARa,CAab;IACA;IACA;IACA;IACA;IACA;IACA;IACA;;IACA,MAAMC,qBAAqB,GAAGzB,QAAQ,CAACd,KAAvC;IACAc,QAAQ,CAACd,KAAT,GAAiBc,QAAQ,CAACd,KAAT,CAAeG,KAAf,EAAjB;IACAW,QAAQ,CAACd,KAAT,CAAec,QAAQ,CAACd,KAAT,CAAeE,MAAf,GAAwB,CAAvC;IACAZ,IAAI,CAACkD,MAAL,CAAY9C,UAAU,CAAC+C,aAAX,CAAyB3B,QAAQ,CAACd,KAAlC,EAAyCqC,SAAS,CAACrC,KAAnD,CAAZ,EAAuE,MAAO,kBAAiBc,QAAQ,CAACd,KAAM,OAAMqC,SAAS,CAACrC,KAAM,aAApI;IACA,MAAM0C,cAAc,GAAG5C,OAAO,CAAC;MAC3B+B,MAAM,EAAE;QAAExB,CAAC,EAAEC;MAAL,CADmB;MAE3BE,OAF2B;MAG3BsB,KAAK,EAAE;QAAE9B,KAAK,EAAE,CAAC,CAAD,EAAIO,QAAQ,CAACY,UAAb,EAAyBZ,QAAQ,CAACe,WAAlC;MAAT;IAHoB,CAAD,CAA9B;IAKAK,aAAa,CAACI,IAAd,CAAmBW,cAAnB;IACA,MAAMC,aAAa,GAAGhD,eAAe,CAAC;MAClCiD,CAAC,EAAEP,SAD+B;MAElCQ,CAAC,EAAEH,cAF+B;MAGlClC,OAHkC;MAIlCgB,UAJkC;MAKlCC,UALkC;MAMlChB,IANkC;MAOlCG,UAPkC;MAQlCF,sBARkC;MASlCC;IATkC,CAAD,CAArC;IAWA,MAAMmC,oBAAoB,GAAGtC,OAAO,CAACO,OAAR,CAAgBC,GAAhB,CAAoB2B,aAAa,CAAC1B,MAAlC,CAA7B;IACA3B,IAAI,CAACkD,MAAL,CAAYM,oBAAoB,CAACZ,QAAjC,EAA2C,MAAM,6CAAjD,EA3Ca,CA4Cb;;IACApB,QAAQ,CAACd,KAAT,GAAiBuC,qBAAjB,CA7Ca,CA8Cb;IACA;;IACAO,oBAAoB,CAAC9C,KAArB,GAA6BO,QAAQ,CAACwC,QAAtC;IACArB,GAAG,GAAG7B,QAAQ,CAAC;MAAEgC,MAAM,EAAE;QAAExB,CAAC,EAAEsC;MAAL,CAAV;MAAgCnC;IAAhC,CAAD,CAAd;IACAkB,GAAG,CAAC1B,KAAJ,GAAYO,QAAQ,CAACwC,QAArB;IACApB,aAAa,CAACI,IAAd,CAAmBY,aAAnB;EACH,CApDD,MAqDK;IACD,MAAMK,OAAO,GAAGzC,QAAQ,CAAC0C,SAAT,GAAqB1C,QAAQ,CAAC2C,QAA9C;IACA,MAAMb,SAAS,GAAGvC,OAAO,CAAC;MACtB+B,MAAM,EAAE;QAAExB;MAAF,CADc;MAEtBG,OAFsB;MAGtBsB,KAAK,EAAE;QACH9B,KAAK,EAAEC,cAAc,GACjB,CAACM,QAAQ,CAAC4C,SAAV,EAAqBH,OAArB,EAA8BzC,QAAQ,CAACY,UAAvC,CADiB,GAEjB,CAACZ,QAAQ,CAAC4C,SAAV,EAAqB5C,QAAQ,CAACY,UAA9B,EAA0C6B,OAA1C;MAHD;IAHe,CAAD,CAAzB;IASA,MAAMN,cAAc,GAAG5C,OAAO,CAAC;MAC3B+B,MAAM,EAAE;QAAExB,CAAC,EAAEC;MAAL,CADmB;MAE3BE,OAF2B;MAG3BsB,KAAK,EAAE;QAAE9B,KAAK,EAAE,CAAC,CAAD,EAAIO,QAAQ,CAACY,UAAb,EAAyBZ,QAAQ,CAACe,WAAlC;MAAT;IAHoB,CAAD,CAA9B;IAKA,MAAM8B,MAAM,GAAGzD,eAAe,CAAC;MAC3BiD,CAAC,EAAE3C,cAAc,GAAGoC,SAAH,GAAeK,cADL;MAE3BG,CAAC,EAAE5C,cAAc,GAAGyC,cAAH,GAAoBL,SAFV;MAG3Bb,UAAU,EAAE,CAACvB,cAHc;MAI3BwB,UAJ2B;MAK3BjB,OAL2B;MAM3BC,IAN2B;MAO3BG,UAP2B;MAQ3BF,sBAR2B;MAS3BC;IAT2B,CAAD,CAA9B;IAWAe,GAAG,GAAG5B,OAAO,CAAC;MAAE+B,MAAM,EAAE;QAAExB,CAAC,EAAE+C;MAAL,CAAV;MAAyB5C,OAAzB;MAAkCsB,KAAK,EAAE;QAAE9B,KAAK,EAAEO,QAAQ,CAACwC;MAAlB;IAAzC,CAAD,CAAb;IACApB,aAAa,CAACI,IAAd,CAAmBM,SAAnB;IACAV,aAAa,CAACI,IAAd,CAAmBW,cAAnB;IACAf,aAAa,CAACI,IAAd,CAAmBqB,MAAnB;EACH;;EACD,KAAK,MAAMC,CAAX,IAAgB1B,aAAhB,EAA+B;IAC3BnB,OAAO,CAAC8C,6BAAR,CAAsCD,CAAtC;EACH;;EACD,OAAO3B,GAAP;AACH,C,CACD;AACA;;AACA,OAAO,SAAS6B,gBAAT,CAA0B;EAAElD,CAAF;EAAKC,MAAL;EAAaC,QAAb;EAAuBC,OAAvB;EAAgCC,IAAI,GAAG,IAAvC;EAA6CC,sBAAsB,GAAG,IAAtE;EAA4EC,cAAc,GAAG,CAA7F;EAAgGC,UAAU,GAAG;AAA7G,CAA1B,EAA+I;EAClJ;EACA;EACA;EACA;EACA;EACA;EACA,MAAM;IAAE4C,WAAF;IAAeC,YAAf;IAA6BtC,UAA7B;IAAyC+B,QAAzC;IAAmDD,SAAnD;IAA8D1B;EAA9D,IAA6EhB,QAAnF;EACA,MAAMN,cAAc,GAAGsB,UAAU,KAAK,cAAtC;EACA,MAAMmC,SAAS,GAAGF,WAAW,GAAGC,YAAd,GAA6BtC,UAA/C;EACA,MAAM6B,OAAO,GAAGC,SAAS,GAAGC,QAA5B;EACA,MAAMS,UAAU,GAAG,CAACpD,QAAQ,CAAC4C,SAAV,EAAqBO,SAArB,EAAgCV,OAAhC,CAAnB;EACA,MAAMxB,UAAU,GAAG,IAAnB;EACA,MAAMC,UAAU,GAAG,KAAnB;EACA,MAAME,aAAa,GAAG,EAAtB;;EACA,IAAIjB,sBAAsB,IAAI,IAA9B,EAAoC;IAChC,MAAMkB,WAAW,GAAG7B,sBAAsB,CAACW,sBAAsB,CAACV,KAAxB,EAA+BC,cAA/B,CAA1C;;IACA,IAAI2B,WAAW,IAAI,IAAnB,EAAyB;MACrBlB,sBAAsB,GAAGZ,OAAO,CAAC;QAC7B+B,MAAM,EAAE;UAAExB,CAAC,EAAEK;QAAL,CADqB;QAE7BF,OAF6B;QAG7BsB,KAAK,EAAE;UAAE9B,KAAK,EAAE4B;QAAT;MAHsB,CAAD,CAAhC;MAKAD,aAAa,CAACI,IAAd,CAAmBrB,sBAAnB;IACH;EACJ;;EACD,IAAID,IAAI,IAAI,IAAZ,EAAkB;IACd,MAAMmB,WAAW,GAAG7B,sBAAsB,CAACU,IAAI,CAACT,KAAN,EAAaC,cAAb,CAA1C;;IACA,IAAI2B,WAAW,IAAI,IAAnB,EAAyB;MACrBnB,IAAI,GAAGX,OAAO,CAAC;QAAE+B,MAAM,EAAE;UAAExB,CAAC,EAAEI;QAAL,CAAV;QAAuBD,OAAvB;QAAgCsB,KAAK,EAAE;UAAE9B,KAAK,EAAE4B;QAAT;MAAvC,CAAD,CAAd;MACAD,aAAa,CAACI,IAAd,CAAmBtB,IAAnB;IACH;EACJ;;EACD,MAAMmD,KAAK,GAAG9D,OAAO,CAAC;IAClB+B,MAAM,EAAE;MAAExB,CAAC,EAAEC;IAAL,CADU;IAElBE,OAFkB;IAGlBsB,KAAK,EAAE;MAAE9B,KAAK,EAAE,CAAC,CAAD,EAAI0D,SAAJ,EAAepE,IAAI,CAACuE,aAAL,CAAmBvD,MAAM,CAACN,KAA1B,IAAmC0D,SAAlD;IAAT;EAHW,CAAD,CAArB;EAKA/B,aAAa,CAACI,IAAd,CAAmB6B,KAAnB;EACA,MAAME,aAAa,GAAG,IAAIvE,mBAAJ,CAAwBoE,UAAxB,EAAoCpD,QAApC,CAAtB;EACA,MAAMwD,YAAY,GAAG,CACjB1D,CAAC,CAACL,KADe,EACR,CAACO,QAAQ,CAACyD,OAAT,CAAiBC,GAAlB,EAAuB1D,QAAQ,CAACyD,OAAT,CAAiBE,IAAxC,CADQ,EAEjB,CAAC3D,QAAQ,CAAC4D,YAAV,EAAwB5D,QAAQ,CAAC6D,WAAjC,CAFiB,EAGjB,CAAC7D,QAAQ,CAAC8D,cAAV,EAA0B9D,QAAQ,CAAC+D,aAAnC,CAHiB,EAGkC,CAAC/D,QAAQ,CAACY,UAAV,CAHlC,EAIjB,CAACZ,QAAQ,CAACiD,WAAT,GAAuBjD,QAAQ,CAACY,UAAjC,CAJiB,EAI6B,CAACZ,QAAQ,CAAC2C,QAAV,CAJ7B,CAArB;EAMA,MAAMqB,MAAM,GAAG/D,OAAO,CAACgE,eAAR,CAAwBV,aAAxB,EAAuC,CAACzD,CAAD,CAAvC,EAA4C,SAA5C,EAAuD0D,YAAvD,CAAf;EACA,MAAMU,cAAc,GAAG3E,OAAO,CAAC;IAAE+B,MAAM,EAAE;MAAExB,CAAC,EAAEkE;IAAL,CAAV;IAAyB/D,OAAzB;IAAkCsB,KAAK,EAAE;MAAE9B,KAAK,EAAE2D;IAAT;EAAzC,CAAD,CAA9B;EACAhC,aAAa,CAACI,IAAd,CAAmBwC,MAAnB;EACA5C,aAAa,CAACI,IAAd,CAAmB0C,cAAnB;EACA,MAAMC,OAAO,GAAGjE,IAAI,IAAI,IAAxB;EACA,MAAMkE,yBAAyB,GAAGjE,sBAAsB,IAAI,IAA5D;EACA,MAAMkE,iBAAiB,GAAGhE,UAAU,KAAK,WAAzC;EACA,MAAMiE,eAAe,GAAGjE,UAAU,GAAGpB,4BAA4B,CAACoB,UAAD,EAAa,IAAb,CAA/B,GAAoD,IAAtF;EACA,MAAMkE,aAAa,GAAG,IAAIrF,mBAAJ,CAAwBQ,cAAc,GAAGwE,cAAc,CAACzE,KAAlB,GACxD4D,KAAK,CAAC5D,KADY,EACLC,cAAc,GAAG2D,KAAK,CAAC5D,KAAT,GAC3ByE,cAAc,CAACzE,KAFG,EAEIC,cAAc,GAAG,CAACM,QAAQ,CAAC4C,SAAV,EAAqBH,OAArB,EAA8BzC,QAAQ,CAACe,WAAvC,CAAH,GACpC,CAACf,QAAQ,CAAC4C,SAAV,EAAqB5C,QAAQ,CAACe,WAA9B,EAA2C0B,OAA3C,CAHkB,EAGmCxB,UAHnC,EAG+CC,UAH/C,EAG2DiD,OAH3D,EAGoEG,eAHpE,EAGqFF,yBAHrF,EAGgHC,iBAHhH,CAAtB;EAIA,MAAM/C,MAAM,GAAG5B,cAAc,GAAG,CAACwE,cAAD,EAAiBb,KAAjB,CAAH,GAA6B,CAACA,KAAD,EAAQa,cAAR,CAA1D;;EACA,IAAIhE,IAAJ,EAAU;IACNoB,MAAM,CAACE,IAAP,CAAYtB,IAAZ;EACH;;EACD,IAAIkE,yBAAJ,EAA+B;IAC3B9C,MAAM,CAACE,IAAP,CAAYrB,sBAAZ;EACH;;EACD,IAAIkE,iBAAJ,EAAuB;IACnB,MAAMG,eAAe,GAAGvE,OAAO,CAACwE,cAAR,CAAuB,EAAvB,EAA2B,SAA3B,EAAsC1F,IAAI,CAAC2F,iBAAL,CAAuBtE,cAAvB,EAAuC,SAAvC,CAAtC,CAAxB;IACAkB,MAAM,CAACE,IAAP,CAAYgD,eAAZ;IACApD,aAAa,CAACI,IAAd,CAAmBgD,eAAnB;EACH;;EACD,MAAMG,OAAO,GAAG1E,OAAO,CAACgE,eAAR,CAAwBM,aAAxB,EAAuCjD,MAAvC,EAA+C,SAA/C,CAAhB;EACA,MAAMH,GAAG,GAAG5B,OAAO,CAAC;IAAE+B,MAAM,EAAE;MAAExB,CAAC,EAAE6E;IAAL,CAAV;IAA0B1E,OAA1B;IAAmCsB,KAAK,EAAE;MAAE9B,KAAK,EAAEO,QAAQ,CAACwC;IAAlB;EAA1C,CAAD,CAAnB;EACApB,aAAa,CAACI,IAAd,CAAmBmD,OAAnB;;EACA,KAAK,MAAM7B,CAAX,IAAgB1B,aAAhB,EAA+B;IAC3BnB,OAAO,CAAC8C,6BAAR,CAAsCD,CAAtC;EACH;;EACD,OAAO3B,GAAP;AACH"},"metadata":{},"sourceType":"module"}