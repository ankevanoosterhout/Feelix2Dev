{"ast":null,"code":"/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { customGrad } from '../../gradients';\nimport { FusedConv2D } from '../../kernel_names';\nimport { makeTypesMatch } from '../../tensor_util';\nimport { convertToTensor } from '../../tensor_util_env';\nimport * as util from '../../util';\nimport { add } from '../add';\nimport * as broadcast_util from '../broadcast_util';\nimport { conv2d as unfusedConv2d } from '../conv2d';\nimport { conv2DBackpropFilter } from '../conv2d_backprop_filter';\nimport { conv2DBackpropInput } from '../conv2d_backprop_input';\nimport * as conv_util from '../conv_util';\nimport { applyActivation, getFusedBiasGradient, getFusedDyActivation, shouldFuse } from '../fused_util';\nimport { op } from '../operation';\nimport { reshape } from '../reshape';\n/**\n * Computes a 2D convolution over the input x, optionally fused with adding a\n * bias and applying an activation.\n *\n * ```js\n * const inputDepth = 2;\n * const inShape = [2, 2, 2, inputDepth];\n * const outputDepth = 2;\n * const fSize = 1;\n * const pad = 0;\n * const strides = 1;\n *\n * const x = tf.tensor4d( [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n * 16], inShape);\n * const w = tf.tensor4d([-1, 1, -2, 0.5], [fSize, fSize, inputDepth,\n * outputDepth]);\n *\n * tf.fused.conv2d({ x, filter: w, strides, pad, dataFormat: 'NHWC',\n * dilations: [1, 1], bias: tf.scalar(5), activation: 'relu' }).print();\n * ```\n *\n * @param obj An object with the following properties:\n * @param x The input tensor, of rank 4 or rank 3, of shape\n *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is\n * assumed.\n * @param filter The filter, rank 4, of shape\n *     `[filterHeight, filterWidth, inDepth, outDepth]`.\n * @param strides The strides of the convolution: `[strideHeight,\n * strideWidth]`.\n * @param pad The type of padding algorithm.\n *   - `same` and stride 1: output will be of same size as input,\n *       regardless of filter size.\n *   - `valid` output will be smaller than input if filter is larger\n *       than 1x1.\n *   - For more info, see this guide:\n *     [https://www.tensorflow.org/api_docs/python/tf/nn/convolution](\n *          https://www.tensorflow.org/api_docs/python/tf/nn/convolution)\n * @param dataFormat An optional string from: \"NHWC\", \"NCHW\". Defaults to\n *     \"NHWC\". Specify the data format of the input and output data. With the\n *     default format \"NHWC\", the data is stored in the order of: [batch,\n *     height, width, channels]. Only \"NHWC\" is currently supported.\n * @param dilations The dilation rates: `[dilationHeight, dilationWidth]`\n *     in which we sample input values across the height and width dimensions\n *     in atrous convolution. Defaults to `[1, 1]`. If `dilations` is a single\n *     number, then `dilationHeight == dilationWidth`. If it is greater than\n *     1, then all values of `strides` must be 1.\n * @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is\n *     provided, it will default to truncate.\n * @param bias Tensor to be added to the result.\n * @param activation Name of activation kernel (defaults to `linear`) to be\n *     applied\n *      after biasAdd.\n * @param preluActivationWeights Tensor of prelu weights to be applied as part\n *     of a `prelu` activation, typically the same shape as `x`.\n * @param leakyreluAlpha Optional. Alpha to be applied as part of a `leakyrelu`\n *     activation.\n */\n\nfunction fusedConv2d_({\n  x,\n  filter,\n  strides,\n  pad,\n  dataFormat = 'NHWC',\n  dilations = [1, 1],\n  dimRoundingMode,\n  bias,\n  activation = 'linear',\n  preluActivationWeights,\n  leakyreluAlpha\n}) {\n  activation = activation || 'linear';\n\n  if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {\n    // TODO: Transpose bias and preluActivationWeights properly for NCHW\n    // format before computation.\n    util.assert(dataFormat === 'NHWC', () => `Error in fused conv2d: got dataFormat of ${dataFormat} but ` + `only NHWC is currently supported for the case of gradient depth ` + `is 0 and the activation is not linear.`);\n    let result = unfusedConv2d(x, filter, strides, pad, dataFormat, dilations, dimRoundingMode);\n\n    if (bias != null) {\n      result = add(result, bias);\n    }\n\n    return applyActivation(result, activation, preluActivationWeights, leakyreluAlpha);\n  }\n\n  const $x = convertToTensor(x, 'x', 'conv2d', 'float32');\n  const $filter = convertToTensor(filter, 'filter', 'conv2d', 'float32');\n  let x4D = $x;\n  let reshapedTo4D = false;\n\n  if ($x.rank === 3) {\n    reshapedTo4D = true;\n    x4D = reshape($x, [1, $x.shape[0], $x.shape[1], $x.shape[2]]);\n  }\n\n  util.assert(x4D.rank === 4, () => `Error in fused conv2d: input must be rank 4, but got rank ` + `${x4D.rank}.`);\n  util.assert($filter.rank === 4, () => `Error in fused conv2d: filter must be rank 4, but got rank ` + `${$filter.rank}.`);\n  conv_util.checkPadOnDimRoundingMode('fused conv2d', pad, dimRoundingMode);\n  const inputChannels = dataFormat === 'NHWC' ? x4D.shape[3] : x4D.shape[1];\n  util.assert($filter.shape[2] === inputChannels, () => `Error in conv2d: depth of input (${inputChannels}) must match ` + `input depth for filter ${$filter.shape[2]}.`);\n  util.assert(conv_util.eitherStridesOrDilationsAreOne(strides, dilations), () => 'Error in conv2D: Either strides or dilations must be 1. ' + `Got strides ${strides} and dilations '${dilations}'`);\n  const convInfo = conv_util.computeConv2DInfo(x4D.shape, $filter.shape, strides, dilations, pad, dimRoundingMode);\n  let $bias;\n\n  if (bias != null) {\n    $bias = convertToTensor(bias, 'bias', 'fused conv2d');\n    [$bias] = makeTypesMatch($bias, $x); // According to TensorFlow, the bias is supposed be a 1-D tensor or a\n    // scalar.\n    //\n    // 3-D or 4-D bias is not disabled for NHWC format, because they are\n    // currently being used in some cases. For examplem in our code base,\n    // https://github.com/tensorflow/tfjs/blob/b53bd47e880367ae57493f0ea628abaf08db2d5d/tfjs-core/src/ops/fused/fused_conv2d_test.ts#L1972.\n\n    if (dataFormat === 'NHWC') {\n      broadcast_util.assertAndGetBroadcastShape(convInfo.outShape, $bias.shape);\n    } else {\n      util.assert($bias.shape.length <= 1, () => `Error in fused conv2d: only supports scalar or 1-D Tensor ` + `bias for NCHW format but got the bias of ` + `rank-${$bias.shape.length}.`);\n      util.assert($bias.shape.length === 0 || $bias.shape[0] === convInfo.outChannels || $bias.shape[0] === 1, () => `Error in fused conv2d: bias shape (${$bias.shape}) is not ` + `compatible with the number of output channels ` + `(${convInfo.outChannels})`);\n    }\n  }\n\n  let $preluActivationWeights;\n\n  if (preluActivationWeights != null) {\n    // PReLU's activation weights could be a scalar, a 1-D tensor or a 3-D\n    // tensor.\n    const alphaShape = preluActivationWeights.shape;\n    util.assert(alphaShape.length <= 1 || alphaShape.length === 3, () => `Error in fused conv2d: only supports scalar, 1-D Tensor or ` + `3-D Tensor PReLU activation weights but got a tensor of ` + `rank-${alphaShape.length}.`);\n\n    if (alphaShape.length === 1) {\n      // Whether the data format is NCHW or NHWC, the 1-D PReLU activation\n      // weights tensor should be aligned with the output channels of conv2d\n      // result.\n      util.assert(alphaShape[0] === 1 || alphaShape[0] === convInfo.outChannels, () => `Error in fused conv2d: PReLU activation weights ` + `(${alphaShape}) is not compatible with the number of output ` + `channels (${convInfo.outChannels}).`);\n    } else if (alphaShape.length === 3) {\n      // Whether the data format is NCHW or NHWC, the PReLU activation weights\n      // tensor should has the compatible shape with the result of conv2d.\n      try {\n        broadcast_util.assertAndGetBroadcastShape(alphaShape, convInfo.outShape);\n      } catch (e) {\n        const errMsg = `Error in fused conv2d: PReLU activation weights (${alphaShape}) ` + `is not compatible with the output shape of the conv2d ` + `(${convInfo.outShape}).`;\n        throw Error(errMsg);\n      }\n    }\n\n    $preluActivationWeights = convertToTensor(preluActivationWeights, 'prelu weights', 'fused conv2d');\n  }\n\n  const grad = (dy, saved) => {\n    util.assert(dataFormat === 'NHWC', () => `Error in gradient of fused conv2D: got dataFormat of ${dataFormat} but only NHWC is currently supported.`);\n    const [$filter, x4D, y, $bias] = saved;\n    const dyActivation = getFusedDyActivation(dy, y, activation);\n    util.assert(conv_util.tupleValuesAreOne(dilations), () => 'Error in gradient of fused conv2D: ' + `dilation rates greater than 1 ` + `are not yet supported in gradients. Got dilations '${dilations}'`);\n    const xDer = conv2DBackpropInput(x4D.shape, dyActivation, $filter, strides, pad);\n    const filterDer = conv2DBackpropFilter(x4D, dyActivation, $filter.shape, strides, pad);\n    const der = [xDer, filterDer];\n\n    if ($bias != null) {\n      const biasDer = getFusedBiasGradient($bias, dyActivation);\n      der.push(biasDer);\n    }\n\n    return der;\n  };\n\n  const inputs = {\n    x: x4D,\n    filter: $filter,\n    bias: $bias,\n    preluActivationWeights: $preluActivationWeights\n  };\n  const attrs = {\n    strides,\n    pad,\n    dataFormat,\n    dilations,\n    dimRoundingMode,\n    activation,\n    leakyreluAlpha\n  }; // Depending on the the params passed in we will have different number of\n  // inputs and thus a a different number of elements in the gradient.\n\n  if (bias == null) {\n    const customOp = customGrad((x4D, filter, save) => {\n      let res = // tslint:disable-next-line: no-unnecessary-type-assertion\n      ENGINE.runKernel(FusedConv2D, inputs, attrs);\n      save([filter, x4D, res]);\n\n      if (reshapedTo4D) {\n        // tslint:disable-next-line: no-unnecessary-type-assertion\n        res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n      }\n\n      return {\n        value: res,\n        gradFunc: grad\n      };\n    });\n    return customOp(x4D, $filter);\n  } else {\n    const customOpWithBias = customGrad((x4D, filter, bias, save) => {\n      let res = ENGINE.runKernel(FusedConv2D, inputs, attrs);\n      save([filter, x4D, res, bias]);\n\n      if (reshapedTo4D) {\n        // tslint:disable-next-line: no-unnecessary-type-assertion\n        res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n      }\n\n      return {\n        value: res,\n        gradFunc: grad\n      };\n    });\n    return customOpWithBias(x4D, $filter, $bias);\n  }\n}\n\nexport const conv2d = op({\n  fusedConv2d_\n});","map":{"version":3,"names":["ENGINE","customGrad","FusedConv2D","makeTypesMatch","convertToTensor","util","add","broadcast_util","conv2d","unfusedConv2d","conv2DBackpropFilter","conv2DBackpropInput","conv_util","applyActivation","getFusedBiasGradient","getFusedDyActivation","shouldFuse","op","reshape","fusedConv2d_","x","filter","strides","pad","dataFormat","dilations","dimRoundingMode","bias","activation","preluActivationWeights","leakyreluAlpha","state","gradientDepth","assert","result","$x","$filter","x4D","reshapedTo4D","rank","shape","checkPadOnDimRoundingMode","inputChannels","eitherStridesOrDilationsAreOne","convInfo","computeConv2DInfo","$bias","assertAndGetBroadcastShape","outShape","length","outChannels","$preluActivationWeights","alphaShape","e","errMsg","Error","grad","dy","saved","y","dyActivation","tupleValuesAreOne","xDer","filterDer","der","biasDer","push","inputs","attrs","customOp","save","res","runKernel","value","gradFunc","customOpWithBias"],"sources":["C:/Users/Anke/Documents/Feelix documents/Feelix2.0-dev/Feelix v2/node_modules/@tensorflow/tfjs-core/dist/ops/fused/conv2d.js"],"sourcesContent":["/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { customGrad } from '../../gradients';\nimport { FusedConv2D } from '../../kernel_names';\nimport { makeTypesMatch } from '../../tensor_util';\nimport { convertToTensor } from '../../tensor_util_env';\nimport * as util from '../../util';\nimport { add } from '../add';\nimport * as broadcast_util from '../broadcast_util';\nimport { conv2d as unfusedConv2d } from '../conv2d';\nimport { conv2DBackpropFilter } from '../conv2d_backprop_filter';\nimport { conv2DBackpropInput } from '../conv2d_backprop_input';\nimport * as conv_util from '../conv_util';\nimport { applyActivation, getFusedBiasGradient, getFusedDyActivation, shouldFuse } from '../fused_util';\nimport { op } from '../operation';\nimport { reshape } from '../reshape';\n/**\n * Computes a 2D convolution over the input x, optionally fused with adding a\n * bias and applying an activation.\n *\n * ```js\n * const inputDepth = 2;\n * const inShape = [2, 2, 2, inputDepth];\n * const outputDepth = 2;\n * const fSize = 1;\n * const pad = 0;\n * const strides = 1;\n *\n * const x = tf.tensor4d( [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n * 16], inShape);\n * const w = tf.tensor4d([-1, 1, -2, 0.5], [fSize, fSize, inputDepth,\n * outputDepth]);\n *\n * tf.fused.conv2d({ x, filter: w, strides, pad, dataFormat: 'NHWC',\n * dilations: [1, 1], bias: tf.scalar(5), activation: 'relu' }).print();\n * ```\n *\n * @param obj An object with the following properties:\n * @param x The input tensor, of rank 4 or rank 3, of shape\n *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is\n * assumed.\n * @param filter The filter, rank 4, of shape\n *     `[filterHeight, filterWidth, inDepth, outDepth]`.\n * @param strides The strides of the convolution: `[strideHeight,\n * strideWidth]`.\n * @param pad The type of padding algorithm.\n *   - `same` and stride 1: output will be of same size as input,\n *       regardless of filter size.\n *   - `valid` output will be smaller than input if filter is larger\n *       than 1x1.\n *   - For more info, see this guide:\n *     [https://www.tensorflow.org/api_docs/python/tf/nn/convolution](\n *          https://www.tensorflow.org/api_docs/python/tf/nn/convolution)\n * @param dataFormat An optional string from: \"NHWC\", \"NCHW\". Defaults to\n *     \"NHWC\". Specify the data format of the input and output data. With the\n *     default format \"NHWC\", the data is stored in the order of: [batch,\n *     height, width, channels]. Only \"NHWC\" is currently supported.\n * @param dilations The dilation rates: `[dilationHeight, dilationWidth]`\n *     in which we sample input values across the height and width dimensions\n *     in atrous convolution. Defaults to `[1, 1]`. If `dilations` is a single\n *     number, then `dilationHeight == dilationWidth`. If it is greater than\n *     1, then all values of `strides` must be 1.\n * @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is\n *     provided, it will default to truncate.\n * @param bias Tensor to be added to the result.\n * @param activation Name of activation kernel (defaults to `linear`) to be\n *     applied\n *      after biasAdd.\n * @param preluActivationWeights Tensor of prelu weights to be applied as part\n *     of a `prelu` activation, typically the same shape as `x`.\n * @param leakyreluAlpha Optional. Alpha to be applied as part of a `leakyrelu`\n *     activation.\n */\nfunction fusedConv2d_({ x, filter, strides, pad, dataFormat = 'NHWC', dilations = [1, 1], dimRoundingMode, bias, activation = 'linear', preluActivationWeights, leakyreluAlpha }) {\n    activation = activation || 'linear';\n    if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {\n        // TODO: Transpose bias and preluActivationWeights properly for NCHW\n        // format before computation.\n        util.assert(dataFormat === 'NHWC', () => `Error in fused conv2d: got dataFormat of ${dataFormat} but ` +\n            `only NHWC is currently supported for the case of gradient depth ` +\n            `is 0 and the activation is not linear.`);\n        let result = unfusedConv2d(x, filter, strides, pad, dataFormat, dilations, dimRoundingMode);\n        if (bias != null) {\n            result = add(result, bias);\n        }\n        return applyActivation(result, activation, preluActivationWeights, leakyreluAlpha);\n    }\n    const $x = convertToTensor(x, 'x', 'conv2d', 'float32');\n    const $filter = convertToTensor(filter, 'filter', 'conv2d', 'float32');\n    let x4D = $x;\n    let reshapedTo4D = false;\n    if ($x.rank === 3) {\n        reshapedTo4D = true;\n        x4D = reshape($x, [1, $x.shape[0], $x.shape[1], $x.shape[2]]);\n    }\n    util.assert(x4D.rank === 4, () => `Error in fused conv2d: input must be rank 4, but got rank ` +\n        `${x4D.rank}.`);\n    util.assert($filter.rank === 4, () => `Error in fused conv2d: filter must be rank 4, but got rank ` +\n        `${$filter.rank}.`);\n    conv_util.checkPadOnDimRoundingMode('fused conv2d', pad, dimRoundingMode);\n    const inputChannels = dataFormat === 'NHWC' ? x4D.shape[3] : x4D.shape[1];\n    util.assert($filter.shape[2] === inputChannels, () => `Error in conv2d: depth of input (${inputChannels}) must match ` +\n        `input depth for filter ${$filter.shape[2]}.`);\n    util.assert(conv_util.eitherStridesOrDilationsAreOne(strides, dilations), () => 'Error in conv2D: Either strides or dilations must be 1. ' +\n        `Got strides ${strides} and dilations '${dilations}'`);\n    const convInfo = conv_util.computeConv2DInfo(x4D.shape, $filter.shape, strides, dilations, pad, dimRoundingMode);\n    let $bias;\n    if (bias != null) {\n        $bias = convertToTensor(bias, 'bias', 'fused conv2d');\n        [$bias] = makeTypesMatch($bias, $x);\n        // According to TensorFlow, the bias is supposed be a 1-D tensor or a\n        // scalar.\n        //\n        // 3-D or 4-D bias is not disabled for NHWC format, because they are\n        // currently being used in some cases. For examplem in our code base,\n        // https://github.com/tensorflow/tfjs/blob/b53bd47e880367ae57493f0ea628abaf08db2d5d/tfjs-core/src/ops/fused/fused_conv2d_test.ts#L1972.\n        if (dataFormat === 'NHWC') {\n            broadcast_util.assertAndGetBroadcastShape(convInfo.outShape, $bias.shape);\n        }\n        else {\n            util.assert($bias.shape.length <= 1, () => `Error in fused conv2d: only supports scalar or 1-D Tensor ` +\n                `bias for NCHW format but got the bias of ` +\n                `rank-${$bias.shape.length}.`);\n            util.assert($bias.shape.length === 0 || $bias.shape[0] === convInfo.outChannels ||\n                $bias.shape[0] === 1, () => `Error in fused conv2d: bias shape (${$bias.shape}) is not ` +\n                `compatible with the number of output channels ` +\n                `(${convInfo.outChannels})`);\n        }\n    }\n    let $preluActivationWeights;\n    if (preluActivationWeights != null) {\n        // PReLU's activation weights could be a scalar, a 1-D tensor or a 3-D\n        // tensor.\n        const alphaShape = preluActivationWeights.shape;\n        util.assert(alphaShape.length <= 1 || alphaShape.length === 3, () => `Error in fused conv2d: only supports scalar, 1-D Tensor or ` +\n            `3-D Tensor PReLU activation weights but got a tensor of ` +\n            `rank-${alphaShape.length}.`);\n        if (alphaShape.length === 1) {\n            // Whether the data format is NCHW or NHWC, the 1-D PReLU activation\n            // weights tensor should be aligned with the output channels of conv2d\n            // result.\n            util.assert(alphaShape[0] === 1 || alphaShape[0] === convInfo.outChannels, () => `Error in fused conv2d: PReLU activation weights ` +\n                `(${alphaShape}) is not compatible with the number of output ` +\n                `channels (${convInfo.outChannels}).`);\n        }\n        else if (alphaShape.length === 3) {\n            // Whether the data format is NCHW or NHWC, the PReLU activation weights\n            // tensor should has the compatible shape with the result of conv2d.\n            try {\n                broadcast_util.assertAndGetBroadcastShape(alphaShape, convInfo.outShape);\n            }\n            catch (e) {\n                const errMsg = `Error in fused conv2d: PReLU activation weights (${alphaShape}) ` +\n                    `is not compatible with the output shape of the conv2d ` +\n                    `(${convInfo.outShape}).`;\n                throw Error(errMsg);\n            }\n        }\n        $preluActivationWeights = convertToTensor(preluActivationWeights, 'prelu weights', 'fused conv2d');\n    }\n    const grad = (dy, saved) => {\n        util.assert(dataFormat === 'NHWC', () => `Error in gradient of fused conv2D: got dataFormat of ${dataFormat} but only NHWC is currently supported.`);\n        const [$filter, x4D, y, $bias] = saved;\n        const dyActivation = getFusedDyActivation(dy, y, activation);\n        util.assert(conv_util.tupleValuesAreOne(dilations), () => 'Error in gradient of fused conv2D: ' +\n            `dilation rates greater than 1 ` +\n            `are not yet supported in gradients. Got dilations '${dilations}'`);\n        const xDer = conv2DBackpropInput(x4D.shape, dyActivation, $filter, strides, pad);\n        const filterDer = conv2DBackpropFilter(x4D, dyActivation, $filter.shape, strides, pad);\n        const der = [xDer, filterDer];\n        if ($bias != null) {\n            const biasDer = getFusedBiasGradient($bias, dyActivation);\n            der.push(biasDer);\n        }\n        return der;\n    };\n    const inputs = {\n        x: x4D,\n        filter: $filter,\n        bias: $bias,\n        preluActivationWeights: $preluActivationWeights\n    };\n    const attrs = {\n        strides,\n        pad,\n        dataFormat,\n        dilations,\n        dimRoundingMode,\n        activation,\n        leakyreluAlpha\n    };\n    // Depending on the the params passed in we will have different number of\n    // inputs and thus a a different number of elements in the gradient.\n    if (bias == null) {\n        const customOp = customGrad((x4D, filter, save) => {\n            let res = \n            // tslint:disable-next-line: no-unnecessary-type-assertion\n            ENGINE.runKernel(FusedConv2D, inputs, attrs);\n            save([filter, x4D, res]);\n            if (reshapedTo4D) {\n                // tslint:disable-next-line: no-unnecessary-type-assertion\n                res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n            }\n            return { value: res, gradFunc: grad };\n        });\n        return customOp(x4D, $filter);\n    }\n    else {\n        const customOpWithBias = customGrad((x4D, filter, bias, save) => {\n            let res = ENGINE.runKernel(FusedConv2D, inputs, attrs);\n            save([filter, x4D, res, bias]);\n            if (reshapedTo4D) {\n                // tslint:disable-next-line: no-unnecessary-type-assertion\n                res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n            }\n            return { value: res, gradFunc: grad };\n        });\n        return customOpWithBias(x4D, $filter, $bias);\n    }\n}\nexport const conv2d = op({ fusedConv2d_ });\n"],"mappings":"AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAASA,MAAT,QAAuB,cAAvB;AACA,SAASC,UAAT,QAA2B,iBAA3B;AACA,SAASC,WAAT,QAA4B,oBAA5B;AACA,SAASC,cAAT,QAA+B,mBAA/B;AACA,SAASC,eAAT,QAAgC,uBAAhC;AACA,OAAO,KAAKC,IAAZ,MAAsB,YAAtB;AACA,SAASC,GAAT,QAAoB,QAApB;AACA,OAAO,KAAKC,cAAZ,MAAgC,mBAAhC;AACA,SAASC,MAAM,IAAIC,aAAnB,QAAwC,WAAxC;AACA,SAASC,oBAAT,QAAqC,2BAArC;AACA,SAASC,mBAAT,QAAoC,0BAApC;AACA,OAAO,KAAKC,SAAZ,MAA2B,cAA3B;AACA,SAASC,eAAT,EAA0BC,oBAA1B,EAAgDC,oBAAhD,EAAsEC,UAAtE,QAAwF,eAAxF;AACA,SAASC,EAAT,QAAmB,cAAnB;AACA,SAASC,OAAT,QAAwB,YAAxB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AACA,SAASC,YAAT,CAAsB;EAAEC,CAAF;EAAKC,MAAL;EAAaC,OAAb;EAAsBC,GAAtB;EAA2BC,UAAU,GAAG,MAAxC;EAAgDC,SAAS,GAAG,CAAC,CAAD,EAAI,CAAJ,CAA5D;EAAoEC,eAApE;EAAqFC,IAArF;EAA2FC,UAAU,GAAG,QAAxG;EAAkHC,sBAAlH;EAA0IC;AAA1I,CAAtB,EAAkL;EAC9KF,UAAU,GAAGA,UAAU,IAAI,QAA3B;;EACA,IAAIZ,UAAU,CAAChB,MAAM,CAAC+B,KAAP,CAAaC,aAAd,EAA6BJ,UAA7B,CAAV,KAAuD,KAA3D,EAAkE;IAC9D;IACA;IACAvB,IAAI,CAAC4B,MAAL,CAAYT,UAAU,KAAK,MAA3B,EAAmC,MAAO,4CAA2CA,UAAW,OAAvD,GACpC,kEADoC,GAEpC,wCAFL;IAGA,IAAIU,MAAM,GAAGzB,aAAa,CAACW,CAAD,EAAIC,MAAJ,EAAYC,OAAZ,EAAqBC,GAArB,EAA0BC,UAA1B,EAAsCC,SAAtC,EAAiDC,eAAjD,CAA1B;;IACA,IAAIC,IAAI,IAAI,IAAZ,EAAkB;MACdO,MAAM,GAAG5B,GAAG,CAAC4B,MAAD,EAASP,IAAT,CAAZ;IACH;;IACD,OAAOd,eAAe,CAACqB,MAAD,EAASN,UAAT,EAAqBC,sBAArB,EAA6CC,cAA7C,CAAtB;EACH;;EACD,MAAMK,EAAE,GAAG/B,eAAe,CAACgB,CAAD,EAAI,GAAJ,EAAS,QAAT,EAAmB,SAAnB,CAA1B;EACA,MAAMgB,OAAO,GAAGhC,eAAe,CAACiB,MAAD,EAAS,QAAT,EAAmB,QAAnB,EAA6B,SAA7B,CAA/B;EACA,IAAIgB,GAAG,GAAGF,EAAV;EACA,IAAIG,YAAY,GAAG,KAAnB;;EACA,IAAIH,EAAE,CAACI,IAAH,KAAY,CAAhB,EAAmB;IACfD,YAAY,GAAG,IAAf;IACAD,GAAG,GAAGnB,OAAO,CAACiB,EAAD,EAAK,CAAC,CAAD,EAAIA,EAAE,CAACK,KAAH,CAAS,CAAT,CAAJ,EAAiBL,EAAE,CAACK,KAAH,CAAS,CAAT,CAAjB,EAA8BL,EAAE,CAACK,KAAH,CAAS,CAAT,CAA9B,CAAL,CAAb;EACH;;EACDnC,IAAI,CAAC4B,MAAL,CAAYI,GAAG,CAACE,IAAJ,KAAa,CAAzB,EAA4B,MAAO,4DAAD,GAC7B,GAAEF,GAAG,CAACE,IAAK,GADhB;EAEAlC,IAAI,CAAC4B,MAAL,CAAYG,OAAO,CAACG,IAAR,KAAiB,CAA7B,EAAgC,MAAO,6DAAD,GACjC,GAAEH,OAAO,CAACG,IAAK,GADpB;EAEA3B,SAAS,CAAC6B,yBAAV,CAAoC,cAApC,EAAoDlB,GAApD,EAAyDG,eAAzD;EACA,MAAMgB,aAAa,GAAGlB,UAAU,KAAK,MAAf,GAAwBa,GAAG,CAACG,KAAJ,CAAU,CAAV,CAAxB,GAAuCH,GAAG,CAACG,KAAJ,CAAU,CAAV,CAA7D;EACAnC,IAAI,CAAC4B,MAAL,CAAYG,OAAO,CAACI,KAAR,CAAc,CAAd,MAAqBE,aAAjC,EAAgD,MAAO,oCAAmCA,aAAc,eAAlD,GACjD,0BAAyBN,OAAO,CAACI,KAAR,CAAc,CAAd,CAAiB,GAD/C;EAEAnC,IAAI,CAAC4B,MAAL,CAAYrB,SAAS,CAAC+B,8BAAV,CAAyCrB,OAAzC,EAAkDG,SAAlD,CAAZ,EAA0E,MAAM,6DAC3E,eAAcH,OAAQ,mBAAkBG,SAAU,GADvD;EAEA,MAAMmB,QAAQ,GAAGhC,SAAS,CAACiC,iBAAV,CAA4BR,GAAG,CAACG,KAAhC,EAAuCJ,OAAO,CAACI,KAA/C,EAAsDlB,OAAtD,EAA+DG,SAA/D,EAA0EF,GAA1E,EAA+EG,eAA/E,CAAjB;EACA,IAAIoB,KAAJ;;EACA,IAAInB,IAAI,IAAI,IAAZ,EAAkB;IACdmB,KAAK,GAAG1C,eAAe,CAACuB,IAAD,EAAO,MAAP,EAAe,cAAf,CAAvB;IACA,CAACmB,KAAD,IAAU3C,cAAc,CAAC2C,KAAD,EAAQX,EAAR,CAAxB,CAFc,CAGd;IACA;IACA;IACA;IACA;IACA;;IACA,IAAIX,UAAU,KAAK,MAAnB,EAA2B;MACvBjB,cAAc,CAACwC,0BAAf,CAA0CH,QAAQ,CAACI,QAAnD,EAA6DF,KAAK,CAACN,KAAnE;IACH,CAFD,MAGK;MACDnC,IAAI,CAAC4B,MAAL,CAAYa,KAAK,CAACN,KAAN,CAAYS,MAAZ,IAAsB,CAAlC,EAAqC,MAAO,4DAAD,GACtC,2CADsC,GAEtC,QAAOH,KAAK,CAACN,KAAN,CAAYS,MAAO,GAF/B;MAGA5C,IAAI,CAAC4B,MAAL,CAAYa,KAAK,CAACN,KAAN,CAAYS,MAAZ,KAAuB,CAAvB,IAA4BH,KAAK,CAACN,KAAN,CAAY,CAAZ,MAAmBI,QAAQ,CAACM,WAAxD,IACRJ,KAAK,CAACN,KAAN,CAAY,CAAZ,MAAmB,CADvB,EAC0B,MAAO,sCAAqCM,KAAK,CAACN,KAAM,WAAlD,GAC3B,gDAD2B,GAE3B,IAAGI,QAAQ,CAACM,WAAY,GAH7B;IAIH;EACJ;;EACD,IAAIC,uBAAJ;;EACA,IAAItB,sBAAsB,IAAI,IAA9B,EAAoC;IAChC;IACA;IACA,MAAMuB,UAAU,GAAGvB,sBAAsB,CAACW,KAA1C;IACAnC,IAAI,CAAC4B,MAAL,CAAYmB,UAAU,CAACH,MAAX,IAAqB,CAArB,IAA0BG,UAAU,CAACH,MAAX,KAAsB,CAA5D,EAA+D,MAAO,6DAAD,GAChE,0DADgE,GAEhE,QAAOG,UAAU,CAACH,MAAO,GAF9B;;IAGA,IAAIG,UAAU,CAACH,MAAX,KAAsB,CAA1B,EAA6B;MACzB;MACA;MACA;MACA5C,IAAI,CAAC4B,MAAL,CAAYmB,UAAU,CAAC,CAAD,CAAV,KAAkB,CAAlB,IAAuBA,UAAU,CAAC,CAAD,CAAV,KAAkBR,QAAQ,CAACM,WAA9D,EAA2E,MAAO,kDAAD,GAC5E,IAAGE,UAAW,gDAD8D,GAE5E,aAAYR,QAAQ,CAACM,WAAY,IAFtC;IAGH,CAPD,MAQK,IAAIE,UAAU,CAACH,MAAX,KAAsB,CAA1B,EAA6B;MAC9B;MACA;MACA,IAAI;QACA1C,cAAc,CAACwC,0BAAf,CAA0CK,UAA1C,EAAsDR,QAAQ,CAACI,QAA/D;MACH,CAFD,CAGA,OAAOK,CAAP,EAAU;QACN,MAAMC,MAAM,GAAI,oDAAmDF,UAAW,IAA/D,GACV,wDADU,GAEV,IAAGR,QAAQ,CAACI,QAAS,IAF1B;QAGA,MAAMO,KAAK,CAACD,MAAD,CAAX;MACH;IACJ;;IACDH,uBAAuB,GAAG/C,eAAe,CAACyB,sBAAD,EAAyB,eAAzB,EAA0C,cAA1C,CAAzC;EACH;;EACD,MAAM2B,IAAI,GAAG,CAACC,EAAD,EAAKC,KAAL,KAAe;IACxBrD,IAAI,CAAC4B,MAAL,CAAYT,UAAU,KAAK,MAA3B,EAAmC,MAAO,wDAAuDA,UAAW,wCAA5G;IACA,MAAM,CAACY,OAAD,EAAUC,GAAV,EAAesB,CAAf,EAAkBb,KAAlB,IAA2BY,KAAjC;IACA,MAAME,YAAY,GAAG7C,oBAAoB,CAAC0C,EAAD,EAAKE,CAAL,EAAQ/B,UAAR,CAAzC;IACAvB,IAAI,CAAC4B,MAAL,CAAYrB,SAAS,CAACiD,iBAAV,CAA4BpC,SAA5B,CAAZ,EAAoD,MAAM,wCACrD,gCADqD,GAErD,sDAAqDA,SAAU,GAFpE;IAGA,MAAMqC,IAAI,GAAGnD,mBAAmB,CAAC0B,GAAG,CAACG,KAAL,EAAYoB,YAAZ,EAA0BxB,OAA1B,EAAmCd,OAAnC,EAA4CC,GAA5C,CAAhC;IACA,MAAMwC,SAAS,GAAGrD,oBAAoB,CAAC2B,GAAD,EAAMuB,YAAN,EAAoBxB,OAAO,CAACI,KAA5B,EAAmClB,OAAnC,EAA4CC,GAA5C,CAAtC;IACA,MAAMyC,GAAG,GAAG,CAACF,IAAD,EAAOC,SAAP,CAAZ;;IACA,IAAIjB,KAAK,IAAI,IAAb,EAAmB;MACf,MAAMmB,OAAO,GAAGnD,oBAAoB,CAACgC,KAAD,EAAQc,YAAR,CAApC;MACAI,GAAG,CAACE,IAAJ,CAASD,OAAT;IACH;;IACD,OAAOD,GAAP;EACH,CAfD;;EAgBA,MAAMG,MAAM,GAAG;IACX/C,CAAC,EAAEiB,GADQ;IAEXhB,MAAM,EAAEe,OAFG;IAGXT,IAAI,EAAEmB,KAHK;IAIXjB,sBAAsB,EAAEsB;EAJb,CAAf;EAMA,MAAMiB,KAAK,GAAG;IACV9C,OADU;IAEVC,GAFU;IAGVC,UAHU;IAIVC,SAJU;IAKVC,eALU;IAMVE,UANU;IAOVE;EAPU,CAAd,CA7G8K,CAsH9K;EACA;;EACA,IAAIH,IAAI,IAAI,IAAZ,EAAkB;IACd,MAAM0C,QAAQ,GAAGpE,UAAU,CAAC,CAACoC,GAAD,EAAMhB,MAAN,EAAciD,IAAd,KAAuB;MAC/C,IAAIC,GAAG,GACP;MACAvE,MAAM,CAACwE,SAAP,CAAiBtE,WAAjB,EAA8BiE,MAA9B,EAAsCC,KAAtC,CAFA;MAGAE,IAAI,CAAC,CAACjD,MAAD,EAASgB,GAAT,EAAckC,GAAd,CAAD,CAAJ;;MACA,IAAIjC,YAAJ,EAAkB;QACd;QACAiC,GAAG,GAAGrD,OAAO,CAACqD,GAAD,EAAM,CAACA,GAAG,CAAC/B,KAAJ,CAAU,CAAV,CAAD,EAAe+B,GAAG,CAAC/B,KAAJ,CAAU,CAAV,CAAf,EAA6B+B,GAAG,CAAC/B,KAAJ,CAAU,CAAV,CAA7B,CAAN,CAAb;MACH;;MACD,OAAO;QAAEiC,KAAK,EAAEF,GAAT;QAAcG,QAAQ,EAAElB;MAAxB,CAAP;IACH,CAV0B,CAA3B;IAWA,OAAOa,QAAQ,CAAChC,GAAD,EAAMD,OAAN,CAAf;EACH,CAbD,MAcK;IACD,MAAMuC,gBAAgB,GAAG1E,UAAU,CAAC,CAACoC,GAAD,EAAMhB,MAAN,EAAcM,IAAd,EAAoB2C,IAApB,KAA6B;MAC7D,IAAIC,GAAG,GAAGvE,MAAM,CAACwE,SAAP,CAAiBtE,WAAjB,EAA8BiE,MAA9B,EAAsCC,KAAtC,CAAV;MACAE,IAAI,CAAC,CAACjD,MAAD,EAASgB,GAAT,EAAckC,GAAd,EAAmB5C,IAAnB,CAAD,CAAJ;;MACA,IAAIW,YAAJ,EAAkB;QACd;QACAiC,GAAG,GAAGrD,OAAO,CAACqD,GAAD,EAAM,CAACA,GAAG,CAAC/B,KAAJ,CAAU,CAAV,CAAD,EAAe+B,GAAG,CAAC/B,KAAJ,CAAU,CAAV,CAAf,EAA6B+B,GAAG,CAAC/B,KAAJ,CAAU,CAAV,CAA7B,CAAN,CAAb;MACH;;MACD,OAAO;QAAEiC,KAAK,EAAEF,GAAT;QAAcG,QAAQ,EAAElB;MAAxB,CAAP;IACH,CARkC,CAAnC;IASA,OAAOmB,gBAAgB,CAACtC,GAAD,EAAMD,OAAN,EAAeU,KAAf,CAAvB;EACH;AACJ;;AACD,OAAO,MAAMtC,MAAM,GAAGS,EAAE,CAAC;EAAEE;AAAF,CAAD,CAAjB"},"metadata":{},"sourceType":"module"}