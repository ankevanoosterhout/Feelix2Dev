{"ast":null,"code":"import _asyncToGenerator from \"C:/Users/Anke/Documents/Feelix documents/Feelix2.0-dev/Feelix v2/node_modules/@babel/runtime/helpers/esm/asyncToGenerator.js\";\n\n/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { dispose, tidy } from '../globals';\nimport { add } from '../ops/add';\nimport { div } from '../ops/div';\nimport { mul } from '../ops/mul';\nimport { sqrt } from '../ops/sqrt';\nimport { square } from '../ops/square';\nimport { sub } from '../ops/sub';\nimport { zerosLike } from '../ops/zeros_like';\nimport { registerClass } from '../serialization';\nimport { Optimizer } from './optimizer';\n/** @doclink Optimizer */\n\nexport class RMSPropOptimizer extends Optimizer {\n  constructor(learningRate, decay = 0.9, momentum = 0.0, epsilon = null, centered = false) {\n    super();\n    this.learningRate = learningRate;\n    this.decay = decay;\n    this.momentum = momentum;\n    this.epsilon = epsilon;\n    this.accumulatedMeanSquares = [];\n    this.accumulatedMoments = [];\n    this.accumulatedMeanGrads = [];\n    this.centered = centered;\n\n    if (epsilon == null) {\n      this.epsilon = ENGINE.backend.epsilon();\n    }\n\n    if (learningRate == null) {\n      throw new Error(`learningRate for RMSPropOptimizer must be defined.`);\n    }\n  }\n\n  applyGradients(variableGradients) {\n    const variableNames = Array.isArray(variableGradients) ? variableGradients.map(item => item.name) : Object.keys(variableGradients);\n    variableNames.forEach((name, i) => {\n      const value = ENGINE.registeredVariables[name];\n      const trainable = false;\n\n      if (this.accumulatedMeanSquares[i] == null) {\n        this.accumulatedMeanSquares[i] = {\n          originalName: `${name}/rms`,\n          variable: tidy(() => zerosLike(value).variable(trainable))\n        };\n      }\n\n      if (this.accumulatedMoments[i] == null) {\n        this.accumulatedMoments[i] = {\n          originalName: `${name}/momentum`,\n          variable: tidy(() => zerosLike(value).variable(trainable))\n        };\n      }\n\n      if (this.accumulatedMeanGrads[i] == null && this.centered) {\n        this.accumulatedMeanGrads[i] = {\n          originalName: `${name}/mg`,\n          variable: tidy(() => zerosLike(value).variable(trainable))\n        };\n      }\n\n      const gradient = Array.isArray(variableGradients) ? variableGradients[i].tensor : variableGradients[name];\n\n      if (gradient == null) {\n        return;\n      }\n\n      const accumulatedMeanSquare = this.accumulatedMeanSquares[i].variable;\n      const accumulatedMoments = this.accumulatedMoments[i].variable;\n      tidy(() => {\n        const newAccumulatedMeanSquare = add(mul(accumulatedMeanSquare, this.decay), mul(square(gradient), 1 - this.decay));\n\n        if (this.centered) {\n          const accumulatedMeanGrad = this.accumulatedMeanGrads[i].variable; // Centered gradient\n\n          const newAccumulatedMeanGrad = add(mul(accumulatedMeanGrad, this.decay), mul(gradient, 1 - this.decay));\n          const gradContribution = div(mul(gradient, this.learningRate), sqrt(sub(newAccumulatedMeanSquare, add(square(newAccumulatedMeanGrad), this.epsilon))));\n          const newAccumulatedMoments = add(mul(accumulatedMoments, this.momentum), gradContribution);\n          accumulatedMeanSquare.assign(newAccumulatedMeanSquare);\n          accumulatedMeanGrad.assign(newAccumulatedMeanGrad);\n          accumulatedMoments.assign(newAccumulatedMoments);\n          const newValue = sub(value, newAccumulatedMoments);\n          value.assign(newValue);\n        } else {\n          // Plain gradient\n          const newAccumulatedMeanSquare = add(mul(accumulatedMeanSquare, this.decay), mul(square(gradient), 1 - this.decay));\n          const newAccumulatedMoments = add(mul(accumulatedMoments, this.momentum), div(mul(gradient, this.learningRate), sqrt(add(newAccumulatedMeanSquare, this.epsilon))));\n          accumulatedMeanSquare.assign(newAccumulatedMeanSquare);\n          accumulatedMoments.assign(newAccumulatedMoments);\n          const newValue = sub(value, newAccumulatedMoments);\n          value.assign(newValue);\n        }\n      });\n    });\n    this.incrementIterations();\n  }\n\n  dispose() {\n    if (this.accumulatedMeanSquares != null) {\n      dispose(this.accumulatedMeanSquares.map(v => v.variable));\n    }\n\n    if (this.accumulatedMeanGrads != null && this.centered) {\n      dispose(this.accumulatedMeanGrads.map(v => v.variable));\n    }\n\n    if (this.accumulatedMoments != null) {\n      dispose(this.accumulatedMoments.map(v => v.variable));\n    }\n  }\n\n  getWeights() {\n    var _this = this;\n\n    return _asyncToGenerator(function* () {\n      // Order matters for Python compatibility.\n      const variables = [..._this.accumulatedMeanSquares, ..._this.accumulatedMoments];\n\n      if (_this.centered) {\n        variables.push(..._this.accumulatedMeanGrads);\n      }\n\n      return [yield _this.saveIterations()].concat(variables.map(v => ({\n        name: v.originalName,\n        tensor: v.variable\n      })));\n    })();\n  }\n\n  setWeights(weightValues) {\n    var _this2 = this;\n\n    return _asyncToGenerator(function* () {\n      weightValues = yield _this2.extractIterations(weightValues);\n      const variableCount = _this2.centered ? weightValues.length / 3 : weightValues.length / 2;\n      const trainable = false;\n      _this2.accumulatedMeanSquares = weightValues.slice(0, variableCount).map(v => ({\n        originalName: v.name,\n        variable: v.tensor.variable(trainable)\n      }));\n      _this2.accumulatedMoments = weightValues.slice(variableCount, variableCount * 2).map(v => ({\n        originalName: v.name,\n        variable: v.tensor.variable(trainable)\n      }));\n\n      if (_this2.centered) {\n        _this2.accumulatedMeanGrads = weightValues.slice(variableCount * 2, variableCount * 3).map(v => ({\n          originalName: v.name,\n          variable: v.tensor.variable(trainable)\n        }));\n      }\n    })();\n  }\n\n  getConfig() {\n    return {\n      'learningRate': this.learningRate,\n      'decay': this.decay,\n      'momentum': this.momentum,\n      'epsilon': this.epsilon,\n      'centered': this.centered\n    };\n  }\n  /** @nocollapse */\n\n\n  static fromConfig(cls, config) {\n    return new cls(config['learningRate'], config['decay'], config['momentum'], config['epsilon'], config['centered']);\n  }\n\n}\n/** @nocollapse */\n\nRMSPropOptimizer.className = 'RMSProp'; // Note: Name matters for Python compatibility.\n\nregisterClass(RMSPropOptimizer);","map":{"version":3,"names":["ENGINE","dispose","tidy","add","div","mul","sqrt","square","sub","zerosLike","registerClass","Optimizer","RMSPropOptimizer","constructor","learningRate","decay","momentum","epsilon","centered","accumulatedMeanSquares","accumulatedMoments","accumulatedMeanGrads","backend","Error","applyGradients","variableGradients","variableNames","Array","isArray","map","item","name","Object","keys","forEach","i","value","registeredVariables","trainable","originalName","variable","gradient","tensor","accumulatedMeanSquare","newAccumulatedMeanSquare","accumulatedMeanGrad","newAccumulatedMeanGrad","gradContribution","newAccumulatedMoments","assign","newValue","incrementIterations","v","getWeights","variables","push","saveIterations","concat","setWeights","weightValues","extractIterations","variableCount","length","slice","getConfig","fromConfig","cls","config","className"],"sources":["C:/Users/Anke/Documents/Feelix documents/Feelix2.0-dev/Feelix v2/node_modules/@tensorflow/tfjs-core/dist/optimizers/rmsprop_optimizer.js"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { dispose, tidy } from '../globals';\nimport { add } from '../ops/add';\nimport { div } from '../ops/div';\nimport { mul } from '../ops/mul';\nimport { sqrt } from '../ops/sqrt';\nimport { square } from '../ops/square';\nimport { sub } from '../ops/sub';\nimport { zerosLike } from '../ops/zeros_like';\nimport { registerClass } from '../serialization';\nimport { Optimizer } from './optimizer';\n/** @doclink Optimizer */\nexport class RMSPropOptimizer extends Optimizer {\n    constructor(learningRate, decay = 0.9, momentum = 0.0, epsilon = null, centered = false) {\n        super();\n        this.learningRate = learningRate;\n        this.decay = decay;\n        this.momentum = momentum;\n        this.epsilon = epsilon;\n        this.accumulatedMeanSquares = [];\n        this.accumulatedMoments = [];\n        this.accumulatedMeanGrads = [];\n        this.centered = centered;\n        if (epsilon == null) {\n            this.epsilon = ENGINE.backend.epsilon();\n        }\n        if (learningRate == null) {\n            throw new Error(`learningRate for RMSPropOptimizer must be defined.`);\n        }\n    }\n    applyGradients(variableGradients) {\n        const variableNames = Array.isArray(variableGradients) ?\n            variableGradients.map(item => item.name) :\n            Object.keys(variableGradients);\n        variableNames.forEach((name, i) => {\n            const value = ENGINE.registeredVariables[name];\n            const trainable = false;\n            if (this.accumulatedMeanSquares[i] == null) {\n                this.accumulatedMeanSquares[i] = {\n                    originalName: `${name}/rms`,\n                    variable: tidy(() => zerosLike(value).variable(trainable))\n                };\n            }\n            if (this.accumulatedMoments[i] == null) {\n                this.accumulatedMoments[i] = {\n                    originalName: `${name}/momentum`,\n                    variable: tidy(() => zerosLike(value).variable(trainable))\n                };\n            }\n            if (this.accumulatedMeanGrads[i] == null && this.centered) {\n                this.accumulatedMeanGrads[i] = {\n                    originalName: `${name}/mg`,\n                    variable: tidy(() => zerosLike(value).variable(trainable))\n                };\n            }\n            const gradient = Array.isArray(variableGradients) ?\n                variableGradients[i].tensor :\n                variableGradients[name];\n            if (gradient == null) {\n                return;\n            }\n            const accumulatedMeanSquare = this.accumulatedMeanSquares[i].variable;\n            const accumulatedMoments = this.accumulatedMoments[i].variable;\n            tidy(() => {\n                const newAccumulatedMeanSquare = add(mul(accumulatedMeanSquare, this.decay), mul(square(gradient), 1 - this.decay));\n                if (this.centered) {\n                    const accumulatedMeanGrad = this.accumulatedMeanGrads[i].variable;\n                    // Centered gradient\n                    const newAccumulatedMeanGrad = add(mul(accumulatedMeanGrad, this.decay), mul(gradient, 1 - this.decay));\n                    const gradContribution = div(mul(gradient, this.learningRate), sqrt(sub(newAccumulatedMeanSquare, add(square(newAccumulatedMeanGrad), this.epsilon))));\n                    const newAccumulatedMoments = add(mul(accumulatedMoments, this.momentum), gradContribution);\n                    accumulatedMeanSquare.assign(newAccumulatedMeanSquare);\n                    accumulatedMeanGrad.assign(newAccumulatedMeanGrad);\n                    accumulatedMoments.assign(newAccumulatedMoments);\n                    const newValue = sub(value, newAccumulatedMoments);\n                    value.assign(newValue);\n                }\n                else {\n                    // Plain gradient\n                    const newAccumulatedMeanSquare = add(mul(accumulatedMeanSquare, this.decay), mul(square(gradient), 1 - this.decay));\n                    const newAccumulatedMoments = add(mul(accumulatedMoments, this.momentum), div(mul(gradient, this.learningRate), sqrt(add(newAccumulatedMeanSquare, this.epsilon))));\n                    accumulatedMeanSquare.assign(newAccumulatedMeanSquare);\n                    accumulatedMoments.assign(newAccumulatedMoments);\n                    const newValue = sub(value, newAccumulatedMoments);\n                    value.assign(newValue);\n                }\n            });\n        });\n        this.incrementIterations();\n    }\n    dispose() {\n        if (this.accumulatedMeanSquares != null) {\n            dispose(this.accumulatedMeanSquares.map(v => v.variable));\n        }\n        if (this.accumulatedMeanGrads != null && this.centered) {\n            dispose(this.accumulatedMeanGrads.map(v => v.variable));\n        }\n        if (this.accumulatedMoments != null) {\n            dispose(this.accumulatedMoments.map(v => v.variable));\n        }\n    }\n    async getWeights() {\n        // Order matters for Python compatibility.\n        const variables = [...this.accumulatedMeanSquares, ...this.accumulatedMoments];\n        if (this.centered) {\n            variables.push(...this.accumulatedMeanGrads);\n        }\n        return [await this.saveIterations()].concat(variables.map(v => ({ name: v.originalName, tensor: v.variable })));\n    }\n    async setWeights(weightValues) {\n        weightValues = await this.extractIterations(weightValues);\n        const variableCount = this.centered ? weightValues.length / 3 : weightValues.length / 2;\n        const trainable = false;\n        this.accumulatedMeanSquares =\n            weightValues.slice(0, variableCount).map(v => ({\n                originalName: v.name,\n                variable: v.tensor.variable(trainable)\n            }));\n        this.accumulatedMoments =\n            weightValues.slice(variableCount, variableCount * 2)\n                .map(v => ({\n                originalName: v.name,\n                variable: v.tensor.variable(trainable)\n            }));\n        if (this.centered) {\n            this.accumulatedMeanGrads =\n                weightValues.slice(variableCount * 2, variableCount * 3)\n                    .map(v => ({\n                    originalName: v.name,\n                    variable: v.tensor.variable(trainable)\n                }));\n        }\n    }\n    getConfig() {\n        return {\n            'learningRate': this.learningRate,\n            'decay': this.decay,\n            'momentum': this.momentum,\n            'epsilon': this.epsilon,\n            'centered': this.centered\n        };\n    }\n    /** @nocollapse */\n    static fromConfig(cls, config) {\n        return new cls(config['learningRate'], config['decay'], config['momentum'], config['epsilon'], config['centered']);\n    }\n}\n/** @nocollapse */\nRMSPropOptimizer.className = 'RMSProp'; // Note: Name matters for Python compatibility.\nregisterClass(RMSPropOptimizer);\n"],"mappings":";;AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAASA,MAAT,QAAuB,WAAvB;AACA,SAASC,OAAT,EAAkBC,IAAlB,QAA8B,YAA9B;AACA,SAASC,GAAT,QAAoB,YAApB;AACA,SAASC,GAAT,QAAoB,YAApB;AACA,SAASC,GAAT,QAAoB,YAApB;AACA,SAASC,IAAT,QAAqB,aAArB;AACA,SAASC,MAAT,QAAuB,eAAvB;AACA,SAASC,GAAT,QAAoB,YAApB;AACA,SAASC,SAAT,QAA0B,mBAA1B;AACA,SAASC,aAAT,QAA8B,kBAA9B;AACA,SAASC,SAAT,QAA0B,aAA1B;AACA;;AACA,OAAO,MAAMC,gBAAN,SAA+BD,SAA/B,CAAyC;EAC5CE,WAAW,CAACC,YAAD,EAAeC,KAAK,GAAG,GAAvB,EAA4BC,QAAQ,GAAG,GAAvC,EAA4CC,OAAO,GAAG,IAAtD,EAA4DC,QAAQ,GAAG,KAAvE,EAA8E;IACrF;IACA,KAAKJ,YAAL,GAAoBA,YAApB;IACA,KAAKC,KAAL,GAAaA,KAAb;IACA,KAAKC,QAAL,GAAgBA,QAAhB;IACA,KAAKC,OAAL,GAAeA,OAAf;IACA,KAAKE,sBAAL,GAA8B,EAA9B;IACA,KAAKC,kBAAL,GAA0B,EAA1B;IACA,KAAKC,oBAAL,GAA4B,EAA5B;IACA,KAAKH,QAAL,GAAgBA,QAAhB;;IACA,IAAID,OAAO,IAAI,IAAf,EAAqB;MACjB,KAAKA,OAAL,GAAejB,MAAM,CAACsB,OAAP,CAAeL,OAAf,EAAf;IACH;;IACD,IAAIH,YAAY,IAAI,IAApB,EAA0B;MACtB,MAAM,IAAIS,KAAJ,CAAW,oDAAX,CAAN;IACH;EACJ;;EACDC,cAAc,CAACC,iBAAD,EAAoB;IAC9B,MAAMC,aAAa,GAAGC,KAAK,CAACC,OAAN,CAAcH,iBAAd,IAClBA,iBAAiB,CAACI,GAAlB,CAAsBC,IAAI,IAAIA,IAAI,CAACC,IAAnC,CADkB,GAElBC,MAAM,CAACC,IAAP,CAAYR,iBAAZ,CAFJ;IAGAC,aAAa,CAACQ,OAAd,CAAsB,CAACH,IAAD,EAAOI,CAAP,KAAa;MAC/B,MAAMC,KAAK,GAAGpC,MAAM,CAACqC,mBAAP,CAA2BN,IAA3B,CAAd;MACA,MAAMO,SAAS,GAAG,KAAlB;;MACA,IAAI,KAAKnB,sBAAL,CAA4BgB,CAA5B,KAAkC,IAAtC,EAA4C;QACxC,KAAKhB,sBAAL,CAA4BgB,CAA5B,IAAiC;UAC7BI,YAAY,EAAG,GAAER,IAAK,MADO;UAE7BS,QAAQ,EAAEtC,IAAI,CAAC,MAAMO,SAAS,CAAC2B,KAAD,CAAT,CAAiBI,QAAjB,CAA0BF,SAA1B,CAAP;QAFe,CAAjC;MAIH;;MACD,IAAI,KAAKlB,kBAAL,CAAwBe,CAAxB,KAA8B,IAAlC,EAAwC;QACpC,KAAKf,kBAAL,CAAwBe,CAAxB,IAA6B;UACzBI,YAAY,EAAG,GAAER,IAAK,WADG;UAEzBS,QAAQ,EAAEtC,IAAI,CAAC,MAAMO,SAAS,CAAC2B,KAAD,CAAT,CAAiBI,QAAjB,CAA0BF,SAA1B,CAAP;QAFW,CAA7B;MAIH;;MACD,IAAI,KAAKjB,oBAAL,CAA0Bc,CAA1B,KAAgC,IAAhC,IAAwC,KAAKjB,QAAjD,EAA2D;QACvD,KAAKG,oBAAL,CAA0Bc,CAA1B,IAA+B;UAC3BI,YAAY,EAAG,GAAER,IAAK,KADK;UAE3BS,QAAQ,EAAEtC,IAAI,CAAC,MAAMO,SAAS,CAAC2B,KAAD,CAAT,CAAiBI,QAAjB,CAA0BF,SAA1B,CAAP;QAFa,CAA/B;MAIH;;MACD,MAAMG,QAAQ,GAAGd,KAAK,CAACC,OAAN,CAAcH,iBAAd,IACbA,iBAAiB,CAACU,CAAD,CAAjB,CAAqBO,MADR,GAEbjB,iBAAiB,CAACM,IAAD,CAFrB;;MAGA,IAAIU,QAAQ,IAAI,IAAhB,EAAsB;QAClB;MACH;;MACD,MAAME,qBAAqB,GAAG,KAAKxB,sBAAL,CAA4BgB,CAA5B,EAA+BK,QAA7D;MACA,MAAMpB,kBAAkB,GAAG,KAAKA,kBAAL,CAAwBe,CAAxB,EAA2BK,QAAtD;MACAtC,IAAI,CAAC,MAAM;QACP,MAAM0C,wBAAwB,GAAGzC,GAAG,CAACE,GAAG,CAACsC,qBAAD,EAAwB,KAAK5B,KAA7B,CAAJ,EAAyCV,GAAG,CAACE,MAAM,CAACkC,QAAD,CAAP,EAAmB,IAAI,KAAK1B,KAA5B,CAA5C,CAApC;;QACA,IAAI,KAAKG,QAAT,EAAmB;UACf,MAAM2B,mBAAmB,GAAG,KAAKxB,oBAAL,CAA0Bc,CAA1B,EAA6BK,QAAzD,CADe,CAEf;;UACA,MAAMM,sBAAsB,GAAG3C,GAAG,CAACE,GAAG,CAACwC,mBAAD,EAAsB,KAAK9B,KAA3B,CAAJ,EAAuCV,GAAG,CAACoC,QAAD,EAAW,IAAI,KAAK1B,KAApB,CAA1C,CAAlC;UACA,MAAMgC,gBAAgB,GAAG3C,GAAG,CAACC,GAAG,CAACoC,QAAD,EAAW,KAAK3B,YAAhB,CAAJ,EAAmCR,IAAI,CAACE,GAAG,CAACoC,wBAAD,EAA2BzC,GAAG,CAACI,MAAM,CAACuC,sBAAD,CAAP,EAAiC,KAAK7B,OAAtC,CAA9B,CAAJ,CAAvC,CAA5B;UACA,MAAM+B,qBAAqB,GAAG7C,GAAG,CAACE,GAAG,CAACe,kBAAD,EAAqB,KAAKJ,QAA1B,CAAJ,EAAyC+B,gBAAzC,CAAjC;UACAJ,qBAAqB,CAACM,MAAtB,CAA6BL,wBAA7B;UACAC,mBAAmB,CAACI,MAApB,CAA2BH,sBAA3B;UACA1B,kBAAkB,CAAC6B,MAAnB,CAA0BD,qBAA1B;UACA,MAAME,QAAQ,GAAG1C,GAAG,CAAC4B,KAAD,EAAQY,qBAAR,CAApB;UACAZ,KAAK,CAACa,MAAN,CAAaC,QAAb;QACH,CAXD,MAYK;UACD;UACA,MAAMN,wBAAwB,GAAGzC,GAAG,CAACE,GAAG,CAACsC,qBAAD,EAAwB,KAAK5B,KAA7B,CAAJ,EAAyCV,GAAG,CAACE,MAAM,CAACkC,QAAD,CAAP,EAAmB,IAAI,KAAK1B,KAA5B,CAA5C,CAApC;UACA,MAAMiC,qBAAqB,GAAG7C,GAAG,CAACE,GAAG,CAACe,kBAAD,EAAqB,KAAKJ,QAA1B,CAAJ,EAAyCZ,GAAG,CAACC,GAAG,CAACoC,QAAD,EAAW,KAAK3B,YAAhB,CAAJ,EAAmCR,IAAI,CAACH,GAAG,CAACyC,wBAAD,EAA2B,KAAK3B,OAAhC,CAAJ,CAAvC,CAA5C,CAAjC;UACA0B,qBAAqB,CAACM,MAAtB,CAA6BL,wBAA7B;UACAxB,kBAAkB,CAAC6B,MAAnB,CAA0BD,qBAA1B;UACA,MAAME,QAAQ,GAAG1C,GAAG,CAAC4B,KAAD,EAAQY,qBAAR,CAApB;UACAZ,KAAK,CAACa,MAAN,CAAaC,QAAb;QACH;MACJ,CAvBG,CAAJ;IAwBH,CArDD;IAsDA,KAAKC,mBAAL;EACH;;EACDlD,OAAO,GAAG;IACN,IAAI,KAAKkB,sBAAL,IAA+B,IAAnC,EAAyC;MACrClB,OAAO,CAAC,KAAKkB,sBAAL,CAA4BU,GAA5B,CAAgCuB,CAAC,IAAIA,CAAC,CAACZ,QAAvC,CAAD,CAAP;IACH;;IACD,IAAI,KAAKnB,oBAAL,IAA6B,IAA7B,IAAqC,KAAKH,QAA9C,EAAwD;MACpDjB,OAAO,CAAC,KAAKoB,oBAAL,CAA0BQ,GAA1B,CAA8BuB,CAAC,IAAIA,CAAC,CAACZ,QAArC,CAAD,CAAP;IACH;;IACD,IAAI,KAAKpB,kBAAL,IAA2B,IAA/B,EAAqC;MACjCnB,OAAO,CAAC,KAAKmB,kBAAL,CAAwBS,GAAxB,CAA4BuB,CAAC,IAAIA,CAAC,CAACZ,QAAnC,CAAD,CAAP;IACH;EACJ;;EACKa,UAAU,GAAG;IAAA;;IAAA;MACf;MACA,MAAMC,SAAS,GAAG,CAAC,GAAG,KAAI,CAACnC,sBAAT,EAAiC,GAAG,KAAI,CAACC,kBAAzC,CAAlB;;MACA,IAAI,KAAI,CAACF,QAAT,EAAmB;QACfoC,SAAS,CAACC,IAAV,CAAe,GAAG,KAAI,CAAClC,oBAAvB;MACH;;MACD,OAAO,OAAO,KAAI,CAACmC,cAAL,EAAP,EAA8BC,MAA9B,CAAqCH,SAAS,CAACzB,GAAV,CAAcuB,CAAC,KAAK;QAAErB,IAAI,EAAEqB,CAAC,CAACb,YAAV;QAAwBG,MAAM,EAAEU,CAAC,CAACZ;MAAlC,CAAL,CAAf,CAArC,CAAP;IANe;EAOlB;;EACKkB,UAAU,CAACC,YAAD,EAAe;IAAA;;IAAA;MAC3BA,YAAY,SAAS,MAAI,CAACC,iBAAL,CAAuBD,YAAvB,CAArB;MACA,MAAME,aAAa,GAAG,MAAI,CAAC3C,QAAL,GAAgByC,YAAY,CAACG,MAAb,GAAsB,CAAtC,GAA0CH,YAAY,CAACG,MAAb,GAAsB,CAAtF;MACA,MAAMxB,SAAS,GAAG,KAAlB;MACA,MAAI,CAACnB,sBAAL,GACIwC,YAAY,CAACI,KAAb,CAAmB,CAAnB,EAAsBF,aAAtB,EAAqChC,GAArC,CAAyCuB,CAAC,KAAK;QAC3Cb,YAAY,EAAEa,CAAC,CAACrB,IAD2B;QAE3CS,QAAQ,EAAEY,CAAC,CAACV,MAAF,CAASF,QAAT,CAAkBF,SAAlB;MAFiC,CAAL,CAA1C,CADJ;MAKA,MAAI,CAAClB,kBAAL,GACIuC,YAAY,CAACI,KAAb,CAAmBF,aAAnB,EAAkCA,aAAa,GAAG,CAAlD,EACKhC,GADL,CACSuB,CAAC,KAAK;QACXb,YAAY,EAAEa,CAAC,CAACrB,IADL;QAEXS,QAAQ,EAAEY,CAAC,CAACV,MAAF,CAASF,QAAT,CAAkBF,SAAlB;MAFC,CAAL,CADV,CADJ;;MAMA,IAAI,MAAI,CAACpB,QAAT,EAAmB;QACf,MAAI,CAACG,oBAAL,GACIsC,YAAY,CAACI,KAAb,CAAmBF,aAAa,GAAG,CAAnC,EAAsCA,aAAa,GAAG,CAAtD,EACKhC,GADL,CACSuB,CAAC,KAAK;UACXb,YAAY,EAAEa,CAAC,CAACrB,IADL;UAEXS,QAAQ,EAAEY,CAAC,CAACV,MAAF,CAASF,QAAT,CAAkBF,SAAlB;QAFC,CAAL,CADV,CADJ;MAMH;IAtB0B;EAuB9B;;EACD0B,SAAS,GAAG;IACR,OAAO;MACH,gBAAgB,KAAKlD,YADlB;MAEH,SAAS,KAAKC,KAFX;MAGH,YAAY,KAAKC,QAHd;MAIH,WAAW,KAAKC,OAJb;MAKH,YAAY,KAAKC;IALd,CAAP;EAOH;EACD;;;EACiB,OAAV+C,UAAU,CAACC,GAAD,EAAMC,MAAN,EAAc;IAC3B,OAAO,IAAID,GAAJ,CAAQC,MAAM,CAAC,cAAD,CAAd,EAAgCA,MAAM,CAAC,OAAD,CAAtC,EAAiDA,MAAM,CAAC,UAAD,CAAvD,EAAqEA,MAAM,CAAC,SAAD,CAA3E,EAAwFA,MAAM,CAAC,UAAD,CAA9F,CAAP;EACH;;AArI2C;AAuIhD;;AACAvD,gBAAgB,CAACwD,SAAjB,GAA6B,SAA7B,C,CAAwC;;AACxC1D,aAAa,CAACE,gBAAD,CAAb"},"metadata":{},"sourceType":"module"}