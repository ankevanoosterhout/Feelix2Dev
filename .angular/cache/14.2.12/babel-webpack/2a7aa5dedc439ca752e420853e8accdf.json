{"ast":null,"code":"import _asyncToGenerator from \"C:/Users/Anke/Documents/Feelix documents/Feelix2.0-dev/Feelix v2/node_modules/@babel/runtime/helpers/esm/asyncToGenerator.js\";\n\nvar _fitDataset, _evaluateDataset;\n\n/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * Interfaces and methods for training models using TensorFlow.js datasets.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { scalar } from '@tensorflow/tfjs-core';\nimport { configureCallbacks, standardizeCallbacks } from '../base_callbacks';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { disposeTensorsInLogs } from '../logs';\nimport { singletonOrArray, toList } from '../utils/generic_utils';\nimport { standardizeClassWeights, standardizeWeights } from './training_utils'; // Default batch size used during tensor-based validation.\n\nconst DEFAULT_VALIDATION_BATCH_SIZE = 32;\n/**\n * Standardize the output of a dataset iterator for use by\n * LayersModel.fitDataset().\n *\n * @param model: A `tf.LayersModel` object.\n * @param iteratorOut The output of a dataset iterator. It is required to be\n *   an object of the form `{xs: TensorOrArrayOrMap, ys:\n * TensorOrArrayOrMap}`, where `TensorOrArrayOrMap` is a single `tf.Tensor`,\n * a `tf.Tensor[]`, or a flat map from string names to `tf.Tensor`s.\n * @returns A flat array of `tf.Tensor` objects: the input `tf.Tensor`s\n *   followed by the target `tf.Tensor`s.  When `tf.Tensor`s are provided\n *   as a map, the order in the resulting array is taken from the `inputNames`\n *   and `outputNames` of the model.\n */\n\nfunction standardizeDataIteratorOutput( // Type `model` as `any` here to avoid circular dependency w/\n// training.ts.\n// tslint:disable-next-line:no-any\nmodel, iteratorOut) {\n  let xs;\n  let ys;\n  const iteratorOutObj = iteratorOut;\n  xs = iteratorOutObj['xs'];\n  ys = iteratorOutObj['ys'];\n  tfc.util.assert(xs != null && ys != null, () => 'A Dataset iterator for fitDataset() is expected to generate ' + 'objects of the form `{xs: xVal, ys: yVal}`, where the two ' + 'values may be `tf.Tensor`, an array of Tensors, or a map of ' + 'string to Tensor.  The provided Dataset instead generates ' + `${iteratorOut}`);\n  const flattenedXs = flattenTensorOrArrayOrMap('input', model.inputNames, xs);\n  const flattenedYs = flattenTensorOrArrayOrMap('output', model.outputNames, ys);\n  const batchSize = flattenedXs[0].shape[0];\n  tfc.util.assert(flattenedXs.length === model.inputs.length, () => `LayersModel has ${model.inputs.length} inputs, but the dataset ` + `provides ${flattenedXs.length} inputs.  (Expected input keys: ` + `${JSON.stringify(model.inputNames)})`);\n  tfc.util.assert(flattenedYs.length === model.outputs.length, () => `LayersModel has ${model.outputs.length} outputs, but the dataset ` + `provides ${flattenedYs.length} outputs.  (Expected output keys: ` + `${JSON.stringify(model.outputNames)})`);\n\n  for (let xIndex = 0; xIndex < flattenedXs.length; xIndex++) {\n    tfc.util.assert(flattenedXs[xIndex].shape[0] === batchSize, () => `Batch size mismatch: input ` + `${model.inputNames[xIndex]} has ${flattenedXs[xIndex].shape[0]}; ` + `expected  ${batchSize} based on input ${model.inputNames[0]}.`);\n  }\n\n  for (let yIndex = 0; yIndex < flattenedYs.length; yIndex++) {\n    tfc.util.assert(flattenedYs[yIndex].shape[0] === batchSize, () => `Batch size mismatch: output ` + `${model.outputNames[yIndex]} has ${flattenedYs[yIndex].shape[0]}; ` + `expected  ${batchSize} based on input ${model.inputNames[0]}.`);\n  }\n\n  return {\n    xs: flattenedXs,\n    ys: flattenedYs\n  };\n}\n\nfunction flattenTensorOrArrayOrMap(inputOrOutput, names, values) {\n  if (values instanceof tfc.Tensor) {\n    return [values];\n  } else if (Array.isArray(values)) {\n    tfc.util.assert(values.length === names.length, () => `Received an array of ${values.length} Tensors, but expected ${names.length} to match the ${inputOrOutput} keys ${names}.`);\n    return values;\n  } else {\n    const result = []; // Check that all the required keys are available.\n\n    for (const name of names) {\n      if (values[name] == null) {\n        throw new ValueError(`The feature data generated by the dataset lacks the required ` + `${inputOrOutput} key '${name}'.`);\n      }\n\n      result.push(values[name]);\n    }\n\n    return result;\n  }\n}\n\nfunction standardizeTensorValidationData(data) {\n  if (data.length === 3) {\n    throw new NotImplementedError('Validation with sample weights is not implemented yet.');\n  }\n\n  return {\n    xs: data[0],\n    ys: data[1]\n  };\n}\n\nexport function fitDataset(_x, _x2, _x3) {\n  return (_fitDataset = _fitDataset || _asyncToGenerator(function* ( // Type `model` as `any` here to avoid circular dependency w/\n  // training.ts.\n  // tslint:disable-next-line:no-any\n  model, dataset, args) {\n    const hasBatchesPerEpoch = args.batchesPerEpoch != null;\n    tfc.util.assert(model.optimizer != null, () => 'You must compile a model before training/testing. Use ' + 'LayersModel.compile(modelCompileConfig).');\n    tfc.util.assert(args != null, () => `For fitDataset(), the 2nd argument (config) is required, ` + `but it is not provided in this call.`);\n    tfc.util.assert(args.epochs != null && args.epochs > 0 && Number.isInteger(args.epochs), () => `For fitDataset(), config.epochs is expected to be a positive ` + `integer, but got ${args.epochs}`);\n    tfc.util.assert(!hasBatchesPerEpoch || args.batchesPerEpoch > 0 && Number.isInteger(args.batchesPerEpoch), () => `For fitDataset(), config.batchesPerEpoch is expected to be a ` + `positive integer if specified, but got ${args.batchesPerEpoch}`);\n    tfc.util.assert( // tslint:disable-next-line:no-any\n    args['validationSplit'] == null, () => '`validationSplit` is not supported by `fitDataset()`. ' + 'Use validationData instead.');\n\n    if (model.isTraining) {\n      throw new Error('Cannot start training because another fit() call is ongoing.');\n    }\n\n    model.isTraining = true;\n\n    try {\n      const doValidation = args.validationData != null;\n      let valXs;\n      let valYs;\n\n      if (doValidation) {\n        if (isDatasetObject(args.validationData)) {\n          tfc.util.assert(args.validationBatches == null || args.validationBatches > 0 && Number.isInteger(args.validationBatches), () => `For fitDataset() with dataset-based validation, ` + `config.validationBatches is expected not to be provided, ` + `or to be a positive integer, ` + `but got ${args.validationBatches}`);\n        } else {\n          const validationData = standardizeTensorValidationData(args.validationData);\n          valXs = validationData.xs;\n          valYs = validationData.ys;\n        }\n      }\n\n      const trainFunction = model.makeTrainFunction();\n      const outLabels = model.getDedupedMetricsNames();\n      let callbackMetrics;\n\n      if (doValidation) {\n        callbackMetrics = outLabels.slice().concat(outLabels.map(n => 'val_' + n));\n      } else {\n        callbackMetrics = outLabels.slice();\n      }\n\n      const callbacks = standardizeCallbacks(args.callbacks, args.yieldEvery);\n      const verbose = args.verbose == null ? 1 : args.verbose;\n      const {\n        callbackList,\n        history\n      } = configureCallbacks(callbacks, verbose, args.epochs, null, null, getStepsPerEpoch(dataset, args), null, // Batch size determined by the dataset itself.\n      doValidation, callbackMetrics);\n      callbackList.setModel(model);\n      model.history = history;\n      yield callbackList.onTrainBegin();\n      model.stopTraining_ = false;\n      let epoch = args.initialEpoch == null ? 0 : args.initialEpoch;\n      let dataIterator = yield dataset.iterator();\n\n      while (epoch < args.epochs) {\n        const epochLogs = {};\n        yield callbackList.onEpochBegin(epoch);\n        let stepsDone = 0;\n        let batchIndex = 0;\n\n        if (!hasBatchesPerEpoch) {\n          dataIterator = yield dataset.iterator();\n        }\n\n        while (hasBatchesPerEpoch ? stepsDone < args.batchesPerEpoch : true) {\n          const iteratorOut = yield dataIterator.next(); // If `batchesPerEpoch` is specified, the dataset should not be\n          // exhausted until all epoches are done.\n\n          if (hasBatchesPerEpoch && iteratorOut.done) {\n            console.warn('You provided `batchesPerEpoch` as ' + `${args.batchesPerEpoch}, ` + 'but your dataset iterator ran out of data after ' + `${stepsDone} batches; ` + 'interrupting training. Make sure that your ' + 'dataset can generate at least `batchesPerEpoch * epochs` ' + 'batches (in this case, ' + `${args.batchesPerEpoch * args.epochs} batches). ` + 'You may need to use the repeat() function when building ' + 'your dataset.');\n            break;\n          }\n\n          if (iteratorOut.value != null) {\n            const {\n              xs,\n              ys\n            } = standardizeDataIteratorOutput(model, iteratorOut.value);\n            const batchLogs = {};\n            batchLogs['batch'] = batchIndex;\n            batchLogs['size'] = xs[0].shape[0];\n            yield callbackList.onBatchBegin(batchIndex, batchLogs);\n            const sampleWeights = [];\n\n            if (args.classWeight != null) {\n              const standardClassWeights = standardizeClassWeights(args.classWeight, model.outputNames);\n\n              for (let i = 0; i < standardClassWeights.length; ++i) {\n                sampleWeights.push(yield standardizeWeights(ys[i], null, standardClassWeights[i]));\n              }\n            } // Train on batch.\n\n\n            const ins = xs.concat(ys).concat(sampleWeights);\n            const outs = trainFunction(ins);\n            tfc.dispose(ins);\n\n            for (let i = 0; i < outLabels.length; ++i) {\n              const label = outLabels[i];\n              const out = outs[i];\n              batchLogs[label] = out;\n              tfc.keep(out);\n            }\n\n            yield callbackList.onBatchEnd(batchIndex, batchLogs);\n            disposeTensorsInLogs(batchLogs);\n            batchIndex++;\n            stepsDone++;\n          }\n\n          if (hasBatchesPerEpoch ? stepsDone >= args.batchesPerEpoch : iteratorOut.done) {\n            // Epoch finished. Perform validation.\n            if (doValidation) {\n              let valOuts;\n\n              if (isDatasetObject(args.validationData)) {\n                valOuts = toList(yield model.evaluateDataset(args.validationData, {\n                  batches: args.validationBatches\n                }));\n              } else {\n                valOuts = toList(model.evaluate(valXs, valYs, {\n                  batchSize: args.validationBatchSize == null ? DEFAULT_VALIDATION_BATCH_SIZE : args.validationBatchSize,\n                  verbose: 0\n                }));\n              }\n\n              for (let i = 0; i < model.metricsNames.length; ++i) {\n                epochLogs[`val_${model.metricsNames[i]}`] = valOuts[i];\n              }\n            } // Call `break` to exit one epoch lopp after validation is done. If\n            // config.batchesPerEpoch is specified, an epoch while loop will\n            // stop when `stepsDone >= config.batchesPerEpoch`. When\n            // config.batchesPerEpoch is not provided, the following `break` is\n            // required to exit the while lopp after dataset is exhausted.\n\n\n            break;\n          }\n\n          if (model.stopTraining_) {\n            break;\n          }\n        }\n\n        yield callbackList.onEpochEnd(epoch, epochLogs);\n        epoch++;\n\n        if (model.stopTraining_) {\n          break;\n        }\n      }\n\n      yield callbackList.onTrainEnd();\n      yield model.history.syncData();\n      return model.history;\n    } finally {\n      model.isTraining = false;\n    }\n  })).apply(this, arguments);\n}\n/** Helper function that determines number of steps (batches) per epoch. */\n\nfunction getStepsPerEpoch(dataset, args) {\n  // Attempt to determine # of batches in an epoch.\n  let stepsPerEpoch = null;\n\n  if (args.batchesPerEpoch != null) {\n    stepsPerEpoch = args.batchesPerEpoch;\n  } else if (Number.isFinite(dataset.size)) {\n    stepsPerEpoch = dataset.size;\n  }\n\n  return stepsPerEpoch;\n} // Check if provided object is a Dataset object by checking its .iterator\n// element.\n\n\nfunction isDatasetObject(dataset) {\n  return typeof dataset.iterator === 'function';\n} // Check if provided object is a LazyIterator object by checking it's .next\n// element.\n\n\nfunction isLazyIteratorObject(iterator) {\n  return typeof iterator.next === 'function';\n}\n\nexport function evaluateDataset(_x4, _x5, _x6) {\n  return (_evaluateDataset = _evaluateDataset || _asyncToGenerator(function* ( // Type `model` as `any` here to avoid circular dependency w/\n  // training.ts.\n  // tslint:disable-next-line:no-any\n  model, dataset, args) {\n    args = args || {};\n    const hasBatches = args.batches != null;\n    const f = model.testFunction;\n    let outs = [];\n\n    if (args.verbose > 0) {\n      throw new NotImplementedError('Verbose mode is not implemented yet.');\n    }\n\n    tfc.util.assert(!hasBatches || args.batches > 0 && Number.isInteger(args.batches), () => 'Test loop expects `batches` to be a positive integer, but ' + `received ${JSON.stringify(args.batches)}`);\n    const dataIterator = isLazyIteratorObject(dataset) ? dataset : yield dataset.iterator(); // Keeps track of number of examples used in this evaluation.\n\n    let numExamples = 0;\n    let batch = 0;\n\n    while (hasBatches ? batch < args.batches : true) {\n      const iteratorOut = yield dataIterator.next();\n      outs = tfc.tidy(() => {\n        if (iteratorOut.value) {\n          // TODO(cais): Once real dataset is available, use\n          //   `map(x => standardizeDataIteratorOutput(model, x).map(f)`.\n          const {\n            xs,\n            ys\n          } = standardizeDataIteratorOutput(model, iteratorOut.value);\n          const xsAndYs = xs.concat(ys);\n          const batchOuts = tfc.tidy(() => f(xsAndYs));\n          tfc.dispose(xsAndYs);\n\n          if (batch === 0) {\n            for (let i = 0; i < batchOuts.length; ++i) {\n              outs.push(scalar(0));\n            }\n          }\n\n          const batchSize = xsAndYs[0].shape[0];\n\n          for (let i = 0; i < batchOuts.length; ++i) {\n            const batchOut = batchOuts[i];\n            const oldScalar = outs[i];\n            outs[i] = tfc.tidy(() => tfc.add(outs[i], tfc.mul(batchSize, batchOut)));\n\n            if (batch > 0) {\n              tfc.dispose(oldScalar);\n            }\n          }\n\n          tfc.dispose(batchOuts);\n          numExamples += batchSize;\n          ++batch;\n        }\n\n        return outs;\n      });\n\n      if (iteratorOut.done) {\n        if (hasBatches) {\n          console.warn('Your dataset iterator ran out of data during evaluateDataset(). ' + 'Interrupting evalution. Make sure that your ' + 'dataset can generate at least `batches` ' + `batches (in this case, ${args.batches} batches). ` + 'You may need to use the repeat() function when building ' + 'your dataset.');\n        }\n\n        break;\n      }\n    }\n\n    for (let i = 0; i < outs.length; ++i) {\n      const oldScalar = outs[i];\n      outs[i] = tfc.div(outs[i], numExamples);\n      tfc.dispose(oldScalar);\n    }\n\n    return singletonOrArray(outs);\n  })).apply(this, arguments);\n}","map":{"version":3,"names":["tfc","scalar","configureCallbacks","standardizeCallbacks","NotImplementedError","ValueError","disposeTensorsInLogs","singletonOrArray","toList","standardizeClassWeights","standardizeWeights","DEFAULT_VALIDATION_BATCH_SIZE","standardizeDataIteratorOutput","model","iteratorOut","xs","ys","iteratorOutObj","util","assert","flattenedXs","flattenTensorOrArrayOrMap","inputNames","flattenedYs","outputNames","batchSize","shape","length","inputs","JSON","stringify","outputs","xIndex","yIndex","inputOrOutput","names","values","Tensor","Array","isArray","result","name","push","standardizeTensorValidationData","data","fitDataset","dataset","args","hasBatchesPerEpoch","batchesPerEpoch","optimizer","epochs","Number","isInteger","isTraining","Error","doValidation","validationData","valXs","valYs","isDatasetObject","validationBatches","trainFunction","makeTrainFunction","outLabels","getDedupedMetricsNames","callbackMetrics","slice","concat","map","n","callbacks","yieldEvery","verbose","callbackList","history","getStepsPerEpoch","setModel","onTrainBegin","stopTraining_","epoch","initialEpoch","dataIterator","iterator","epochLogs","onEpochBegin","stepsDone","batchIndex","next","done","console","warn","value","batchLogs","onBatchBegin","sampleWeights","classWeight","standardClassWeights","i","ins","outs","dispose","label","out","keep","onBatchEnd","valOuts","evaluateDataset","batches","evaluate","validationBatchSize","metricsNames","onEpochEnd","onTrainEnd","syncData","stepsPerEpoch","isFinite","size","isLazyIteratorObject","hasBatches","f","testFunction","numExamples","batch","tidy","xsAndYs","batchOuts","batchOut","oldScalar","add","mul","div"],"sources":["C:/Users/Anke/Documents/Feelix documents/Feelix2.0-dev/Feelix v2/node_modules/@tensorflow/tfjs-layers/dist/engine/training_dataset.js"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n * Interfaces and methods for training models using TensorFlow.js datasets.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { scalar } from '@tensorflow/tfjs-core';\nimport { configureCallbacks, standardizeCallbacks } from '../base_callbacks';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { disposeTensorsInLogs } from '../logs';\nimport { singletonOrArray, toList } from '../utils/generic_utils';\nimport { standardizeClassWeights, standardizeWeights } from './training_utils';\n// Default batch size used during tensor-based validation.\nconst DEFAULT_VALIDATION_BATCH_SIZE = 32;\n/**\n * Standardize the output of a dataset iterator for use by\n * LayersModel.fitDataset().\n *\n * @param model: A `tf.LayersModel` object.\n * @param iteratorOut The output of a dataset iterator. It is required to be\n *   an object of the form `{xs: TensorOrArrayOrMap, ys:\n * TensorOrArrayOrMap}`, where `TensorOrArrayOrMap` is a single `tf.Tensor`,\n * a `tf.Tensor[]`, or a flat map from string names to `tf.Tensor`s.\n * @returns A flat array of `tf.Tensor` objects: the input `tf.Tensor`s\n *   followed by the target `tf.Tensor`s.  When `tf.Tensor`s are provided\n *   as a map, the order in the resulting array is taken from the `inputNames`\n *   and `outputNames` of the model.\n */\nfunction standardizeDataIteratorOutput(\n// Type `model` as `any` here to avoid circular dependency w/\n// training.ts.\n// tslint:disable-next-line:no-any\nmodel, iteratorOut) {\n    let xs;\n    let ys;\n    const iteratorOutObj = iteratorOut;\n    xs = iteratorOutObj['xs'];\n    ys = iteratorOutObj['ys'];\n    tfc.util.assert(xs != null && ys != null, () => 'A Dataset iterator for fitDataset() is expected to generate ' +\n        'objects of the form `{xs: xVal, ys: yVal}`, where the two ' +\n        'values may be `tf.Tensor`, an array of Tensors, or a map of ' +\n        'string to Tensor.  The provided Dataset instead generates ' +\n        `${iteratorOut}`);\n    const flattenedXs = flattenTensorOrArrayOrMap('input', model.inputNames, xs);\n    const flattenedYs = flattenTensorOrArrayOrMap('output', model.outputNames, ys);\n    const batchSize = flattenedXs[0].shape[0];\n    tfc.util.assert(flattenedXs.length === model.inputs.length, () => `LayersModel has ${model.inputs.length} inputs, but the dataset ` +\n        `provides ${flattenedXs.length} inputs.  (Expected input keys: ` +\n        `${JSON.stringify(model.inputNames)})`);\n    tfc.util.assert(flattenedYs.length === model.outputs.length, () => `LayersModel has ${model.outputs.length} outputs, but the dataset ` +\n        `provides ${flattenedYs.length} outputs.  (Expected output keys: ` +\n        `${JSON.stringify(model.outputNames)})`);\n    for (let xIndex = 0; xIndex < flattenedXs.length; xIndex++) {\n        tfc.util.assert(flattenedXs[xIndex].shape[0] === batchSize, () => `Batch size mismatch: input ` +\n            `${model.inputNames[xIndex]} has ${flattenedXs[xIndex].shape[0]}; ` +\n            `expected  ${batchSize} based on input ${model.inputNames[0]}.`);\n    }\n    for (let yIndex = 0; yIndex < flattenedYs.length; yIndex++) {\n        tfc.util.assert(flattenedYs[yIndex].shape[0] === batchSize, () => `Batch size mismatch: output ` +\n            `${model.outputNames[yIndex]} has ${flattenedYs[yIndex].shape[0]}; ` +\n            `expected  ${batchSize} based on input ${model.inputNames[0]}.`);\n    }\n    return { xs: flattenedXs, ys: flattenedYs };\n}\nfunction flattenTensorOrArrayOrMap(inputOrOutput, names, values) {\n    if (values instanceof tfc.Tensor) {\n        return [values];\n    }\n    else if (Array.isArray(values)) {\n        tfc.util.assert(values.length === names.length, () => `Received an array of ${values.length} Tensors, but expected ${names.length} to match the ${inputOrOutput} keys ${names}.`);\n        return values;\n    }\n    else {\n        const result = [];\n        // Check that all the required keys are available.\n        for (const name of names) {\n            if (values[name] == null) {\n                throw new ValueError(`The feature data generated by the dataset lacks the required ` +\n                    `${inputOrOutput} key '${name}'.`);\n            }\n            result.push(values[name]);\n        }\n        return result;\n    }\n}\nfunction standardizeTensorValidationData(data) {\n    if (data.length === 3) {\n        throw new NotImplementedError('Validation with sample weights is not implemented yet.');\n    }\n    return { xs: data[0], ys: data[1] };\n}\nexport async function fitDataset(\n// Type `model` as `any` here to avoid circular dependency w/\n// training.ts.\n// tslint:disable-next-line:no-any\nmodel, dataset, args) {\n    const hasBatchesPerEpoch = args.batchesPerEpoch != null;\n    tfc.util.assert(model.optimizer != null, () => 'You must compile a model before training/testing. Use ' +\n        'LayersModel.compile(modelCompileConfig).');\n    tfc.util.assert(args != null, () => `For fitDataset(), the 2nd argument (config) is required, ` +\n        `but it is not provided in this call.`);\n    tfc.util.assert(args.epochs != null && args.epochs > 0 && Number.isInteger(args.epochs), () => `For fitDataset(), config.epochs is expected to be a positive ` +\n        `integer, but got ${args.epochs}`);\n    tfc.util.assert(!hasBatchesPerEpoch ||\n        (args.batchesPerEpoch > 0 && Number.isInteger(args.batchesPerEpoch)), () => `For fitDataset(), config.batchesPerEpoch is expected to be a ` +\n        `positive integer if specified, but got ${args.batchesPerEpoch}`);\n    tfc.util.assert(\n    // tslint:disable-next-line:no-any\n    args['validationSplit'] == null, () => '`validationSplit` is not supported by `fitDataset()`. ' +\n        'Use validationData instead.');\n    if (model.isTraining) {\n        throw new Error('Cannot start training because another fit() call is ongoing.');\n    }\n    model.isTraining = true;\n    try {\n        const doValidation = args.validationData != null;\n        let valXs;\n        let valYs;\n        if (doValidation) {\n            if (isDatasetObject(args.validationData)) {\n                tfc.util.assert(args.validationBatches == null ||\n                    (args.validationBatches > 0 &&\n                        Number.isInteger(args.validationBatches)), () => `For fitDataset() with dataset-based validation, ` +\n                    `config.validationBatches is expected not to be provided, ` +\n                    `or to be a positive integer, ` +\n                    `but got ${args.validationBatches}`);\n            }\n            else {\n                const validationData = standardizeTensorValidationData(args.validationData);\n                valXs = validationData.xs;\n                valYs = validationData.ys;\n            }\n        }\n        const trainFunction = model.makeTrainFunction();\n        const outLabels = model.getDedupedMetricsNames();\n        let callbackMetrics;\n        if (doValidation) {\n            callbackMetrics =\n                outLabels.slice().concat(outLabels.map(n => 'val_' + n));\n        }\n        else {\n            callbackMetrics = outLabels.slice();\n        }\n        const callbacks = standardizeCallbacks(args.callbacks, args.yieldEvery);\n        const verbose = args.verbose == null ? 1 : args.verbose;\n        const { callbackList, history } = configureCallbacks(callbacks, verbose, args.epochs, null, null, getStepsPerEpoch(dataset, args), null, // Batch size determined by the dataset itself.\n        doValidation, callbackMetrics);\n        callbackList.setModel(model);\n        model.history = history;\n        await callbackList.onTrainBegin();\n        model.stopTraining_ = false;\n        let epoch = args.initialEpoch == null ? 0 : args.initialEpoch;\n        let dataIterator = await dataset.iterator();\n        while (epoch < args.epochs) {\n            const epochLogs = {};\n            await callbackList.onEpochBegin(epoch);\n            let stepsDone = 0;\n            let batchIndex = 0;\n            if (!hasBatchesPerEpoch) {\n                dataIterator = await dataset.iterator();\n            }\n            while (hasBatchesPerEpoch ? stepsDone < args.batchesPerEpoch : true) {\n                const iteratorOut = await dataIterator.next();\n                // If `batchesPerEpoch` is specified, the dataset should not be\n                // exhausted until all epoches are done.\n                if (hasBatchesPerEpoch && iteratorOut.done) {\n                    console.warn('You provided `batchesPerEpoch` as ' +\n                        `${args.batchesPerEpoch}, ` +\n                        'but your dataset iterator ran out of data after ' +\n                        `${stepsDone} batches; ` +\n                        'interrupting training. Make sure that your ' +\n                        'dataset can generate at least `batchesPerEpoch * epochs` ' +\n                        'batches (in this case, ' +\n                        `${args.batchesPerEpoch * args.epochs} batches). ` +\n                        'You may need to use the repeat() function when building ' +\n                        'your dataset.');\n                    break;\n                }\n                if (iteratorOut.value != null) {\n                    const { xs, ys } = standardizeDataIteratorOutput(model, iteratorOut.value);\n                    const batchLogs = {};\n                    batchLogs['batch'] = batchIndex;\n                    batchLogs['size'] = xs[0].shape[0];\n                    await callbackList.onBatchBegin(batchIndex, batchLogs);\n                    const sampleWeights = [];\n                    if (args.classWeight != null) {\n                        const standardClassWeights = standardizeClassWeights(args.classWeight, model.outputNames);\n                        for (let i = 0; i < standardClassWeights.length; ++i) {\n                            sampleWeights.push(await standardizeWeights(ys[i], null, standardClassWeights[i]));\n                        }\n                    }\n                    // Train on batch.\n                    const ins = xs.concat(ys).concat(sampleWeights);\n                    const outs = trainFunction(ins);\n                    tfc.dispose(ins);\n                    for (let i = 0; i < outLabels.length; ++i) {\n                        const label = outLabels[i];\n                        const out = outs[i];\n                        batchLogs[label] = out;\n                        tfc.keep(out);\n                    }\n                    await callbackList.onBatchEnd(batchIndex, batchLogs);\n                    disposeTensorsInLogs(batchLogs);\n                    batchIndex++;\n                    stepsDone++;\n                }\n                if (hasBatchesPerEpoch ? stepsDone >= args.batchesPerEpoch :\n                    iteratorOut.done) {\n                    // Epoch finished. Perform validation.\n                    if (doValidation) {\n                        let valOuts;\n                        if (isDatasetObject(args.validationData)) {\n                            valOuts = toList(await model.evaluateDataset(args.validationData, { batches: args.validationBatches }));\n                        }\n                        else {\n                            valOuts = toList(model.evaluate(valXs, valYs, {\n                                batchSize: args.validationBatchSize == null ?\n                                    DEFAULT_VALIDATION_BATCH_SIZE :\n                                    args.validationBatchSize,\n                                verbose: 0\n                            }));\n                        }\n                        for (let i = 0; i < model.metricsNames.length; ++i) {\n                            epochLogs[`val_${model.metricsNames[i]}`] = valOuts[i];\n                        }\n                    }\n                    // Call `break` to exit one epoch lopp after validation is done. If\n                    // config.batchesPerEpoch is specified, an epoch while loop will\n                    // stop when `stepsDone >= config.batchesPerEpoch`. When\n                    // config.batchesPerEpoch is not provided, the following `break` is\n                    // required to exit the while lopp after dataset is exhausted.\n                    break;\n                }\n                if (model.stopTraining_) {\n                    break;\n                }\n            }\n            await callbackList.onEpochEnd(epoch, epochLogs);\n            epoch++;\n            if (model.stopTraining_) {\n                break;\n            }\n        }\n        await callbackList.onTrainEnd();\n        await model.history.syncData();\n        return model.history;\n    }\n    finally {\n        model.isTraining = false;\n    }\n}\n/** Helper function that determines number of steps (batches) per epoch. */\nfunction getStepsPerEpoch(dataset, args) {\n    // Attempt to determine # of batches in an epoch.\n    let stepsPerEpoch = null;\n    if (args.batchesPerEpoch != null) {\n        stepsPerEpoch = args.batchesPerEpoch;\n    }\n    else if (Number.isFinite(dataset.size)) {\n        stepsPerEpoch = dataset.size;\n    }\n    return stepsPerEpoch;\n}\n// Check if provided object is a Dataset object by checking its .iterator\n// element.\nfunction isDatasetObject(dataset) {\n    return (typeof dataset.iterator === 'function');\n}\n// Check if provided object is a LazyIterator object by checking it's .next\n// element.\nfunction isLazyIteratorObject(iterator) {\n    return (typeof iterator.next === 'function');\n}\nexport async function evaluateDataset(\n// Type `model` as `any` here to avoid circular dependency w/\n// training.ts.\n// tslint:disable-next-line:no-any\nmodel, dataset, args) {\n    args = args || {};\n    const hasBatches = args.batches != null;\n    const f = model.testFunction;\n    let outs = [];\n    if (args.verbose > 0) {\n        throw new NotImplementedError('Verbose mode is not implemented yet.');\n    }\n    tfc.util.assert(!hasBatches || (args.batches > 0 && Number.isInteger(args.batches)), () => 'Test loop expects `batches` to be a positive integer, but ' +\n        `received ${JSON.stringify(args.batches)}`);\n    const dataIterator = isLazyIteratorObject(dataset) ?\n        dataset :\n        await dataset.iterator();\n    // Keeps track of number of examples used in this evaluation.\n    let numExamples = 0;\n    let batch = 0;\n    while (hasBatches ? batch < args.batches : true) {\n        const iteratorOut = await dataIterator.next();\n        outs = tfc.tidy(() => {\n            if (iteratorOut.value) {\n                // TODO(cais): Once real dataset is available, use\n                //   `map(x => standardizeDataIteratorOutput(model, x).map(f)`.\n                const { xs, ys } = standardizeDataIteratorOutput(model, iteratorOut.value);\n                const xsAndYs = xs.concat(ys);\n                const batchOuts = tfc.tidy(() => f(xsAndYs));\n                tfc.dispose(xsAndYs);\n                if (batch === 0) {\n                    for (let i = 0; i < batchOuts.length; ++i) {\n                        outs.push(scalar(0));\n                    }\n                }\n                const batchSize = xsAndYs[0].shape[0];\n                for (let i = 0; i < batchOuts.length; ++i) {\n                    const batchOut = batchOuts[i];\n                    const oldScalar = outs[i];\n                    outs[i] =\n                        tfc.tidy(() => tfc.add(outs[i], tfc.mul(batchSize, batchOut)));\n                    if (batch > 0) {\n                        tfc.dispose(oldScalar);\n                    }\n                }\n                tfc.dispose(batchOuts);\n                numExamples += batchSize;\n                ++batch;\n            }\n            return outs;\n        });\n        if (iteratorOut.done) {\n            if (hasBatches) {\n                console.warn('Your dataset iterator ran out of data during evaluateDataset(). ' +\n                    'Interrupting evalution. Make sure that your ' +\n                    'dataset can generate at least `batches` ' +\n                    `batches (in this case, ${args.batches} batches). ` +\n                    'You may need to use the repeat() function when building ' +\n                    'your dataset.');\n            }\n            break;\n        }\n    }\n    for (let i = 0; i < outs.length; ++i) {\n        const oldScalar = outs[i];\n        outs[i] = tfc.div(outs[i], numExamples);\n        tfc.dispose(oldScalar);\n    }\n    return singletonOrArray(outs);\n}\n"],"mappings":";;;;AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AACA;AACA;AACA;AACA,OAAO,KAAKA,GAAZ,MAAqB,uBAArB;AACA,SAASC,MAAT,QAAuB,uBAAvB;AACA,SAASC,kBAAT,EAA6BC,oBAA7B,QAAyD,mBAAzD;AACA,SAASC,mBAAT,EAA8BC,UAA9B,QAAgD,WAAhD;AACA,SAASC,oBAAT,QAAqC,SAArC;AACA,SAASC,gBAAT,EAA2BC,MAA3B,QAAyC,wBAAzC;AACA,SAASC,uBAAT,EAAkCC,kBAAlC,QAA4D,kBAA5D,C,CACA;;AACA,MAAMC,6BAA6B,GAAG,EAAtC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AACA,SAASC,6BAAT,EACA;AACA;AACA;AACAC,KAJA,EAIOC,WAJP,EAIoB;EAChB,IAAIC,EAAJ;EACA,IAAIC,EAAJ;EACA,MAAMC,cAAc,GAAGH,WAAvB;EACAC,EAAE,GAAGE,cAAc,CAAC,IAAD,CAAnB;EACAD,EAAE,GAAGC,cAAc,CAAC,IAAD,CAAnB;EACAjB,GAAG,CAACkB,IAAJ,CAASC,MAAT,CAAgBJ,EAAE,IAAI,IAAN,IAAcC,EAAE,IAAI,IAApC,EAA0C,MAAM,iEAC5C,4DAD4C,GAE5C,8DAF4C,GAG5C,4DAH4C,GAI3C,GAAEF,WAAY,EAJnB;EAKA,MAAMM,WAAW,GAAGC,yBAAyB,CAAC,OAAD,EAAUR,KAAK,CAACS,UAAhB,EAA4BP,EAA5B,CAA7C;EACA,MAAMQ,WAAW,GAAGF,yBAAyB,CAAC,QAAD,EAAWR,KAAK,CAACW,WAAjB,EAA8BR,EAA9B,CAA7C;EACA,MAAMS,SAAS,GAAGL,WAAW,CAAC,CAAD,CAAX,CAAeM,KAAf,CAAqB,CAArB,CAAlB;EACA1B,GAAG,CAACkB,IAAJ,CAASC,MAAT,CAAgBC,WAAW,CAACO,MAAZ,KAAuBd,KAAK,CAACe,MAAN,CAAaD,MAApD,EAA4D,MAAO,mBAAkBd,KAAK,CAACe,MAAN,CAAaD,MAAO,2BAAvC,GAC7D,YAAWP,WAAW,CAACO,MAAO,kCAD+B,GAE7D,GAAEE,IAAI,CAACC,SAAL,CAAejB,KAAK,CAACS,UAArB,CAAiC,GAFxC;EAGAtB,GAAG,CAACkB,IAAJ,CAASC,MAAT,CAAgBI,WAAW,CAACI,MAAZ,KAAuBd,KAAK,CAACkB,OAAN,CAAcJ,MAArD,EAA6D,MAAO,mBAAkBd,KAAK,CAACkB,OAAN,CAAcJ,MAAO,4BAAxC,GAC9D,YAAWJ,WAAW,CAACI,MAAO,oCADgC,GAE9D,GAAEE,IAAI,CAACC,SAAL,CAAejB,KAAK,CAACW,WAArB,CAAkC,GAFzC;;EAGA,KAAK,IAAIQ,MAAM,GAAG,CAAlB,EAAqBA,MAAM,GAAGZ,WAAW,CAACO,MAA1C,EAAkDK,MAAM,EAAxD,EAA4D;IACxDhC,GAAG,CAACkB,IAAJ,CAASC,MAAT,CAAgBC,WAAW,CAACY,MAAD,CAAX,CAAoBN,KAApB,CAA0B,CAA1B,MAAiCD,SAAjD,EAA4D,MAAO,6BAAD,GAC7D,GAAEZ,KAAK,CAACS,UAAN,CAAiBU,MAAjB,CAAyB,QAAOZ,WAAW,CAACY,MAAD,CAAX,CAAoBN,KAApB,CAA0B,CAA1B,CAA6B,IADF,GAE7D,aAAYD,SAAU,mBAAkBZ,KAAK,CAACS,UAAN,CAAiB,CAAjB,CAAoB,GAFjE;EAGH;;EACD,KAAK,IAAIW,MAAM,GAAG,CAAlB,EAAqBA,MAAM,GAAGV,WAAW,CAACI,MAA1C,EAAkDM,MAAM,EAAxD,EAA4D;IACxDjC,GAAG,CAACkB,IAAJ,CAASC,MAAT,CAAgBI,WAAW,CAACU,MAAD,CAAX,CAAoBP,KAApB,CAA0B,CAA1B,MAAiCD,SAAjD,EAA4D,MAAO,8BAAD,GAC7D,GAAEZ,KAAK,CAACW,WAAN,CAAkBS,MAAlB,CAA0B,QAAOV,WAAW,CAACU,MAAD,CAAX,CAAoBP,KAApB,CAA0B,CAA1B,CAA6B,IADH,GAE7D,aAAYD,SAAU,mBAAkBZ,KAAK,CAACS,UAAN,CAAiB,CAAjB,CAAoB,GAFjE;EAGH;;EACD,OAAO;IAAEP,EAAE,EAAEK,WAAN;IAAmBJ,EAAE,EAAEO;EAAvB,CAAP;AACH;;AACD,SAASF,yBAAT,CAAmCa,aAAnC,EAAkDC,KAAlD,EAAyDC,MAAzD,EAAiE;EAC7D,IAAIA,MAAM,YAAYpC,GAAG,CAACqC,MAA1B,EAAkC;IAC9B,OAAO,CAACD,MAAD,CAAP;EACH,CAFD,MAGK,IAAIE,KAAK,CAACC,OAAN,CAAcH,MAAd,CAAJ,EAA2B;IAC5BpC,GAAG,CAACkB,IAAJ,CAASC,MAAT,CAAgBiB,MAAM,CAACT,MAAP,KAAkBQ,KAAK,CAACR,MAAxC,EAAgD,MAAO,wBAAuBS,MAAM,CAACT,MAAO,0BAAyBQ,KAAK,CAACR,MAAO,iBAAgBO,aAAc,SAAQC,KAAM,GAA9K;IACA,OAAOC,MAAP;EACH,CAHI,MAIA;IACD,MAAMI,MAAM,GAAG,EAAf,CADC,CAED;;IACA,KAAK,MAAMC,IAAX,IAAmBN,KAAnB,EAA0B;MACtB,IAAIC,MAAM,CAACK,IAAD,CAAN,IAAgB,IAApB,EAA0B;QACtB,MAAM,IAAIpC,UAAJ,CAAgB,+DAAD,GAChB,GAAE6B,aAAc,SAAQO,IAAK,IAD5B,CAAN;MAEH;;MACDD,MAAM,CAACE,IAAP,CAAYN,MAAM,CAACK,IAAD,CAAlB;IACH;;IACD,OAAOD,MAAP;EACH;AACJ;;AACD,SAASG,+BAAT,CAAyCC,IAAzC,EAA+C;EAC3C,IAAIA,IAAI,CAACjB,MAAL,KAAgB,CAApB,EAAuB;IACnB,MAAM,IAAIvB,mBAAJ,CAAwB,wDAAxB,CAAN;EACH;;EACD,OAAO;IAAEW,EAAE,EAAE6B,IAAI,CAAC,CAAD,CAAV;IAAe5B,EAAE,EAAE4B,IAAI,CAAC,CAAD;EAAvB,CAAP;AACH;;AACD,gBAAsBC,UAAtB;EAAA,uDAAO,YACP;EACA;EACA;EACAhC,KAJO,EAIAiC,OAJA,EAISC,IAJT,EAIe;IAClB,MAAMC,kBAAkB,GAAGD,IAAI,CAACE,eAAL,IAAwB,IAAnD;IACAjD,GAAG,CAACkB,IAAJ,CAASC,MAAT,CAAgBN,KAAK,CAACqC,SAAN,IAAmB,IAAnC,EAAyC,MAAM,2DAC3C,0CADJ;IAEAlD,GAAG,CAACkB,IAAJ,CAASC,MAAT,CAAgB4B,IAAI,IAAI,IAAxB,EAA8B,MAAO,2DAAD,GAC/B,sCADL;IAEA/C,GAAG,CAACkB,IAAJ,CAASC,MAAT,CAAgB4B,IAAI,CAACI,MAAL,IAAe,IAAf,IAAuBJ,IAAI,CAACI,MAAL,GAAc,CAArC,IAA0CC,MAAM,CAACC,SAAP,CAAiBN,IAAI,CAACI,MAAtB,CAA1D,EAAyF,MAAO,+DAAD,GAC1F,oBAAmBJ,IAAI,CAACI,MAAO,EADpC;IAEAnD,GAAG,CAACkB,IAAJ,CAASC,MAAT,CAAgB,CAAC6B,kBAAD,IACXD,IAAI,CAACE,eAAL,GAAuB,CAAvB,IAA4BG,MAAM,CAACC,SAAP,CAAiBN,IAAI,CAACE,eAAtB,CADjC,EAC0E,MAAO,+DAAD,GAC3E,0CAAyCF,IAAI,CAACE,eAAgB,EAFnE;IAGAjD,GAAG,CAACkB,IAAJ,CAASC,MAAT,EACA;IACA4B,IAAI,CAAC,iBAAD,CAAJ,IAA2B,IAF3B,EAEiC,MAAM,2DACnC,6BAHJ;;IAIA,IAAIlC,KAAK,CAACyC,UAAV,EAAsB;MAClB,MAAM,IAAIC,KAAJ,CAAU,8DAAV,CAAN;IACH;;IACD1C,KAAK,CAACyC,UAAN,GAAmB,IAAnB;;IACA,IAAI;MACA,MAAME,YAAY,GAAGT,IAAI,CAACU,cAAL,IAAuB,IAA5C;MACA,IAAIC,KAAJ;MACA,IAAIC,KAAJ;;MACA,IAAIH,YAAJ,EAAkB;QACd,IAAII,eAAe,CAACb,IAAI,CAACU,cAAN,CAAnB,EAA0C;UACtCzD,GAAG,CAACkB,IAAJ,CAASC,MAAT,CAAgB4B,IAAI,CAACc,iBAAL,IAA0B,IAA1B,IACXd,IAAI,CAACc,iBAAL,GAAyB,CAAzB,IACGT,MAAM,CAACC,SAAP,CAAiBN,IAAI,CAACc,iBAAtB,CAFR,EAEmD,MAAO,kDAAD,GACpD,2DADoD,GAEpD,+BAFoD,GAGpD,WAAUd,IAAI,CAACc,iBAAkB,EALtC;QAMH,CAPD,MAQK;UACD,MAAMJ,cAAc,GAAGd,+BAA+B,CAACI,IAAI,CAACU,cAAN,CAAtD;UACAC,KAAK,GAAGD,cAAc,CAAC1C,EAAvB;UACA4C,KAAK,GAAGF,cAAc,CAACzC,EAAvB;QACH;MACJ;;MACD,MAAM8C,aAAa,GAAGjD,KAAK,CAACkD,iBAAN,EAAtB;MACA,MAAMC,SAAS,GAAGnD,KAAK,CAACoD,sBAAN,EAAlB;MACA,IAAIC,eAAJ;;MACA,IAAIV,YAAJ,EAAkB;QACdU,eAAe,GACXF,SAAS,CAACG,KAAV,GAAkBC,MAAlB,CAAyBJ,SAAS,CAACK,GAAV,CAAcC,CAAC,IAAI,SAASA,CAA5B,CAAzB,CADJ;MAEH,CAHD,MAIK;QACDJ,eAAe,GAAGF,SAAS,CAACG,KAAV,EAAlB;MACH;;MACD,MAAMI,SAAS,GAAGpE,oBAAoB,CAAC4C,IAAI,CAACwB,SAAN,EAAiBxB,IAAI,CAACyB,UAAtB,CAAtC;MACA,MAAMC,OAAO,GAAG1B,IAAI,CAAC0B,OAAL,IAAgB,IAAhB,GAAuB,CAAvB,GAA2B1B,IAAI,CAAC0B,OAAhD;MACA,MAAM;QAAEC,YAAF;QAAgBC;MAAhB,IAA4BzE,kBAAkB,CAACqE,SAAD,EAAYE,OAAZ,EAAqB1B,IAAI,CAACI,MAA1B,EAAkC,IAAlC,EAAwC,IAAxC,EAA8CyB,gBAAgB,CAAC9B,OAAD,EAAUC,IAAV,CAA9D,EAA+E,IAA/E,EAAqF;MACzIS,YADoD,EACtCU,eADsC,CAApD;MAEAQ,YAAY,CAACG,QAAb,CAAsBhE,KAAtB;MACAA,KAAK,CAAC8D,OAAN,GAAgBA,OAAhB;MACA,MAAMD,YAAY,CAACI,YAAb,EAAN;MACAjE,KAAK,CAACkE,aAAN,GAAsB,KAAtB;MACA,IAAIC,KAAK,GAAGjC,IAAI,CAACkC,YAAL,IAAqB,IAArB,GAA4B,CAA5B,GAAgClC,IAAI,CAACkC,YAAjD;MACA,IAAIC,YAAY,SAASpC,OAAO,CAACqC,QAAR,EAAzB;;MACA,OAAOH,KAAK,GAAGjC,IAAI,CAACI,MAApB,EAA4B;QACxB,MAAMiC,SAAS,GAAG,EAAlB;QACA,MAAMV,YAAY,CAACW,YAAb,CAA0BL,KAA1B,CAAN;QACA,IAAIM,SAAS,GAAG,CAAhB;QACA,IAAIC,UAAU,GAAG,CAAjB;;QACA,IAAI,CAACvC,kBAAL,EAAyB;UACrBkC,YAAY,SAASpC,OAAO,CAACqC,QAAR,EAArB;QACH;;QACD,OAAOnC,kBAAkB,GAAGsC,SAAS,GAAGvC,IAAI,CAACE,eAApB,GAAsC,IAA/D,EAAqE;UACjE,MAAMnC,WAAW,SAASoE,YAAY,CAACM,IAAb,EAA1B,CADiE,CAEjE;UACA;;UACA,IAAIxC,kBAAkB,IAAIlC,WAAW,CAAC2E,IAAtC,EAA4C;YACxCC,OAAO,CAACC,IAAR,CAAa,uCACR,GAAE5C,IAAI,CAACE,eAAgB,IADf,GAET,kDAFS,GAGR,GAAEqC,SAAU,YAHJ,GAIT,6CAJS,GAKT,2DALS,GAMT,yBANS,GAOR,GAAEvC,IAAI,CAACE,eAAL,GAAuBF,IAAI,CAACI,MAAO,aAP7B,GAQT,0DARS,GAST,eATJ;YAUA;UACH;;UACD,IAAIrC,WAAW,CAAC8E,KAAZ,IAAqB,IAAzB,EAA+B;YAC3B,MAAM;cAAE7E,EAAF;cAAMC;YAAN,IAAaJ,6BAA6B,CAACC,KAAD,EAAQC,WAAW,CAAC8E,KAApB,CAAhD;YACA,MAAMC,SAAS,GAAG,EAAlB;YACAA,SAAS,CAAC,OAAD,CAAT,GAAqBN,UAArB;YACAM,SAAS,CAAC,MAAD,CAAT,GAAoB9E,EAAE,CAAC,CAAD,CAAF,CAAMW,KAAN,CAAY,CAAZ,CAApB;YACA,MAAMgD,YAAY,CAACoB,YAAb,CAA0BP,UAA1B,EAAsCM,SAAtC,CAAN;YACA,MAAME,aAAa,GAAG,EAAtB;;YACA,IAAIhD,IAAI,CAACiD,WAAL,IAAoB,IAAxB,EAA8B;cAC1B,MAAMC,oBAAoB,GAAGxF,uBAAuB,CAACsC,IAAI,CAACiD,WAAN,EAAmBnF,KAAK,CAACW,WAAzB,CAApD;;cACA,KAAK,IAAI0E,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGD,oBAAoB,CAACtE,MAAzC,EAAiD,EAAEuE,CAAnD,EAAsD;gBAClDH,aAAa,CAACrD,IAAd,OAAyBhC,kBAAkB,CAACM,EAAE,CAACkF,CAAD,CAAH,EAAQ,IAAR,EAAcD,oBAAoB,CAACC,CAAD,CAAlC,CAA3C;cACH;YACJ,CAZ0B,CAa3B;;;YACA,MAAMC,GAAG,GAAGpF,EAAE,CAACqD,MAAH,CAAUpD,EAAV,EAAcoD,MAAd,CAAqB2B,aAArB,CAAZ;YACA,MAAMK,IAAI,GAAGtC,aAAa,CAACqC,GAAD,CAA1B;YACAnG,GAAG,CAACqG,OAAJ,CAAYF,GAAZ;;YACA,KAAK,IAAID,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGlC,SAAS,CAACrC,MAA9B,EAAsC,EAAEuE,CAAxC,EAA2C;cACvC,MAAMI,KAAK,GAAGtC,SAAS,CAACkC,CAAD,CAAvB;cACA,MAAMK,GAAG,GAAGH,IAAI,CAACF,CAAD,CAAhB;cACAL,SAAS,CAACS,KAAD,CAAT,GAAmBC,GAAnB;cACAvG,GAAG,CAACwG,IAAJ,CAASD,GAAT;YACH;;YACD,MAAM7B,YAAY,CAAC+B,UAAb,CAAwBlB,UAAxB,EAAoCM,SAApC,CAAN;YACAvF,oBAAoB,CAACuF,SAAD,CAApB;YACAN,UAAU;YACVD,SAAS;UACZ;;UACD,IAAItC,kBAAkB,GAAGsC,SAAS,IAAIvC,IAAI,CAACE,eAArB,GAClBnC,WAAW,CAAC2E,IADhB,EACsB;YAClB;YACA,IAAIjC,YAAJ,EAAkB;cACd,IAAIkD,OAAJ;;cACA,IAAI9C,eAAe,CAACb,IAAI,CAACU,cAAN,CAAnB,EAA0C;gBACtCiD,OAAO,GAAGlG,MAAM,OAAOK,KAAK,CAAC8F,eAAN,CAAsB5D,IAAI,CAACU,cAA3B,EAA2C;kBAAEmD,OAAO,EAAE7D,IAAI,CAACc;gBAAhB,CAA3C,CAAP,CAAhB;cACH,CAFD,MAGK;gBACD6C,OAAO,GAAGlG,MAAM,CAACK,KAAK,CAACgG,QAAN,CAAenD,KAAf,EAAsBC,KAAtB,EAA6B;kBAC1ClC,SAAS,EAAEsB,IAAI,CAAC+D,mBAAL,IAA4B,IAA5B,GACPnG,6BADO,GAEPoC,IAAI,CAAC+D,mBAHiC;kBAI1CrC,OAAO,EAAE;gBAJiC,CAA7B,CAAD,CAAhB;cAMH;;cACD,KAAK,IAAIyB,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGrF,KAAK,CAACkG,YAAN,CAAmBpF,MAAvC,EAA+C,EAAEuE,CAAjD,EAAoD;gBAChDd,SAAS,CAAE,OAAMvE,KAAK,CAACkG,YAAN,CAAmBb,CAAnB,CAAsB,EAA9B,CAAT,GAA4CQ,OAAO,CAACR,CAAD,CAAnD;cACH;YACJ,CAlBiB,CAmBlB;YACA;YACA;YACA;YACA;;;YACA;UACH;;UACD,IAAIrF,KAAK,CAACkE,aAAV,EAAyB;YACrB;UACH;QACJ;;QACD,MAAML,YAAY,CAACsC,UAAb,CAAwBhC,KAAxB,EAA+BI,SAA/B,CAAN;QACAJ,KAAK;;QACL,IAAInE,KAAK,CAACkE,aAAV,EAAyB;UACrB;QACH;MACJ;;MACD,MAAML,YAAY,CAACuC,UAAb,EAAN;MACA,MAAMpG,KAAK,CAAC8D,OAAN,CAAcuC,QAAd,EAAN;MACA,OAAOrG,KAAK,CAAC8D,OAAb;IACH,CApID,SAqIQ;MACJ9D,KAAK,CAACyC,UAAN,GAAmB,KAAnB;IACH;EACJ,CA/JD;AAAA;AAgKA;;AACA,SAASsB,gBAAT,CAA0B9B,OAA1B,EAAmCC,IAAnC,EAAyC;EACrC;EACA,IAAIoE,aAAa,GAAG,IAApB;;EACA,IAAIpE,IAAI,CAACE,eAAL,IAAwB,IAA5B,EAAkC;IAC9BkE,aAAa,GAAGpE,IAAI,CAACE,eAArB;EACH,CAFD,MAGK,IAAIG,MAAM,CAACgE,QAAP,CAAgBtE,OAAO,CAACuE,IAAxB,CAAJ,EAAmC;IACpCF,aAAa,GAAGrE,OAAO,CAACuE,IAAxB;EACH;;EACD,OAAOF,aAAP;AACH,C,CACD;AACA;;;AACA,SAASvD,eAAT,CAAyBd,OAAzB,EAAkC;EAC9B,OAAQ,OAAOA,OAAO,CAACqC,QAAf,KAA4B,UAApC;AACH,C,CACD;AACA;;;AACA,SAASmC,oBAAT,CAA8BnC,QAA9B,EAAwC;EACpC,OAAQ,OAAOA,QAAQ,CAACK,IAAhB,KAAyB,UAAjC;AACH;;AACD,gBAAsBmB,eAAtB;EAAA,iEAAO,YACP;EACA;EACA;EACA9F,KAJO,EAIAiC,OAJA,EAISC,IAJT,EAIe;IAClBA,IAAI,GAAGA,IAAI,IAAI,EAAf;IACA,MAAMwE,UAAU,GAAGxE,IAAI,CAAC6D,OAAL,IAAgB,IAAnC;IACA,MAAMY,CAAC,GAAG3G,KAAK,CAAC4G,YAAhB;IACA,IAAIrB,IAAI,GAAG,EAAX;;IACA,IAAIrD,IAAI,CAAC0B,OAAL,GAAe,CAAnB,EAAsB;MAClB,MAAM,IAAIrE,mBAAJ,CAAwB,sCAAxB,CAAN;IACH;;IACDJ,GAAG,CAACkB,IAAJ,CAASC,MAAT,CAAgB,CAACoG,UAAD,IAAgBxE,IAAI,CAAC6D,OAAL,GAAe,CAAf,IAAoBxD,MAAM,CAACC,SAAP,CAAiBN,IAAI,CAAC6D,OAAtB,CAApD,EAAqF,MAAM,+DACtF,YAAW/E,IAAI,CAACC,SAAL,CAAeiB,IAAI,CAAC6D,OAApB,CAA6B,EAD7C;IAEA,MAAM1B,YAAY,GAAGoC,oBAAoB,CAACxE,OAAD,CAApB,GACjBA,OADiB,SAEXA,OAAO,CAACqC,QAAR,EAFV,CAVkB,CAalB;;IACA,IAAIuC,WAAW,GAAG,CAAlB;IACA,IAAIC,KAAK,GAAG,CAAZ;;IACA,OAAOJ,UAAU,GAAGI,KAAK,GAAG5E,IAAI,CAAC6D,OAAhB,GAA0B,IAA3C,EAAiD;MAC7C,MAAM9F,WAAW,SAASoE,YAAY,CAACM,IAAb,EAA1B;MACAY,IAAI,GAAGpG,GAAG,CAAC4H,IAAJ,CAAS,MAAM;QAClB,IAAI9G,WAAW,CAAC8E,KAAhB,EAAuB;UACnB;UACA;UACA,MAAM;YAAE7E,EAAF;YAAMC;UAAN,IAAaJ,6BAA6B,CAACC,KAAD,EAAQC,WAAW,CAAC8E,KAApB,CAAhD;UACA,MAAMiC,OAAO,GAAG9G,EAAE,CAACqD,MAAH,CAAUpD,EAAV,CAAhB;UACA,MAAM8G,SAAS,GAAG9H,GAAG,CAAC4H,IAAJ,CAAS,MAAMJ,CAAC,CAACK,OAAD,CAAhB,CAAlB;UACA7H,GAAG,CAACqG,OAAJ,CAAYwB,OAAZ;;UACA,IAAIF,KAAK,KAAK,CAAd,EAAiB;YACb,KAAK,IAAIzB,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAG4B,SAAS,CAACnG,MAA9B,EAAsC,EAAEuE,CAAxC,EAA2C;cACvCE,IAAI,CAAC1D,IAAL,CAAUzC,MAAM,CAAC,CAAD,CAAhB;YACH;UACJ;;UACD,MAAMwB,SAAS,GAAGoG,OAAO,CAAC,CAAD,CAAP,CAAWnG,KAAX,CAAiB,CAAjB,CAAlB;;UACA,KAAK,IAAIwE,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAG4B,SAAS,CAACnG,MAA9B,EAAsC,EAAEuE,CAAxC,EAA2C;YACvC,MAAM6B,QAAQ,GAAGD,SAAS,CAAC5B,CAAD,CAA1B;YACA,MAAM8B,SAAS,GAAG5B,IAAI,CAACF,CAAD,CAAtB;YACAE,IAAI,CAACF,CAAD,CAAJ,GACIlG,GAAG,CAAC4H,IAAJ,CAAS,MAAM5H,GAAG,CAACiI,GAAJ,CAAQ7B,IAAI,CAACF,CAAD,CAAZ,EAAiBlG,GAAG,CAACkI,GAAJ,CAAQzG,SAAR,EAAmBsG,QAAnB,CAAjB,CAAf,CADJ;;YAEA,IAAIJ,KAAK,GAAG,CAAZ,EAAe;cACX3H,GAAG,CAACqG,OAAJ,CAAY2B,SAAZ;YACH;UACJ;;UACDhI,GAAG,CAACqG,OAAJ,CAAYyB,SAAZ;UACAJ,WAAW,IAAIjG,SAAf;UACA,EAAEkG,KAAF;QACH;;QACD,OAAOvB,IAAP;MACH,CA5BM,CAAP;;MA6BA,IAAItF,WAAW,CAAC2E,IAAhB,EAAsB;QAClB,IAAI8B,UAAJ,EAAgB;UACZ7B,OAAO,CAACC,IAAR,CAAa,qEACT,8CADS,GAET,0CAFS,GAGR,0BAAyB5C,IAAI,CAAC6D,OAAQ,aAH9B,GAIT,0DAJS,GAKT,eALJ;QAMH;;QACD;MACH;IACJ;;IACD,KAAK,IAAIV,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGE,IAAI,CAACzE,MAAzB,EAAiC,EAAEuE,CAAnC,EAAsC;MAClC,MAAM8B,SAAS,GAAG5B,IAAI,CAACF,CAAD,CAAtB;MACAE,IAAI,CAACF,CAAD,CAAJ,GAAUlG,GAAG,CAACmI,GAAJ,CAAQ/B,IAAI,CAACF,CAAD,CAAZ,EAAiBwB,WAAjB,CAAV;MACA1H,GAAG,CAACqG,OAAJ,CAAY2B,SAAZ;IACH;;IACD,OAAOzH,gBAAgB,CAAC6F,IAAD,CAAvB;EACH,CArED;AAAA"},"metadata":{},"sourceType":"module"}