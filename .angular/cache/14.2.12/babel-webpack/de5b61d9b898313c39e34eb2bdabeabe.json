{"ast":null,"code":"/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { broadcast_util, upcastType, util } from '@tensorflow/tfjs-core';\nimport { mapActivationToShaderProgram } from '../kernel_utils/kernel_funcs_utils';\nimport { MatMulPackedProgram } from '../mulmat_packed_gpu';\nimport { multiply } from './Multiply';\nimport { reshape } from './Reshape';\nimport { sum } from './Sum';\nimport { transpose } from './Transpose'; // Empirically determined minimal shared dimension in matmul before we forward\n// to a.mul(b).sum() in order to take advantage of GPU parallelism. See\n// https://github.com/tensorflow/tfjs-core/pull/1379 for benchmarks.\n\nexport const MATMUL_SHARED_DIM_THRESHOLD = 1000;\nexport function batchMatMulImpl({\n  a,\n  b,\n  transposeA,\n  transposeB,\n  backend,\n  bias = null,\n  preluActivationWeights = null,\n  leakyreluAlpha = 0,\n  activation = null\n}) {\n  const aRank = a.shape.length;\n  const bRank = b.shape.length;\n  const innerShapeA = transposeA ? a.shape[aRank - 2] : a.shape[aRank - 1];\n  const innerShapeB = transposeB ? b.shape[bRank - 1] : b.shape[bRank - 2];\n  const outerShapeA = transposeA ? a.shape[aRank - 1] : a.shape[aRank - 2];\n  const outerShapeB = transposeB ? b.shape[bRank - 2] : b.shape[bRank - 1];\n  const outerDimsA = a.shape.slice(0, -2);\n  const outerDimsB = b.shape.slice(0, -2);\n  const batchDimA = util.sizeFromShape(outerDimsA);\n  const batchDimB = util.sizeFromShape(outerDimsB);\n  const outShapeOuterDims = broadcast_util.assertAndGetBroadcastShape(a.shape.slice(0, -2), b.shape.slice(0, -2));\n  const outShape = outShapeOuterDims.concat([outerShapeA, outerShapeB]);\n  util.assert(innerShapeA === innerShapeB, () => `Error in matMul: inner shapes (${innerShapeA}) and (` + `${innerShapeB}) of Tensors with shapes ${a.shape} and ` + `${b.shape} and transposeA=${transposeA}` + ` and transposeB=${transposeB} must match.`);\n  const a3dShape = transposeA ? [batchDimA, innerShapeA, outerShapeA] : [batchDimA, outerShapeA, innerShapeA];\n  const b3dShape = transposeB ? [batchDimB, outerShapeB, innerShapeB] : [batchDimB, innerShapeB, outerShapeB]; // The rest of the implementation is designed to operate on rank-3 tensors\n\n  const a3d = reshape({\n    inputs: {\n      x: a\n    },\n    backend,\n    attrs: {\n      shape: a3dShape\n    }\n  });\n  const b3d = reshape({\n    inputs: {\n      x: b\n    },\n    backend,\n    attrs: {\n      shape: b3dShape\n    }\n  });\n  const intermediates = [a3d, b3d];\n  const batchDim = Math.max(batchDimA, batchDimB);\n  const sharedDim = transposeA ? a3d.shape[1] : a3d.shape[2];\n  const hasBias = bias != null;\n  const hasPreluActivationWeights = preluActivationWeights != null;\n  const hasLeakyreluAlpha = activation === 'leakyrelu';\n  const fusedActivation = activation != null ? mapActivationToShaderProgram(activation, true) : null;\n  const containsFusedOps = hasBias || hasPreluActivationWeights || hasLeakyreluAlpha || fusedActivation != null;\n  let out; // Since the matrices are vectors, it is faster to call mul().sum()\n  // because sum() is O(sqrt(N)) due to divide-and-conquer.\n\n  if ((outerShapeA === 1 || outerShapeB === 1) && sharedDim > MATMUL_SHARED_DIM_THRESHOLD && containsFusedOps === false) {\n    let aVec = a3d;\n    let bVec = b3d;\n\n    if (transposeA) {\n      aVec = transpose({\n        inputs: {\n          x: a3d\n        },\n        backend,\n        attrs: {\n          perm: [0, 2, 1]\n        }\n      });\n      intermediates.push(aVec);\n    }\n\n    if (transposeB) {\n      bVec = transpose({\n        inputs: {\n          x: b3d\n        },\n        backend,\n        attrs: {\n          perm: [0, 2, 1]\n        }\n      });\n      intermediates.push(bVec);\n    }\n\n    const shouldReshapeA = outerShapeB !== 1;\n    const shouldReshapeB = outerShapeB === 1;\n    let aVec3d = aVec;\n\n    if (shouldReshapeA) {\n      aVec3d = reshape({\n        inputs: {\n          x: aVec\n        },\n        backend,\n        attrs: {\n          shape: [batchDim, sharedDim, 1]\n        }\n      });\n      intermediates.push(aVec3d);\n    }\n\n    const axis = outerShapeB === 1 ? 2 : 1;\n    let bVec3d = bVec;\n\n    if (shouldReshapeB) {\n      bVec3d = reshape({\n        inputs: {\n          x: bVec\n        },\n        backend,\n        attrs: {\n          shape: [batchDim, 1, sharedDim]\n        }\n      });\n      intermediates.push(bVec3d);\n    }\n\n    const product = multiply({\n      inputs: {\n        a: aVec3d,\n        b: bVec3d\n      },\n      backend\n    });\n    out = sum({\n      inputs: {\n        x: product\n      },\n      backend,\n      attrs: {\n        axis,\n        keepDims: true\n      }\n    });\n    intermediates.push(product);\n  } else {\n    const dtype = upcastType(a.dtype, b.dtype);\n    const program = new MatMulPackedProgram(a3dShape, b3dShape, [batchDim, outerShapeA, outerShapeB], transposeA, transposeB, hasBias, fusedActivation, hasPreluActivationWeights, hasLeakyreluAlpha);\n    const inputs = [a3d, b3d];\n\n    if (bias != null) {\n      inputs.push(bias);\n    }\n\n    if (hasPreluActivationWeights) {\n      inputs.push(preluActivationWeights);\n    }\n\n    if (hasLeakyreluAlpha) {\n      const $leakyreluAlpha = backend.makeTensorInfo([], 'float32', util.createScalarValue(leakyreluAlpha, 'float32'));\n      inputs.push($leakyreluAlpha);\n      intermediates.push($leakyreluAlpha);\n    }\n\n    out = backend.runWebGLProgram(program, inputs, dtype);\n  }\n\n  const outReshaped = reshape({\n    inputs: {\n      x: out\n    },\n    backend,\n    attrs: {\n      shape: outShape\n    }\n  });\n  intermediates.push(out);\n\n  for (const i of intermediates) {\n    backend.disposeIntermediateTensorInfo(i);\n  }\n\n  return outReshaped;\n}","map":{"version":3,"names":["broadcast_util","upcastType","util","mapActivationToShaderProgram","MatMulPackedProgram","multiply","reshape","sum","transpose","MATMUL_SHARED_DIM_THRESHOLD","batchMatMulImpl","a","b","transposeA","transposeB","backend","bias","preluActivationWeights","leakyreluAlpha","activation","aRank","shape","length","bRank","innerShapeA","innerShapeB","outerShapeA","outerShapeB","outerDimsA","slice","outerDimsB","batchDimA","sizeFromShape","batchDimB","outShapeOuterDims","assertAndGetBroadcastShape","outShape","concat","assert","a3dShape","b3dShape","a3d","inputs","x","attrs","b3d","intermediates","batchDim","Math","max","sharedDim","hasBias","hasPreluActivationWeights","hasLeakyreluAlpha","fusedActivation","containsFusedOps","out","aVec","bVec","perm","push","shouldReshapeA","shouldReshapeB","aVec3d","axis","bVec3d","product","keepDims","dtype","program","$leakyreluAlpha","makeTensorInfo","createScalarValue","runWebGLProgram","outReshaped","i","disposeIntermediateTensorInfo"],"sources":["C:/Users/Anke/Documents/Feelix documents/Feelix2.0-dev/Feelix v2/node_modules/@tensorflow/tfjs-backend-webgl/dist/kernels/BatchMatMul_impl.js"],"sourcesContent":["/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { broadcast_util, upcastType, util } from '@tensorflow/tfjs-core';\nimport { mapActivationToShaderProgram } from '../kernel_utils/kernel_funcs_utils';\nimport { MatMulPackedProgram } from '../mulmat_packed_gpu';\nimport { multiply } from './Multiply';\nimport { reshape } from './Reshape';\nimport { sum } from './Sum';\nimport { transpose } from './Transpose';\n// Empirically determined minimal shared dimension in matmul before we forward\n// to a.mul(b).sum() in order to take advantage of GPU parallelism. See\n// https://github.com/tensorflow/tfjs-core/pull/1379 for benchmarks.\nexport const MATMUL_SHARED_DIM_THRESHOLD = 1000;\nexport function batchMatMulImpl({ a, b, transposeA, transposeB, backend, bias = null, preluActivationWeights = null, leakyreluAlpha = 0, activation = null }) {\n    const aRank = a.shape.length;\n    const bRank = b.shape.length;\n    const innerShapeA = transposeA ? a.shape[aRank - 2] : a.shape[aRank - 1];\n    const innerShapeB = transposeB ? b.shape[bRank - 1] : b.shape[bRank - 2];\n    const outerShapeA = transposeA ? a.shape[aRank - 1] : a.shape[aRank - 2];\n    const outerShapeB = transposeB ? b.shape[bRank - 2] : b.shape[bRank - 1];\n    const outerDimsA = a.shape.slice(0, -2);\n    const outerDimsB = b.shape.slice(0, -2);\n    const batchDimA = util.sizeFromShape(outerDimsA);\n    const batchDimB = util.sizeFromShape(outerDimsB);\n    const outShapeOuterDims = broadcast_util.assertAndGetBroadcastShape(a.shape.slice(0, -2), b.shape.slice(0, -2));\n    const outShape = outShapeOuterDims.concat([outerShapeA, outerShapeB]);\n    util.assert(innerShapeA === innerShapeB, () => `Error in matMul: inner shapes (${innerShapeA}) and (` +\n        `${innerShapeB}) of Tensors with shapes ${a.shape} and ` +\n        `${b.shape} and transposeA=${transposeA}` +\n        ` and transposeB=${transposeB} must match.`);\n    const a3dShape = transposeA ?\n        [batchDimA, innerShapeA, outerShapeA] :\n        [batchDimA, outerShapeA, innerShapeA];\n    const b3dShape = transposeB ?\n        [batchDimB, outerShapeB, innerShapeB] :\n        [batchDimB, innerShapeB, outerShapeB];\n    // The rest of the implementation is designed to operate on rank-3 tensors\n    const a3d = reshape({ inputs: { x: a }, backend, attrs: { shape: a3dShape } });\n    const b3d = reshape({ inputs: { x: b }, backend, attrs: { shape: b3dShape } });\n    const intermediates = [a3d, b3d];\n    const batchDim = Math.max(batchDimA, batchDimB);\n    const sharedDim = transposeA ? a3d.shape[1] : a3d.shape[2];\n    const hasBias = bias != null;\n    const hasPreluActivationWeights = preluActivationWeights != null;\n    const hasLeakyreluAlpha = activation === 'leakyrelu';\n    const fusedActivation = activation != null ?\n        mapActivationToShaderProgram(activation, true) :\n        null;\n    const containsFusedOps = hasBias || hasPreluActivationWeights ||\n        hasLeakyreluAlpha || fusedActivation != null;\n    let out;\n    // Since the matrices are vectors, it is faster to call mul().sum()\n    // because sum() is O(sqrt(N)) due to divide-and-conquer.\n    if ((outerShapeA === 1 || outerShapeB === 1) &&\n        sharedDim > MATMUL_SHARED_DIM_THRESHOLD && containsFusedOps === false) {\n        let aVec = a3d;\n        let bVec = b3d;\n        if (transposeA) {\n            aVec = transpose({ inputs: { x: a3d }, backend, attrs: { perm: [0, 2, 1] } });\n            intermediates.push(aVec);\n        }\n        if (transposeB) {\n            bVec = transpose({ inputs: { x: b3d }, backend, attrs: { perm: [0, 2, 1] } });\n            intermediates.push(bVec);\n        }\n        const shouldReshapeA = outerShapeB !== 1;\n        const shouldReshapeB = outerShapeB === 1;\n        let aVec3d = aVec;\n        if (shouldReshapeA) {\n            aVec3d = reshape({\n                inputs: { x: aVec },\n                backend,\n                attrs: { shape: [batchDim, sharedDim, 1] }\n            });\n            intermediates.push(aVec3d);\n        }\n        const axis = outerShapeB === 1 ? 2 : 1;\n        let bVec3d = bVec;\n        if (shouldReshapeB) {\n            bVec3d = reshape({\n                inputs: { x: bVec },\n                backend,\n                attrs: { shape: [batchDim, 1, sharedDim] }\n            });\n            intermediates.push(bVec3d);\n        }\n        const product = multiply({ inputs: { a: aVec3d, b: bVec3d }, backend });\n        out = sum({ inputs: { x: product }, backend, attrs: { axis, keepDims: true } });\n        intermediates.push(product);\n    }\n    else {\n        const dtype = upcastType(a.dtype, b.dtype);\n        const program = new MatMulPackedProgram(a3dShape, b3dShape, [batchDim, outerShapeA, outerShapeB], transposeA, transposeB, hasBias, fusedActivation, hasPreluActivationWeights, hasLeakyreluAlpha);\n        const inputs = [a3d, b3d];\n        if (bias != null) {\n            inputs.push(bias);\n        }\n        if (hasPreluActivationWeights) {\n            inputs.push(preluActivationWeights);\n        }\n        if (hasLeakyreluAlpha) {\n            const $leakyreluAlpha = backend.makeTensorInfo([], 'float32', util.createScalarValue(leakyreluAlpha, 'float32'));\n            inputs.push($leakyreluAlpha);\n            intermediates.push($leakyreluAlpha);\n        }\n        out = backend.runWebGLProgram(program, inputs, dtype);\n    }\n    const outReshaped = reshape({ inputs: { x: out }, backend, attrs: { shape: outShape } });\n    intermediates.push(out);\n    for (const i of intermediates) {\n        backend.disposeIntermediateTensorInfo(i);\n    }\n    return outReshaped;\n}\n"],"mappings":"AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAASA,cAAT,EAAyBC,UAAzB,EAAqCC,IAArC,QAAiD,uBAAjD;AACA,SAASC,4BAAT,QAA6C,oCAA7C;AACA,SAASC,mBAAT,QAAoC,sBAApC;AACA,SAASC,QAAT,QAAyB,YAAzB;AACA,SAASC,OAAT,QAAwB,WAAxB;AACA,SAASC,GAAT,QAAoB,OAApB;AACA,SAASC,SAAT,QAA0B,aAA1B,C,CACA;AACA;AACA;;AACA,OAAO,MAAMC,2BAA2B,GAAG,IAApC;AACP,OAAO,SAASC,eAAT,CAAyB;EAAEC,CAAF;EAAKC,CAAL;EAAQC,UAAR;EAAoBC,UAApB;EAAgCC,OAAhC;EAAyCC,IAAI,GAAG,IAAhD;EAAsDC,sBAAsB,GAAG,IAA/E;EAAqFC,cAAc,GAAG,CAAtG;EAAyGC,UAAU,GAAG;AAAtH,CAAzB,EAAuJ;EAC1J,MAAMC,KAAK,GAAGT,CAAC,CAACU,KAAF,CAAQC,MAAtB;EACA,MAAMC,KAAK,GAAGX,CAAC,CAACS,KAAF,CAAQC,MAAtB;EACA,MAAME,WAAW,GAAGX,UAAU,GAAGF,CAAC,CAACU,KAAF,CAAQD,KAAK,GAAG,CAAhB,CAAH,GAAwBT,CAAC,CAACU,KAAF,CAAQD,KAAK,GAAG,CAAhB,CAAtD;EACA,MAAMK,WAAW,GAAGX,UAAU,GAAGF,CAAC,CAACS,KAAF,CAAQE,KAAK,GAAG,CAAhB,CAAH,GAAwBX,CAAC,CAACS,KAAF,CAAQE,KAAK,GAAG,CAAhB,CAAtD;EACA,MAAMG,WAAW,GAAGb,UAAU,GAAGF,CAAC,CAACU,KAAF,CAAQD,KAAK,GAAG,CAAhB,CAAH,GAAwBT,CAAC,CAACU,KAAF,CAAQD,KAAK,GAAG,CAAhB,CAAtD;EACA,MAAMO,WAAW,GAAGb,UAAU,GAAGF,CAAC,CAACS,KAAF,CAAQE,KAAK,GAAG,CAAhB,CAAH,GAAwBX,CAAC,CAACS,KAAF,CAAQE,KAAK,GAAG,CAAhB,CAAtD;EACA,MAAMK,UAAU,GAAGjB,CAAC,CAACU,KAAF,CAAQQ,KAAR,CAAc,CAAd,EAAiB,CAAC,CAAlB,CAAnB;EACA,MAAMC,UAAU,GAAGlB,CAAC,CAACS,KAAF,CAAQQ,KAAR,CAAc,CAAd,EAAiB,CAAC,CAAlB,CAAnB;EACA,MAAME,SAAS,GAAG7B,IAAI,CAAC8B,aAAL,CAAmBJ,UAAnB,CAAlB;EACA,MAAMK,SAAS,GAAG/B,IAAI,CAAC8B,aAAL,CAAmBF,UAAnB,CAAlB;EACA,MAAMI,iBAAiB,GAAGlC,cAAc,CAACmC,0BAAf,CAA0CxB,CAAC,CAACU,KAAF,CAAQQ,KAAR,CAAc,CAAd,EAAiB,CAAC,CAAlB,CAA1C,EAAgEjB,CAAC,CAACS,KAAF,CAAQQ,KAAR,CAAc,CAAd,EAAiB,CAAC,CAAlB,CAAhE,CAA1B;EACA,MAAMO,QAAQ,GAAGF,iBAAiB,CAACG,MAAlB,CAAyB,CAACX,WAAD,EAAcC,WAAd,CAAzB,CAAjB;EACAzB,IAAI,CAACoC,MAAL,CAAYd,WAAW,KAAKC,WAA5B,EAAyC,MAAO,kCAAiCD,WAAY,SAA9C,GAC1C,GAAEC,WAAY,4BAA2Bd,CAAC,CAACU,KAAM,OADP,GAE1C,GAAET,CAAC,CAACS,KAAM,mBAAkBR,UAAW,EAFG,GAG1C,mBAAkBC,UAAW,cAHlC;EAIA,MAAMyB,QAAQ,GAAG1B,UAAU,GACvB,CAACkB,SAAD,EAAYP,WAAZ,EAAyBE,WAAzB,CADuB,GAEvB,CAACK,SAAD,EAAYL,WAAZ,EAAyBF,WAAzB,CAFJ;EAGA,MAAMgB,QAAQ,GAAG1B,UAAU,GACvB,CAACmB,SAAD,EAAYN,WAAZ,EAAyBF,WAAzB,CADuB,GAEvB,CAACQ,SAAD,EAAYR,WAAZ,EAAyBE,WAAzB,CAFJ,CApB0J,CAuB1J;;EACA,MAAMc,GAAG,GAAGnC,OAAO,CAAC;IAAEoC,MAAM,EAAE;MAAEC,CAAC,EAAEhC;IAAL,CAAV;IAAoBI,OAApB;IAA6B6B,KAAK,EAAE;MAAEvB,KAAK,EAAEkB;IAAT;EAApC,CAAD,CAAnB;EACA,MAAMM,GAAG,GAAGvC,OAAO,CAAC;IAAEoC,MAAM,EAAE;MAAEC,CAAC,EAAE/B;IAAL,CAAV;IAAoBG,OAApB;IAA6B6B,KAAK,EAAE;MAAEvB,KAAK,EAAEmB;IAAT;EAApC,CAAD,CAAnB;EACA,MAAMM,aAAa,GAAG,CAACL,GAAD,EAAMI,GAAN,CAAtB;EACA,MAAME,QAAQ,GAAGC,IAAI,CAACC,GAAL,CAASlB,SAAT,EAAoBE,SAApB,CAAjB;EACA,MAAMiB,SAAS,GAAGrC,UAAU,GAAG4B,GAAG,CAACpB,KAAJ,CAAU,CAAV,CAAH,GAAkBoB,GAAG,CAACpB,KAAJ,CAAU,CAAV,CAA9C;EACA,MAAM8B,OAAO,GAAGnC,IAAI,IAAI,IAAxB;EACA,MAAMoC,yBAAyB,GAAGnC,sBAAsB,IAAI,IAA5D;EACA,MAAMoC,iBAAiB,GAAGlC,UAAU,KAAK,WAAzC;EACA,MAAMmC,eAAe,GAAGnC,UAAU,IAAI,IAAd,GACpBhB,4BAA4B,CAACgB,UAAD,EAAa,IAAb,CADR,GAEpB,IAFJ;EAGA,MAAMoC,gBAAgB,GAAGJ,OAAO,IAAIC,yBAAX,IACrBC,iBADqB,IACAC,eAAe,IAAI,IAD5C;EAEA,IAAIE,GAAJ,CArC0J,CAsC1J;EACA;;EACA,IAAI,CAAC9B,WAAW,KAAK,CAAhB,IAAqBC,WAAW,KAAK,CAAtC,KACAuB,SAAS,GAAGzC,2BADZ,IAC2C8C,gBAAgB,KAAK,KADpE,EAC2E;IACvE,IAAIE,IAAI,GAAGhB,GAAX;IACA,IAAIiB,IAAI,GAAGb,GAAX;;IACA,IAAIhC,UAAJ,EAAgB;MACZ4C,IAAI,GAAGjD,SAAS,CAAC;QAAEkC,MAAM,EAAE;UAAEC,CAAC,EAAEF;QAAL,CAAV;QAAsB1B,OAAtB;QAA+B6B,KAAK,EAAE;UAAEe,IAAI,EAAE,CAAC,CAAD,EAAI,CAAJ,EAAO,CAAP;QAAR;MAAtC,CAAD,CAAhB;MACAb,aAAa,CAACc,IAAd,CAAmBH,IAAnB;IACH;;IACD,IAAI3C,UAAJ,EAAgB;MACZ4C,IAAI,GAAGlD,SAAS,CAAC;QAAEkC,MAAM,EAAE;UAAEC,CAAC,EAAEE;QAAL,CAAV;QAAsB9B,OAAtB;QAA+B6B,KAAK,EAAE;UAAEe,IAAI,EAAE,CAAC,CAAD,EAAI,CAAJ,EAAO,CAAP;QAAR;MAAtC,CAAD,CAAhB;MACAb,aAAa,CAACc,IAAd,CAAmBF,IAAnB;IACH;;IACD,MAAMG,cAAc,GAAGlC,WAAW,KAAK,CAAvC;IACA,MAAMmC,cAAc,GAAGnC,WAAW,KAAK,CAAvC;IACA,IAAIoC,MAAM,GAAGN,IAAb;;IACA,IAAII,cAAJ,EAAoB;MAChBE,MAAM,GAAGzD,OAAO,CAAC;QACboC,MAAM,EAAE;UAAEC,CAAC,EAAEc;QAAL,CADK;QAEb1C,OAFa;QAGb6B,KAAK,EAAE;UAAEvB,KAAK,EAAE,CAAC0B,QAAD,EAAWG,SAAX,EAAsB,CAAtB;QAAT;MAHM,CAAD,CAAhB;MAKAJ,aAAa,CAACc,IAAd,CAAmBG,MAAnB;IACH;;IACD,MAAMC,IAAI,GAAGrC,WAAW,KAAK,CAAhB,GAAoB,CAApB,GAAwB,CAArC;IACA,IAAIsC,MAAM,GAAGP,IAAb;;IACA,IAAII,cAAJ,EAAoB;MAChBG,MAAM,GAAG3D,OAAO,CAAC;QACboC,MAAM,EAAE;UAAEC,CAAC,EAAEe;QAAL,CADK;QAEb3C,OAFa;QAGb6B,KAAK,EAAE;UAAEvB,KAAK,EAAE,CAAC0B,QAAD,EAAW,CAAX,EAAcG,SAAd;QAAT;MAHM,CAAD,CAAhB;MAKAJ,aAAa,CAACc,IAAd,CAAmBK,MAAnB;IACH;;IACD,MAAMC,OAAO,GAAG7D,QAAQ,CAAC;MAAEqC,MAAM,EAAE;QAAE/B,CAAC,EAAEoD,MAAL;QAAanD,CAAC,EAAEqD;MAAhB,CAAV;MAAoClD;IAApC,CAAD,CAAxB;IACAyC,GAAG,GAAGjD,GAAG,CAAC;MAAEmC,MAAM,EAAE;QAAEC,CAAC,EAAEuB;MAAL,CAAV;MAA0BnD,OAA1B;MAAmC6B,KAAK,EAAE;QAAEoB,IAAF;QAAQG,QAAQ,EAAE;MAAlB;IAA1C,CAAD,CAAT;IACArB,aAAa,CAACc,IAAd,CAAmBM,OAAnB;EACH,CApCD,MAqCK;IACD,MAAME,KAAK,GAAGnE,UAAU,CAACU,CAAC,CAACyD,KAAH,EAAUxD,CAAC,CAACwD,KAAZ,CAAxB;IACA,MAAMC,OAAO,GAAG,IAAIjE,mBAAJ,CAAwBmC,QAAxB,EAAkCC,QAAlC,EAA4C,CAACO,QAAD,EAAWrB,WAAX,EAAwBC,WAAxB,CAA5C,EAAkFd,UAAlF,EAA8FC,UAA9F,EAA0GqC,OAA1G,EAAmHG,eAAnH,EAAoIF,yBAApI,EAA+JC,iBAA/J,CAAhB;IACA,MAAMX,MAAM,GAAG,CAACD,GAAD,EAAMI,GAAN,CAAf;;IACA,IAAI7B,IAAI,IAAI,IAAZ,EAAkB;MACd0B,MAAM,CAACkB,IAAP,CAAY5C,IAAZ;IACH;;IACD,IAAIoC,yBAAJ,EAA+B;MAC3BV,MAAM,CAACkB,IAAP,CAAY3C,sBAAZ;IACH;;IACD,IAAIoC,iBAAJ,EAAuB;MACnB,MAAMiB,eAAe,GAAGvD,OAAO,CAACwD,cAAR,CAAuB,EAAvB,EAA2B,SAA3B,EAAsCrE,IAAI,CAACsE,iBAAL,CAAuBtD,cAAvB,EAAuC,SAAvC,CAAtC,CAAxB;MACAwB,MAAM,CAACkB,IAAP,CAAYU,eAAZ;MACAxB,aAAa,CAACc,IAAd,CAAmBU,eAAnB;IACH;;IACDd,GAAG,GAAGzC,OAAO,CAAC0D,eAAR,CAAwBJ,OAAxB,EAAiC3B,MAAjC,EAAyC0B,KAAzC,CAAN;EACH;;EACD,MAAMM,WAAW,GAAGpE,OAAO,CAAC;IAAEoC,MAAM,EAAE;MAAEC,CAAC,EAAEa;IAAL,CAAV;IAAsBzC,OAAtB;IAA+B6B,KAAK,EAAE;MAAEvB,KAAK,EAAEe;IAAT;EAAtC,CAAD,CAA3B;EACAU,aAAa,CAACc,IAAd,CAAmBJ,GAAnB;;EACA,KAAK,MAAMmB,CAAX,IAAgB7B,aAAhB,EAA+B;IAC3B/B,OAAO,CAAC6D,6BAAR,CAAsCD,CAAtC;EACH;;EACD,OAAOD,WAAP;AACH"},"metadata":{},"sourceType":"module"}