{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n *  Advanced activation layers.\n */\nimport { cast, clipByValue, elu, greater, leakyRelu, mul, prelu, relu, serialization } from '@tensorflow/tfjs-core';\nimport { Softmax as softmaxActivation } from '../activations';\nimport { getConstraint, serializeConstraint } from '../constraints';\nimport { InputSpec, Layer } from '../engine/topology';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { getInitializer, serializeInitializer } from '../initializers';\nimport { getRegularizer, serializeRegularizer } from '../regularizers';\nimport { getExactlyOneShape, getExactlyOneTensor } from '../utils/types_utils';\nexport class ReLU extends Layer {\n  constructor(args) {\n    super(args == null ? {} : args);\n    this.supportsMasking = true;\n\n    if (args != null) {\n      this.maxValue = args.maxValue;\n    }\n  }\n\n  call(inputs, kwargs) {\n    inputs = getExactlyOneTensor(inputs);\n    let output = relu(inputs);\n\n    if (this.maxValue != null) {\n      output = clipByValue(output, 0, this.maxValue);\n    }\n\n    return output;\n  }\n\n  computeOutputShape(inputShape) {\n    return inputShape;\n  }\n\n  getConfig() {\n    const config = {\n      maxValue: this.maxValue\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n}\n/** @nocollapse */\n\nReLU.className = 'ReLU';\nserialization.registerClass(ReLU);\nexport class LeakyReLU extends Layer {\n  constructor(args) {\n    super(args == null ? {} : args);\n    this.DEFAULT_ALPHA = 0.3;\n\n    if (args == null) {\n      args = {};\n    }\n\n    this.alpha = args.alpha == null ? this.DEFAULT_ALPHA : args.alpha;\n  }\n\n  call(inputs, kwargs) {\n    const x = getExactlyOneTensor(inputs);\n    return leakyRelu(x, this.alpha);\n  }\n\n  computeOutputShape(inputShape) {\n    return inputShape;\n  }\n\n  getConfig() {\n    const config = {\n      alpha: this.alpha\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n}\n/** @nocollapse */\n\nLeakyReLU.className = 'LeakyReLU';\nserialization.registerClass(LeakyReLU);\nexport class PReLU extends Layer {\n  constructor(args) {\n    super(args == null ? {} : args);\n    this.DEFAULT_ALPHA_INITIALIZER = 'zeros';\n\n    if (args == null) {\n      args = {};\n    }\n\n    this.supportsMasking = true;\n    this.alphaInitializer = getInitializer(args.alphaInitializer || this.DEFAULT_ALPHA_INITIALIZER);\n    this.alphaRegularizer = getRegularizer(args.alphaRegularizer);\n    this.alphaConstraint = getConstraint(args.alphaConstraint);\n\n    if (args.sharedAxes == null) {\n      this.sharedAxes = null;\n    } else if (Array.isArray(args.sharedAxes)) {\n      this.sharedAxes = args.sharedAxes;\n    } else if (typeof args.sharedAxes === 'number') {\n      this.sharedAxes = [args.sharedAxes];\n    } else {\n      throw new ValueError(`Expected sharedAxes to be a number or an array of numbers, ` + `but got ${args.sharedAxes}`);\n    }\n  }\n\n  build(inputShape) {\n    inputShape = getExactlyOneShape(inputShape);\n    const paramShape = inputShape.slice(1);\n\n    if (this.sharedAxes != null) {\n      for (const i of this.sharedAxes) {\n        paramShape[i - 1] = 1;\n      }\n    }\n\n    this.alpha = this.addWeight('alpha', paramShape, 'float32', this.alphaInitializer, this.alphaRegularizer, true, this.alphaConstraint); // Set input spec.\n\n    const axes = {};\n\n    if (this.sharedAxes != null) {\n      for (let i = 1; i < inputShape.length; ++i) {\n        axes[i] = inputShape[i];\n      }\n    }\n\n    this.inputSpec = [new InputSpec({\n      ndim: inputShape.length,\n      axes\n    })];\n    this.built = true;\n  }\n\n  call(inputs, kwargs) {\n    inputs = getExactlyOneTensor(inputs);\n    return prelu(inputs, this.alpha.read());\n  }\n\n  getConfig() {\n    const config = {\n      alphaInitializer: serializeInitializer(this.alphaInitializer),\n      alphaRegularizer: serializeRegularizer(this.alphaRegularizer),\n      alphaConstraint: serializeConstraint(this.alphaConstraint),\n      sharedAxes: this.sharedAxes\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n}\n/** @nocollapse */\n\nPReLU.className = 'PReLU';\nserialization.registerClass(PReLU);\nexport class ELU extends Layer {\n  constructor(args) {\n    super(args == null ? {} : args);\n    this.DEFAULT_ALPHA = 1.0;\n\n    if (args == null) {\n      args = {};\n    }\n\n    if (args.alpha != null && args.alpha !== this.DEFAULT_ALPHA) {\n      throw new NotImplementedError(`Non-default alpha value (${args.alpha}) is not supported by the ` + `ELU layer yet.`);\n    }\n\n    this.alpha = args.alpha == null ? this.DEFAULT_ALPHA : args.alpha;\n  }\n\n  call(inputs, kwargs) {\n    const x = getExactlyOneTensor(inputs);\n    return elu(x);\n  }\n\n  computeOutputShape(inputShape) {\n    return inputShape;\n  }\n\n  getConfig() {\n    const config = {\n      alpha: this.alpha\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n}\n/** @nocollapse */\n\nELU.className = 'ELU';\nserialization.registerClass(ELU);\nexport class ThresholdedReLU extends Layer {\n  constructor(args) {\n    super(args == null ? {} : args);\n    this.DEFAULT_THETA = 1.0;\n\n    if (args == null) {\n      args = {};\n    }\n\n    this.theta = args.theta == null ? this.DEFAULT_THETA : args.theta;\n  }\n\n  call(inputs, kwargs) {\n    const x = getExactlyOneTensor(inputs);\n    return mul(x, cast(greater(x, this.theta), 'float32'));\n  }\n\n  computeOutputShape(inputShape) {\n    return inputShape;\n  }\n\n  getConfig() {\n    const config = {\n      theta: this.theta\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n}\n/** @nocollapse */\n\nThresholdedReLU.className = 'ThresholdedReLU';\nserialization.registerClass(ThresholdedReLU);\nexport class Softmax extends Layer {\n  constructor(args) {\n    super(args == null ? {} : args);\n    this.DEFAULT_AXIS = 1.0;\n\n    if (args == null) {\n      args = {};\n    }\n\n    this.softmax = new softmaxActivation().apply;\n    this.axis = args.axis == null ? this.DEFAULT_AXIS : args.axis;\n  }\n\n  call(inputs, kwargs) {\n    const x = getExactlyOneTensor(inputs);\n    return this.softmax(x, this.axis);\n  }\n\n  computeOutputShape(inputShape) {\n    return inputShape;\n  }\n\n  getConfig() {\n    const config = {\n      axis: this.axis\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n}\n/** @nocollapse */\n\nSoftmax.className = 'Softmax';\nserialization.registerClass(Softmax);","map":{"version":3,"names":["cast","clipByValue","elu","greater","leakyRelu","mul","prelu","relu","serialization","Softmax","softmaxActivation","getConstraint","serializeConstraint","InputSpec","Layer","NotImplementedError","ValueError","getInitializer","serializeInitializer","getRegularizer","serializeRegularizer","getExactlyOneShape","getExactlyOneTensor","ReLU","constructor","args","supportsMasking","maxValue","call","inputs","kwargs","output","computeOutputShape","inputShape","getConfig","config","baseConfig","Object","assign","className","registerClass","LeakyReLU","DEFAULT_ALPHA","alpha","x","PReLU","DEFAULT_ALPHA_INITIALIZER","alphaInitializer","alphaRegularizer","alphaConstraint","sharedAxes","Array","isArray","build","paramShape","slice","i","addWeight","axes","length","inputSpec","ndim","built","read","ELU","ThresholdedReLU","DEFAULT_THETA","theta","DEFAULT_AXIS","softmax","apply","axis"],"sources":["C:/Users/Anke/Documents/Feelix documents/Feelix2.0-dev/Feelix v2/node_modules/@tensorflow/tfjs-layers/dist/layers/advanced_activations.js"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n *  Advanced activation layers.\n */\nimport { cast, clipByValue, elu, greater, leakyRelu, mul, prelu, relu, serialization } from '@tensorflow/tfjs-core';\nimport { Softmax as softmaxActivation } from '../activations';\nimport { getConstraint, serializeConstraint } from '../constraints';\nimport { InputSpec, Layer } from '../engine/topology';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { getInitializer, serializeInitializer } from '../initializers';\nimport { getRegularizer, serializeRegularizer } from '../regularizers';\nimport { getExactlyOneShape, getExactlyOneTensor } from '../utils/types_utils';\nexport class ReLU extends Layer {\n    constructor(args) {\n        super(args == null ? {} : args);\n        this.supportsMasking = true;\n        if (args != null) {\n            this.maxValue = args.maxValue;\n        }\n    }\n    call(inputs, kwargs) {\n        inputs = getExactlyOneTensor(inputs);\n        let output = relu(inputs);\n        if (this.maxValue != null) {\n            output = clipByValue(output, 0, this.maxValue);\n        }\n        return output;\n    }\n    computeOutputShape(inputShape) {\n        return inputShape;\n    }\n    getConfig() {\n        const config = { maxValue: this.maxValue };\n        const baseConfig = super.getConfig();\n        Object.assign(config, baseConfig);\n        return config;\n    }\n}\n/** @nocollapse */\nReLU.className = 'ReLU';\nserialization.registerClass(ReLU);\nexport class LeakyReLU extends Layer {\n    constructor(args) {\n        super(args == null ? {} : args);\n        this.DEFAULT_ALPHA = 0.3;\n        if (args == null) {\n            args = {};\n        }\n        this.alpha = args.alpha == null ? this.DEFAULT_ALPHA : args.alpha;\n    }\n    call(inputs, kwargs) {\n        const x = getExactlyOneTensor(inputs);\n        return leakyRelu(x, this.alpha);\n    }\n    computeOutputShape(inputShape) {\n        return inputShape;\n    }\n    getConfig() {\n        const config = { alpha: this.alpha };\n        const baseConfig = super.getConfig();\n        Object.assign(config, baseConfig);\n        return config;\n    }\n}\n/** @nocollapse */\nLeakyReLU.className = 'LeakyReLU';\nserialization.registerClass(LeakyReLU);\nexport class PReLU extends Layer {\n    constructor(args) {\n        super(args == null ? {} : args);\n        this.DEFAULT_ALPHA_INITIALIZER = 'zeros';\n        if (args == null) {\n            args = {};\n        }\n        this.supportsMasking = true;\n        this.alphaInitializer =\n            getInitializer(args.alphaInitializer || this.DEFAULT_ALPHA_INITIALIZER);\n        this.alphaRegularizer = getRegularizer(args.alphaRegularizer);\n        this.alphaConstraint = getConstraint(args.alphaConstraint);\n        if (args.sharedAxes == null) {\n            this.sharedAxes = null;\n        }\n        else if (Array.isArray(args.sharedAxes)) {\n            this.sharedAxes = args.sharedAxes;\n        }\n        else if (typeof args.sharedAxes === 'number') {\n            this.sharedAxes = [args.sharedAxes];\n        }\n        else {\n            throw new ValueError(`Expected sharedAxes to be a number or an array of numbers, ` +\n                `but got ${args.sharedAxes}`);\n        }\n    }\n    build(inputShape) {\n        inputShape = getExactlyOneShape(inputShape);\n        const paramShape = inputShape.slice(1);\n        if (this.sharedAxes != null) {\n            for (const i of this.sharedAxes) {\n                paramShape[i - 1] = 1;\n            }\n        }\n        this.alpha = this.addWeight('alpha', paramShape, 'float32', this.alphaInitializer, this.alphaRegularizer, true, this.alphaConstraint);\n        // Set input spec.\n        const axes = {};\n        if (this.sharedAxes != null) {\n            for (let i = 1; i < inputShape.length; ++i) {\n                axes[i] = inputShape[i];\n            }\n        }\n        this.inputSpec = [new InputSpec({\n                ndim: inputShape.length,\n                axes,\n            })];\n        this.built = true;\n    }\n    call(inputs, kwargs) {\n        inputs = getExactlyOneTensor(inputs);\n        return prelu(inputs, this.alpha.read());\n    }\n    getConfig() {\n        const config = {\n            alphaInitializer: serializeInitializer(this.alphaInitializer),\n            alphaRegularizer: serializeRegularizer(this.alphaRegularizer),\n            alphaConstraint: serializeConstraint(this.alphaConstraint),\n            sharedAxes: this.sharedAxes\n        };\n        const baseConfig = super.getConfig();\n        Object.assign(config, baseConfig);\n        return config;\n    }\n}\n/** @nocollapse */\nPReLU.className = 'PReLU';\nserialization.registerClass(PReLU);\nexport class ELU extends Layer {\n    constructor(args) {\n        super(args == null ? {} : args);\n        this.DEFAULT_ALPHA = 1.0;\n        if (args == null) {\n            args = {};\n        }\n        if (args.alpha != null && args.alpha !== this.DEFAULT_ALPHA) {\n            throw new NotImplementedError(`Non-default alpha value (${args.alpha}) is not supported by the ` +\n                `ELU layer yet.`);\n        }\n        this.alpha = args.alpha == null ? this.DEFAULT_ALPHA : args.alpha;\n    }\n    call(inputs, kwargs) {\n        const x = getExactlyOneTensor(inputs);\n        return elu(x);\n    }\n    computeOutputShape(inputShape) {\n        return inputShape;\n    }\n    getConfig() {\n        const config = { alpha: this.alpha };\n        const baseConfig = super.getConfig();\n        Object.assign(config, baseConfig);\n        return config;\n    }\n}\n/** @nocollapse */\nELU.className = 'ELU';\nserialization.registerClass(ELU);\nexport class ThresholdedReLU extends Layer {\n    constructor(args) {\n        super(args == null ? {} : args);\n        this.DEFAULT_THETA = 1.0;\n        if (args == null) {\n            args = {};\n        }\n        this.theta = args.theta == null ? this.DEFAULT_THETA : args.theta;\n    }\n    call(inputs, kwargs) {\n        const x = getExactlyOneTensor(inputs);\n        return mul(x, cast(greater(x, this.theta), 'float32'));\n    }\n    computeOutputShape(inputShape) {\n        return inputShape;\n    }\n    getConfig() {\n        const config = { theta: this.theta };\n        const baseConfig = super.getConfig();\n        Object.assign(config, baseConfig);\n        return config;\n    }\n}\n/** @nocollapse */\nThresholdedReLU.className = 'ThresholdedReLU';\nserialization.registerClass(ThresholdedReLU);\nexport class Softmax extends Layer {\n    constructor(args) {\n        super(args == null ? {} : args);\n        this.DEFAULT_AXIS = 1.0;\n        if (args == null) {\n            args = {};\n        }\n        this.softmax = new softmaxActivation().apply;\n        this.axis = args.axis == null ? this.DEFAULT_AXIS : args.axis;\n    }\n    call(inputs, kwargs) {\n        const x = getExactlyOneTensor(inputs);\n        return this.softmax(x, this.axis);\n    }\n    computeOutputShape(inputShape) {\n        return inputShape;\n    }\n    getConfig() {\n        const config = { axis: this.axis };\n        const baseConfig = super.getConfig();\n        Object.assign(config, baseConfig);\n        return config;\n    }\n}\n/** @nocollapse */\nSoftmax.className = 'Softmax';\nserialization.registerClass(Softmax);\n"],"mappings":"AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AACA;AACA;AACA;AACA,SAASA,IAAT,EAAeC,WAAf,EAA4BC,GAA5B,EAAiCC,OAAjC,EAA0CC,SAA1C,EAAqDC,GAArD,EAA0DC,KAA1D,EAAiEC,IAAjE,EAAuEC,aAAvE,QAA4F,uBAA5F;AACA,SAASC,OAAO,IAAIC,iBAApB,QAA6C,gBAA7C;AACA,SAASC,aAAT,EAAwBC,mBAAxB,QAAmD,gBAAnD;AACA,SAASC,SAAT,EAAoBC,KAApB,QAAiC,oBAAjC;AACA,SAASC,mBAAT,EAA8BC,UAA9B,QAAgD,WAAhD;AACA,SAASC,cAAT,EAAyBC,oBAAzB,QAAqD,iBAArD;AACA,SAASC,cAAT,EAAyBC,oBAAzB,QAAqD,iBAArD;AACA,SAASC,kBAAT,EAA6BC,mBAA7B,QAAwD,sBAAxD;AACA,OAAO,MAAMC,IAAN,SAAmBT,KAAnB,CAAyB;EAC5BU,WAAW,CAACC,IAAD,EAAO;IACd,MAAMA,IAAI,IAAI,IAAR,GAAe,EAAf,GAAoBA,IAA1B;IACA,KAAKC,eAAL,GAAuB,IAAvB;;IACA,IAAID,IAAI,IAAI,IAAZ,EAAkB;MACd,KAAKE,QAAL,GAAgBF,IAAI,CAACE,QAArB;IACH;EACJ;;EACDC,IAAI,CAACC,MAAD,EAASC,MAAT,EAAiB;IACjBD,MAAM,GAAGP,mBAAmB,CAACO,MAAD,CAA5B;IACA,IAAIE,MAAM,GAAGxB,IAAI,CAACsB,MAAD,CAAjB;;IACA,IAAI,KAAKF,QAAL,IAAiB,IAArB,EAA2B;MACvBI,MAAM,GAAG9B,WAAW,CAAC8B,MAAD,EAAS,CAAT,EAAY,KAAKJ,QAAjB,CAApB;IACH;;IACD,OAAOI,MAAP;EACH;;EACDC,kBAAkB,CAACC,UAAD,EAAa;IAC3B,OAAOA,UAAP;EACH;;EACDC,SAAS,GAAG;IACR,MAAMC,MAAM,GAAG;MAAER,QAAQ,EAAE,KAAKA;IAAjB,CAAf;IACA,MAAMS,UAAU,GAAG,MAAMF,SAAN,EAAnB;IACAG,MAAM,CAACC,MAAP,CAAcH,MAAd,EAAsBC,UAAtB;IACA,OAAOD,MAAP;EACH;;AAxB2B;AA0BhC;;AACAZ,IAAI,CAACgB,SAAL,GAAiB,MAAjB;AACA/B,aAAa,CAACgC,aAAd,CAA4BjB,IAA5B;AACA,OAAO,MAAMkB,SAAN,SAAwB3B,KAAxB,CAA8B;EACjCU,WAAW,CAACC,IAAD,EAAO;IACd,MAAMA,IAAI,IAAI,IAAR,GAAe,EAAf,GAAoBA,IAA1B;IACA,KAAKiB,aAAL,GAAqB,GAArB;;IACA,IAAIjB,IAAI,IAAI,IAAZ,EAAkB;MACdA,IAAI,GAAG,EAAP;IACH;;IACD,KAAKkB,KAAL,GAAalB,IAAI,CAACkB,KAAL,IAAc,IAAd,GAAqB,KAAKD,aAA1B,GAA0CjB,IAAI,CAACkB,KAA5D;EACH;;EACDf,IAAI,CAACC,MAAD,EAASC,MAAT,EAAiB;IACjB,MAAMc,CAAC,GAAGtB,mBAAmB,CAACO,MAAD,CAA7B;IACA,OAAOzB,SAAS,CAACwC,CAAD,EAAI,KAAKD,KAAT,CAAhB;EACH;;EACDX,kBAAkB,CAACC,UAAD,EAAa;IAC3B,OAAOA,UAAP;EACH;;EACDC,SAAS,GAAG;IACR,MAAMC,MAAM,GAAG;MAAEQ,KAAK,EAAE,KAAKA;IAAd,CAAf;IACA,MAAMP,UAAU,GAAG,MAAMF,SAAN,EAAnB;IACAG,MAAM,CAACC,MAAP,CAAcH,MAAd,EAAsBC,UAAtB;IACA,OAAOD,MAAP;EACH;;AArBgC;AAuBrC;;AACAM,SAAS,CAACF,SAAV,GAAsB,WAAtB;AACA/B,aAAa,CAACgC,aAAd,CAA4BC,SAA5B;AACA,OAAO,MAAMI,KAAN,SAAoB/B,KAApB,CAA0B;EAC7BU,WAAW,CAACC,IAAD,EAAO;IACd,MAAMA,IAAI,IAAI,IAAR,GAAe,EAAf,GAAoBA,IAA1B;IACA,KAAKqB,yBAAL,GAAiC,OAAjC;;IACA,IAAIrB,IAAI,IAAI,IAAZ,EAAkB;MACdA,IAAI,GAAG,EAAP;IACH;;IACD,KAAKC,eAAL,GAAuB,IAAvB;IACA,KAAKqB,gBAAL,GACI9B,cAAc,CAACQ,IAAI,CAACsB,gBAAL,IAAyB,KAAKD,yBAA/B,CADlB;IAEA,KAAKE,gBAAL,GAAwB7B,cAAc,CAACM,IAAI,CAACuB,gBAAN,CAAtC;IACA,KAAKC,eAAL,GAAuBtC,aAAa,CAACc,IAAI,CAACwB,eAAN,CAApC;;IACA,IAAIxB,IAAI,CAACyB,UAAL,IAAmB,IAAvB,EAA6B;MACzB,KAAKA,UAAL,GAAkB,IAAlB;IACH,CAFD,MAGK,IAAIC,KAAK,CAACC,OAAN,CAAc3B,IAAI,CAACyB,UAAnB,CAAJ,EAAoC;MACrC,KAAKA,UAAL,GAAkBzB,IAAI,CAACyB,UAAvB;IACH,CAFI,MAGA,IAAI,OAAOzB,IAAI,CAACyB,UAAZ,KAA2B,QAA/B,EAAyC;MAC1C,KAAKA,UAAL,GAAkB,CAACzB,IAAI,CAACyB,UAAN,CAAlB;IACH,CAFI,MAGA;MACD,MAAM,IAAIlC,UAAJ,CAAgB,6DAAD,GAChB,WAAUS,IAAI,CAACyB,UAAW,EADzB,CAAN;IAEH;EACJ;;EACDG,KAAK,CAACpB,UAAD,EAAa;IACdA,UAAU,GAAGZ,kBAAkB,CAACY,UAAD,CAA/B;IACA,MAAMqB,UAAU,GAAGrB,UAAU,CAACsB,KAAX,CAAiB,CAAjB,CAAnB;;IACA,IAAI,KAAKL,UAAL,IAAmB,IAAvB,EAA6B;MACzB,KAAK,MAAMM,CAAX,IAAgB,KAAKN,UAArB,EAAiC;QAC7BI,UAAU,CAACE,CAAC,GAAG,CAAL,CAAV,GAAoB,CAApB;MACH;IACJ;;IACD,KAAKb,KAAL,GAAa,KAAKc,SAAL,CAAe,OAAf,EAAwBH,UAAxB,EAAoC,SAApC,EAA+C,KAAKP,gBAApD,EAAsE,KAAKC,gBAA3E,EAA6F,IAA7F,EAAmG,KAAKC,eAAxG,CAAb,CARc,CASd;;IACA,MAAMS,IAAI,GAAG,EAAb;;IACA,IAAI,KAAKR,UAAL,IAAmB,IAAvB,EAA6B;MACzB,KAAK,IAAIM,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGvB,UAAU,CAAC0B,MAA/B,EAAuC,EAAEH,CAAzC,EAA4C;QACxCE,IAAI,CAACF,CAAD,CAAJ,GAAUvB,UAAU,CAACuB,CAAD,CAApB;MACH;IACJ;;IACD,KAAKI,SAAL,GAAiB,CAAC,IAAI/C,SAAJ,CAAc;MACxBgD,IAAI,EAAE5B,UAAU,CAAC0B,MADO;MAExBD;IAFwB,CAAd,CAAD,CAAjB;IAIA,KAAKI,KAAL,GAAa,IAAb;EACH;;EACDlC,IAAI,CAACC,MAAD,EAASC,MAAT,EAAiB;IACjBD,MAAM,GAAGP,mBAAmB,CAACO,MAAD,CAA5B;IACA,OAAOvB,KAAK,CAACuB,MAAD,EAAS,KAAKc,KAAL,CAAWoB,IAAX,EAAT,CAAZ;EACH;;EACD7B,SAAS,GAAG;IACR,MAAMC,MAAM,GAAG;MACXY,gBAAgB,EAAE7B,oBAAoB,CAAC,KAAK6B,gBAAN,CAD3B;MAEXC,gBAAgB,EAAE5B,oBAAoB,CAAC,KAAK4B,gBAAN,CAF3B;MAGXC,eAAe,EAAErC,mBAAmB,CAAC,KAAKqC,eAAN,CAHzB;MAIXC,UAAU,EAAE,KAAKA;IAJN,CAAf;IAMA,MAAMd,UAAU,GAAG,MAAMF,SAAN,EAAnB;IACAG,MAAM,CAACC,MAAP,CAAcH,MAAd,EAAsBC,UAAtB;IACA,OAAOD,MAAP;EACH;;AA9D4B;AAgEjC;;AACAU,KAAK,CAACN,SAAN,GAAkB,OAAlB;AACA/B,aAAa,CAACgC,aAAd,CAA4BK,KAA5B;AACA,OAAO,MAAMmB,GAAN,SAAkBlD,KAAlB,CAAwB;EAC3BU,WAAW,CAACC,IAAD,EAAO;IACd,MAAMA,IAAI,IAAI,IAAR,GAAe,EAAf,GAAoBA,IAA1B;IACA,KAAKiB,aAAL,GAAqB,GAArB;;IACA,IAAIjB,IAAI,IAAI,IAAZ,EAAkB;MACdA,IAAI,GAAG,EAAP;IACH;;IACD,IAAIA,IAAI,CAACkB,KAAL,IAAc,IAAd,IAAsBlB,IAAI,CAACkB,KAAL,KAAe,KAAKD,aAA9C,EAA6D;MACzD,MAAM,IAAI3B,mBAAJ,CAAyB,4BAA2BU,IAAI,CAACkB,KAAM,4BAAvC,GACzB,gBADC,CAAN;IAEH;;IACD,KAAKA,KAAL,GAAalB,IAAI,CAACkB,KAAL,IAAc,IAAd,GAAqB,KAAKD,aAA1B,GAA0CjB,IAAI,CAACkB,KAA5D;EACH;;EACDf,IAAI,CAACC,MAAD,EAASC,MAAT,EAAiB;IACjB,MAAMc,CAAC,GAAGtB,mBAAmB,CAACO,MAAD,CAA7B;IACA,OAAO3B,GAAG,CAAC0C,CAAD,CAAV;EACH;;EACDZ,kBAAkB,CAACC,UAAD,EAAa;IAC3B,OAAOA,UAAP;EACH;;EACDC,SAAS,GAAG;IACR,MAAMC,MAAM,GAAG;MAAEQ,KAAK,EAAE,KAAKA;IAAd,CAAf;IACA,MAAMP,UAAU,GAAG,MAAMF,SAAN,EAAnB;IACAG,MAAM,CAACC,MAAP,CAAcH,MAAd,EAAsBC,UAAtB;IACA,OAAOD,MAAP;EACH;;AAzB0B;AA2B/B;;AACA6B,GAAG,CAACzB,SAAJ,GAAgB,KAAhB;AACA/B,aAAa,CAACgC,aAAd,CAA4BwB,GAA5B;AACA,OAAO,MAAMC,eAAN,SAA8BnD,KAA9B,CAAoC;EACvCU,WAAW,CAACC,IAAD,EAAO;IACd,MAAMA,IAAI,IAAI,IAAR,GAAe,EAAf,GAAoBA,IAA1B;IACA,KAAKyC,aAAL,GAAqB,GAArB;;IACA,IAAIzC,IAAI,IAAI,IAAZ,EAAkB;MACdA,IAAI,GAAG,EAAP;IACH;;IACD,KAAK0C,KAAL,GAAa1C,IAAI,CAAC0C,KAAL,IAAc,IAAd,GAAqB,KAAKD,aAA1B,GAA0CzC,IAAI,CAAC0C,KAA5D;EACH;;EACDvC,IAAI,CAACC,MAAD,EAASC,MAAT,EAAiB;IACjB,MAAMc,CAAC,GAAGtB,mBAAmB,CAACO,MAAD,CAA7B;IACA,OAAOxB,GAAG,CAACuC,CAAD,EAAI5C,IAAI,CAACG,OAAO,CAACyC,CAAD,EAAI,KAAKuB,KAAT,CAAR,EAAyB,SAAzB,CAAR,CAAV;EACH;;EACDnC,kBAAkB,CAACC,UAAD,EAAa;IAC3B,OAAOA,UAAP;EACH;;EACDC,SAAS,GAAG;IACR,MAAMC,MAAM,GAAG;MAAEgC,KAAK,EAAE,KAAKA;IAAd,CAAf;IACA,MAAM/B,UAAU,GAAG,MAAMF,SAAN,EAAnB;IACAG,MAAM,CAACC,MAAP,CAAcH,MAAd,EAAsBC,UAAtB;IACA,OAAOD,MAAP;EACH;;AArBsC;AAuB3C;;AACA8B,eAAe,CAAC1B,SAAhB,GAA4B,iBAA5B;AACA/B,aAAa,CAACgC,aAAd,CAA4ByB,eAA5B;AACA,OAAO,MAAMxD,OAAN,SAAsBK,KAAtB,CAA4B;EAC/BU,WAAW,CAACC,IAAD,EAAO;IACd,MAAMA,IAAI,IAAI,IAAR,GAAe,EAAf,GAAoBA,IAA1B;IACA,KAAK2C,YAAL,GAAoB,GAApB;;IACA,IAAI3C,IAAI,IAAI,IAAZ,EAAkB;MACdA,IAAI,GAAG,EAAP;IACH;;IACD,KAAK4C,OAAL,GAAe,IAAI3D,iBAAJ,GAAwB4D,KAAvC;IACA,KAAKC,IAAL,GAAY9C,IAAI,CAAC8C,IAAL,IAAa,IAAb,GAAoB,KAAKH,YAAzB,GAAwC3C,IAAI,CAAC8C,IAAzD;EACH;;EACD3C,IAAI,CAACC,MAAD,EAASC,MAAT,EAAiB;IACjB,MAAMc,CAAC,GAAGtB,mBAAmB,CAACO,MAAD,CAA7B;IACA,OAAO,KAAKwC,OAAL,CAAazB,CAAb,EAAgB,KAAK2B,IAArB,CAAP;EACH;;EACDvC,kBAAkB,CAACC,UAAD,EAAa;IAC3B,OAAOA,UAAP;EACH;;EACDC,SAAS,GAAG;IACR,MAAMC,MAAM,GAAG;MAAEoC,IAAI,EAAE,KAAKA;IAAb,CAAf;IACA,MAAMnC,UAAU,GAAG,MAAMF,SAAN,EAAnB;IACAG,MAAM,CAACC,MAAP,CAAcH,MAAd,EAAsBC,UAAtB;IACA,OAAOD,MAAP;EACH;;AAtB8B;AAwBnC;;AACA1B,OAAO,CAAC8B,SAAR,GAAoB,SAApB;AACA/B,aAAa,CAACgC,aAAd,CAA4B/B,OAA5B"},"metadata":{},"sourceType":"module"}