{"ast":null,"code":"/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport * as broadcast_util from './broadcast_util';\nimport { elu } from './elu';\nimport { leakyRelu } from './leaky_relu';\nimport { mul } from './mul';\nimport { prelu } from './prelu';\nimport { relu } from './relu';\nimport { relu6 } from './relu6';\nimport { reshape } from './reshape';\nimport { sigmoid } from './sigmoid';\nimport { step } from './step';\nimport { sum } from './sum'; // Returns gradient for fused activation.\n\nexport function getFusedDyActivation(dy, y, activation) {\n  if (activation == null || activation === 'linear') {\n    return dy;\n  }\n\n  if (activation === 'relu') {\n    return mul(dy, step(y));\n  }\n\n  throw new Error(`Cannot compute gradient for fused activation ${activation}.`);\n} // Returns gradient for fused bias.\n\nexport function getFusedBiasGradient(bias, dyActivation) {\n  let res = dyActivation;\n  const reduceAxes = broadcast_util.getReductionAxes(bias.shape, dyActivation.shape);\n\n  if (reduceAxes.length > 0) {\n    res = sum(res, reduceAxes);\n  }\n\n  return reshape(res, bias.shape);\n}\nexport function applyActivation(x, activation, preluActivationWeights, leakyreluAlpha) {\n  if (activation === 'linear') {\n    return x;\n  } else if (activation === 'relu') {\n    return relu(x);\n  } else if (activation === 'elu') {\n    return elu(x);\n  } else if (activation === 'relu6') {\n    return relu6(x);\n  } else if (activation === 'prelu') {\n    return prelu(x, preluActivationWeights);\n  } else if (activation === 'leakyrelu') {\n    return leakyRelu(x, leakyreluAlpha);\n  } else if (activation === 'sigmoid') {\n    return sigmoid(x);\n  }\n\n  throw new Error(`Unknown fused activation ${activation}.`);\n} // Whether we should call fused ops.\n\nexport const shouldFuse = (gradientDepth, activation) => {\n  const gradientMode = gradientDepth > 0;\n  return !gradientMode || activation === 'linear';\n};","map":{"version":3,"names":["broadcast_util","elu","leakyRelu","mul","prelu","relu","relu6","reshape","sigmoid","step","sum","getFusedDyActivation","dy","y","activation","Error","getFusedBiasGradient","bias","dyActivation","res","reduceAxes","getReductionAxes","shape","length","applyActivation","x","preluActivationWeights","leakyreluAlpha","shouldFuse","gradientDepth","gradientMode"],"sources":["C:/Users/Anke/Documents/Feelix documents/Feelix2.0-dev/Feelix v2/node_modules/@tensorflow/tfjs-core/dist/ops/fused_util.js"],"sourcesContent":["/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport * as broadcast_util from './broadcast_util';\nimport { elu } from './elu';\nimport { leakyRelu } from './leaky_relu';\nimport { mul } from './mul';\nimport { prelu } from './prelu';\nimport { relu } from './relu';\nimport { relu6 } from './relu6';\nimport { reshape } from './reshape';\nimport { sigmoid } from './sigmoid';\nimport { step } from './step';\nimport { sum } from './sum';\n// Returns gradient for fused activation.\nexport function getFusedDyActivation(dy, y, activation) {\n    if (activation == null || activation === 'linear') {\n        return dy;\n    }\n    if (activation === 'relu') {\n        return mul(dy, step(y));\n    }\n    throw new Error(`Cannot compute gradient for fused activation ${activation}.`);\n}\n// Returns gradient for fused bias.\nexport function getFusedBiasGradient(bias, dyActivation) {\n    let res = dyActivation;\n    const reduceAxes = broadcast_util.getReductionAxes(bias.shape, dyActivation.shape);\n    if (reduceAxes.length > 0) {\n        res = sum(res, reduceAxes);\n    }\n    return reshape(res, bias.shape);\n}\nexport function applyActivation(x, activation, preluActivationWeights, leakyreluAlpha) {\n    if (activation === 'linear') {\n        return x;\n    }\n    else if (activation === 'relu') {\n        return relu(x);\n    }\n    else if (activation === 'elu') {\n        return elu(x);\n    }\n    else if (activation === 'relu6') {\n        return relu6(x);\n    }\n    else if (activation === 'prelu') {\n        return prelu(x, preluActivationWeights);\n    }\n    else if (activation === 'leakyrelu') {\n        return leakyRelu(x, leakyreluAlpha);\n    }\n    else if (activation === 'sigmoid') {\n        return sigmoid(x);\n    }\n    throw new Error(`Unknown fused activation ${activation}.`);\n}\n// Whether we should call fused ops.\nexport const shouldFuse = (gradientDepth, activation) => {\n    const gradientMode = gradientDepth > 0;\n    return !gradientMode || activation === 'linear';\n};\n"],"mappings":"AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,KAAKA,cAAZ,MAAgC,kBAAhC;AACA,SAASC,GAAT,QAAoB,OAApB;AACA,SAASC,SAAT,QAA0B,cAA1B;AACA,SAASC,GAAT,QAAoB,OAApB;AACA,SAASC,KAAT,QAAsB,SAAtB;AACA,SAASC,IAAT,QAAqB,QAArB;AACA,SAASC,KAAT,QAAsB,SAAtB;AACA,SAASC,OAAT,QAAwB,WAAxB;AACA,SAASC,OAAT,QAAwB,WAAxB;AACA,SAASC,IAAT,QAAqB,QAArB;AACA,SAASC,GAAT,QAAoB,OAApB,C,CACA;;AACA,OAAO,SAASC,oBAAT,CAA8BC,EAA9B,EAAkCC,CAAlC,EAAqCC,UAArC,EAAiD;EACpD,IAAIA,UAAU,IAAI,IAAd,IAAsBA,UAAU,KAAK,QAAzC,EAAmD;IAC/C,OAAOF,EAAP;EACH;;EACD,IAAIE,UAAU,KAAK,MAAnB,EAA2B;IACvB,OAAOX,GAAG,CAACS,EAAD,EAAKH,IAAI,CAACI,CAAD,CAAT,CAAV;EACH;;EACD,MAAM,IAAIE,KAAJ,CAAW,gDAA+CD,UAAW,GAArE,CAAN;AACH,C,CACD;;AACA,OAAO,SAASE,oBAAT,CAA8BC,IAA9B,EAAoCC,YAApC,EAAkD;EACrD,IAAIC,GAAG,GAAGD,YAAV;EACA,MAAME,UAAU,GAAGpB,cAAc,CAACqB,gBAAf,CAAgCJ,IAAI,CAACK,KAArC,EAA4CJ,YAAY,CAACI,KAAzD,CAAnB;;EACA,IAAIF,UAAU,CAACG,MAAX,GAAoB,CAAxB,EAA2B;IACvBJ,GAAG,GAAGT,GAAG,CAACS,GAAD,EAAMC,UAAN,CAAT;EACH;;EACD,OAAOb,OAAO,CAACY,GAAD,EAAMF,IAAI,CAACK,KAAX,CAAd;AACH;AACD,OAAO,SAASE,eAAT,CAAyBC,CAAzB,EAA4BX,UAA5B,EAAwCY,sBAAxC,EAAgEC,cAAhE,EAAgF;EACnF,IAAIb,UAAU,KAAK,QAAnB,EAA6B;IACzB,OAAOW,CAAP;EACH,CAFD,MAGK,IAAIX,UAAU,KAAK,MAAnB,EAA2B;IAC5B,OAAOT,IAAI,CAACoB,CAAD,CAAX;EACH,CAFI,MAGA,IAAIX,UAAU,KAAK,KAAnB,EAA0B;IAC3B,OAAOb,GAAG,CAACwB,CAAD,CAAV;EACH,CAFI,MAGA,IAAIX,UAAU,KAAK,OAAnB,EAA4B;IAC7B,OAAOR,KAAK,CAACmB,CAAD,CAAZ;EACH,CAFI,MAGA,IAAIX,UAAU,KAAK,OAAnB,EAA4B;IAC7B,OAAOV,KAAK,CAACqB,CAAD,EAAIC,sBAAJ,CAAZ;EACH,CAFI,MAGA,IAAIZ,UAAU,KAAK,WAAnB,EAAgC;IACjC,OAAOZ,SAAS,CAACuB,CAAD,EAAIE,cAAJ,CAAhB;EACH,CAFI,MAGA,IAAIb,UAAU,KAAK,SAAnB,EAA8B;IAC/B,OAAON,OAAO,CAACiB,CAAD,CAAd;EACH;;EACD,MAAM,IAAIV,KAAJ,CAAW,4BAA2BD,UAAW,GAAjD,CAAN;AACH,C,CACD;;AACA,OAAO,MAAMc,UAAU,GAAG,CAACC,aAAD,EAAgBf,UAAhB,KAA+B;EACrD,MAAMgB,YAAY,GAAGD,aAAa,GAAG,CAArC;EACA,OAAO,CAACC,YAAD,IAAiBhB,UAAU,KAAK,QAAvC;AACH,CAHM"},"metadata":{},"sourceType":"module"}