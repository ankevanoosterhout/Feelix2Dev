{"ast":null,"code":"import _asyncToGenerator from \"C:/Users/Anke/Documents/Feelix documents/Feelix2.0-dev/Feelix v2/node_modules/@babel/runtime/helpers/esm/asyncToGenerator.js\";\n\n/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { dispose, tidy } from '../globals';\nimport { add } from '../ops/add';\nimport { div } from '../ops/div';\nimport { fill } from '../ops/fill';\nimport { mul } from '../ops/mul';\nimport { sqrt } from '../ops/sqrt';\nimport { square } from '../ops/square';\nimport { registerClass } from '../serialization';\nimport { Optimizer } from './optimizer';\n/** @doclink Optimizer */\n\nexport class AdagradOptimizer extends Optimizer {\n  constructor(learningRate, initialAccumulatorValue = 0.1) {\n    super();\n    this.learningRate = learningRate;\n    this.initialAccumulatorValue = initialAccumulatorValue;\n    this.accumulatedGrads = [];\n  }\n\n  applyGradients(variableGradients) {\n    const variableNames = Array.isArray(variableGradients) ? variableGradients.map(item => item.name) : Object.keys(variableGradients);\n    variableNames.forEach((name, i) => {\n      const value = ENGINE.registeredVariables[name];\n\n      if (this.accumulatedGrads[i] == null) {\n        const trainable = false;\n        this.accumulatedGrads[i] = {\n          originalName: `${name}/accumulator`,\n          variable: tidy(() => fill(value.shape, this.initialAccumulatorValue).variable(trainable))\n        };\n      }\n\n      const gradient = Array.isArray(variableGradients) ? variableGradients[i].tensor : variableGradients[name];\n\n      if (gradient == null) {\n        return;\n      }\n\n      const accumulatedGrad = this.accumulatedGrads[i].variable;\n      tidy(() => {\n        const newAccumulatedGrad = add(accumulatedGrad, square(gradient));\n        accumulatedGrad.assign(newAccumulatedGrad);\n        const newValue = add(mul(div(gradient, sqrt(add(newAccumulatedGrad, ENGINE.backend.epsilon()))), -this.learningRate), value);\n        value.assign(newValue);\n      });\n    });\n    this.incrementIterations();\n  }\n\n  dispose() {\n    if (this.accumulatedGrads != null) {\n      dispose(this.accumulatedGrads.map(v => v.variable));\n    }\n  }\n\n  getWeights() {\n    var _this = this;\n\n    return _asyncToGenerator(function* () {\n      // Order matters for Python compatibility.\n      return [yield _this.saveIterations()].concat(_this.accumulatedGrads.map(v => ({\n        name: v.originalName,\n        tensor: v.variable\n      })));\n    })();\n  }\n\n  setWeights(weightValues) {\n    var _this2 = this;\n\n    return _asyncToGenerator(function* () {\n      weightValues = yield _this2.extractIterations(weightValues);\n      const trainable = false;\n      _this2.accumulatedGrads = weightValues.map(v => ({\n        originalName: v.name,\n        variable: v.tensor.variable(trainable)\n      }));\n    })();\n  }\n\n  getConfig() {\n    return {\n      'learningRate': this.learningRate,\n      'initialAccumulatorValue': this.initialAccumulatorValue\n    };\n  }\n  /** @nocollapse */\n\n\n  static fromConfig(cls, config) {\n    return new cls(config['learningRate'], config['initialAccumulatorValue']);\n  }\n\n}\n/** @nocollapse */\n\nAdagradOptimizer.className = 'Adagrad'; // Note: Name matters for Python compatibility.\n\nregisterClass(AdagradOptimizer);","map":{"version":3,"names":["ENGINE","dispose","tidy","add","div","fill","mul","sqrt","square","registerClass","Optimizer","AdagradOptimizer","constructor","learningRate","initialAccumulatorValue","accumulatedGrads","applyGradients","variableGradients","variableNames","Array","isArray","map","item","name","Object","keys","forEach","i","value","registeredVariables","trainable","originalName","variable","shape","gradient","tensor","accumulatedGrad","newAccumulatedGrad","assign","newValue","backend","epsilon","incrementIterations","v","getWeights","saveIterations","concat","setWeights","weightValues","extractIterations","getConfig","fromConfig","cls","config","className"],"sources":["C:/Users/Anke/Documents/Feelix documents/Feelix2.0-dev/Feelix v2/node_modules/@tensorflow/tfjs-core/dist/optimizers/adagrad_optimizer.js"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { dispose, tidy } from '../globals';\nimport { add } from '../ops/add';\nimport { div } from '../ops/div';\nimport { fill } from '../ops/fill';\nimport { mul } from '../ops/mul';\nimport { sqrt } from '../ops/sqrt';\nimport { square } from '../ops/square';\nimport { registerClass } from '../serialization';\nimport { Optimizer } from './optimizer';\n/** @doclink Optimizer */\nexport class AdagradOptimizer extends Optimizer {\n    constructor(learningRate, initialAccumulatorValue = 0.1) {\n        super();\n        this.learningRate = learningRate;\n        this.initialAccumulatorValue = initialAccumulatorValue;\n        this.accumulatedGrads = [];\n    }\n    applyGradients(variableGradients) {\n        const variableNames = Array.isArray(variableGradients) ?\n            variableGradients.map(item => item.name) :\n            Object.keys(variableGradients);\n        variableNames.forEach((name, i) => {\n            const value = ENGINE.registeredVariables[name];\n            if (this.accumulatedGrads[i] == null) {\n                const trainable = false;\n                this.accumulatedGrads[i] = {\n                    originalName: `${name}/accumulator`,\n                    variable: tidy(() => fill(value.shape, this.initialAccumulatorValue)\n                        .variable(trainable))\n                };\n            }\n            const gradient = Array.isArray(variableGradients) ?\n                variableGradients[i].tensor :\n                variableGradients[name];\n            if (gradient == null) {\n                return;\n            }\n            const accumulatedGrad = this.accumulatedGrads[i].variable;\n            tidy(() => {\n                const newAccumulatedGrad = add(accumulatedGrad, square(gradient));\n                accumulatedGrad.assign(newAccumulatedGrad);\n                const newValue = add(mul(div(gradient, sqrt(add(newAccumulatedGrad, ENGINE.backend.epsilon()))), -this.learningRate), value);\n                value.assign(newValue);\n            });\n        });\n        this.incrementIterations();\n    }\n    dispose() {\n        if (this.accumulatedGrads != null) {\n            dispose(this.accumulatedGrads.map(v => v.variable));\n        }\n    }\n    async getWeights() {\n        // Order matters for Python compatibility.\n        return [await this.saveIterations()].concat(this.accumulatedGrads.map(v => ({ name: v.originalName, tensor: v.variable })));\n    }\n    async setWeights(weightValues) {\n        weightValues = await this.extractIterations(weightValues);\n        const trainable = false;\n        this.accumulatedGrads = weightValues.map(v => ({ originalName: v.name, variable: v.tensor.variable(trainable) }));\n    }\n    getConfig() {\n        return {\n            'learningRate': this.learningRate,\n            'initialAccumulatorValue': this.initialAccumulatorValue,\n        };\n    }\n    /** @nocollapse */\n    static fromConfig(cls, config) {\n        return new cls(config['learningRate'], config['initialAccumulatorValue']);\n    }\n}\n/** @nocollapse */\nAdagradOptimizer.className = 'Adagrad'; // Note: Name matters for Python compatibility.\nregisterClass(AdagradOptimizer);\n"],"mappings":";;AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAASA,MAAT,QAAuB,WAAvB;AACA,SAASC,OAAT,EAAkBC,IAAlB,QAA8B,YAA9B;AACA,SAASC,GAAT,QAAoB,YAApB;AACA,SAASC,GAAT,QAAoB,YAApB;AACA,SAASC,IAAT,QAAqB,aAArB;AACA,SAASC,GAAT,QAAoB,YAApB;AACA,SAASC,IAAT,QAAqB,aAArB;AACA,SAASC,MAAT,QAAuB,eAAvB;AACA,SAASC,aAAT,QAA8B,kBAA9B;AACA,SAASC,SAAT,QAA0B,aAA1B;AACA;;AACA,OAAO,MAAMC,gBAAN,SAA+BD,SAA/B,CAAyC;EAC5CE,WAAW,CAACC,YAAD,EAAeC,uBAAuB,GAAG,GAAzC,EAA8C;IACrD;IACA,KAAKD,YAAL,GAAoBA,YAApB;IACA,KAAKC,uBAAL,GAA+BA,uBAA/B;IACA,KAAKC,gBAAL,GAAwB,EAAxB;EACH;;EACDC,cAAc,CAACC,iBAAD,EAAoB;IAC9B,MAAMC,aAAa,GAAGC,KAAK,CAACC,OAAN,CAAcH,iBAAd,IAClBA,iBAAiB,CAACI,GAAlB,CAAsBC,IAAI,IAAIA,IAAI,CAACC,IAAnC,CADkB,GAElBC,MAAM,CAACC,IAAP,CAAYR,iBAAZ,CAFJ;IAGAC,aAAa,CAACQ,OAAd,CAAsB,CAACH,IAAD,EAAOI,CAAP,KAAa;MAC/B,MAAMC,KAAK,GAAG5B,MAAM,CAAC6B,mBAAP,CAA2BN,IAA3B,CAAd;;MACA,IAAI,KAAKR,gBAAL,CAAsBY,CAAtB,KAA4B,IAAhC,EAAsC;QAClC,MAAMG,SAAS,GAAG,KAAlB;QACA,KAAKf,gBAAL,CAAsBY,CAAtB,IAA2B;UACvBI,YAAY,EAAG,GAAER,IAAK,cADC;UAEvBS,QAAQ,EAAE9B,IAAI,CAAC,MAAMG,IAAI,CAACuB,KAAK,CAACK,KAAP,EAAc,KAAKnB,uBAAnB,CAAJ,CAChBkB,QADgB,CACPF,SADO,CAAP;QAFS,CAA3B;MAKH;;MACD,MAAMI,QAAQ,GAAGf,KAAK,CAACC,OAAN,CAAcH,iBAAd,IACbA,iBAAiB,CAACU,CAAD,CAAjB,CAAqBQ,MADR,GAEblB,iBAAiB,CAACM,IAAD,CAFrB;;MAGA,IAAIW,QAAQ,IAAI,IAAhB,EAAsB;QAClB;MACH;;MACD,MAAME,eAAe,GAAG,KAAKrB,gBAAL,CAAsBY,CAAtB,EAAyBK,QAAjD;MACA9B,IAAI,CAAC,MAAM;QACP,MAAMmC,kBAAkB,GAAGlC,GAAG,CAACiC,eAAD,EAAkB5B,MAAM,CAAC0B,QAAD,CAAxB,CAA9B;QACAE,eAAe,CAACE,MAAhB,CAAuBD,kBAAvB;QACA,MAAME,QAAQ,GAAGpC,GAAG,CAACG,GAAG,CAACF,GAAG,CAAC8B,QAAD,EAAW3B,IAAI,CAACJ,GAAG,CAACkC,kBAAD,EAAqBrC,MAAM,CAACwC,OAAP,CAAeC,OAAf,EAArB,CAAJ,CAAf,CAAJ,EAAyE,CAAC,KAAK5B,YAA/E,CAAJ,EAAkGe,KAAlG,CAApB;QACAA,KAAK,CAACU,MAAN,CAAaC,QAAb;MACH,CALG,CAAJ;IAMH,CAvBD;IAwBA,KAAKG,mBAAL;EACH;;EACDzC,OAAO,GAAG;IACN,IAAI,KAAKc,gBAAL,IAAyB,IAA7B,EAAmC;MAC/Bd,OAAO,CAAC,KAAKc,gBAAL,CAAsBM,GAAtB,CAA0BsB,CAAC,IAAIA,CAAC,CAACX,QAAjC,CAAD,CAAP;IACH;EACJ;;EACKY,UAAU,GAAG;IAAA;;IAAA;MACf;MACA,OAAO,OAAO,KAAI,CAACC,cAAL,EAAP,EAA8BC,MAA9B,CAAqC,KAAI,CAAC/B,gBAAL,CAAsBM,GAAtB,CAA0BsB,CAAC,KAAK;QAAEpB,IAAI,EAAEoB,CAAC,CAACZ,YAAV;QAAwBI,MAAM,EAAEQ,CAAC,CAACX;MAAlC,CAAL,CAA3B,CAArC,CAAP;IAFe;EAGlB;;EACKe,UAAU,CAACC,YAAD,EAAe;IAAA;;IAAA;MAC3BA,YAAY,SAAS,MAAI,CAACC,iBAAL,CAAuBD,YAAvB,CAArB;MACA,MAAMlB,SAAS,GAAG,KAAlB;MACA,MAAI,CAACf,gBAAL,GAAwBiC,YAAY,CAAC3B,GAAb,CAAiBsB,CAAC,KAAK;QAAEZ,YAAY,EAAEY,CAAC,CAACpB,IAAlB;QAAwBS,QAAQ,EAAEW,CAAC,CAACR,MAAF,CAASH,QAAT,CAAkBF,SAAlB;MAAlC,CAAL,CAAlB,CAAxB;IAH2B;EAI9B;;EACDoB,SAAS,GAAG;IACR,OAAO;MACH,gBAAgB,KAAKrC,YADlB;MAEH,2BAA2B,KAAKC;IAF7B,CAAP;EAIH;EACD;;;EACiB,OAAVqC,UAAU,CAACC,GAAD,EAAMC,MAAN,EAAc;IAC3B,OAAO,IAAID,GAAJ,CAAQC,MAAM,CAAC,cAAD,CAAd,EAAgCA,MAAM,CAAC,yBAAD,CAAtC,CAAP;EACH;;AA5D2C;AA8DhD;;AACA1C,gBAAgB,CAAC2C,SAAjB,GAA6B,SAA7B,C,CAAwC;;AACxC7C,aAAa,CAACE,gBAAD,CAAb"},"metadata":{},"sourceType":"module"}