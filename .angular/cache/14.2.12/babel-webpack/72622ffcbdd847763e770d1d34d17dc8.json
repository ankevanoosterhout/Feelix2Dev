{"ast":null,"code":"/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { customGrad } from '../../gradients';\nimport { FusedDepthwiseConv2D } from '../../kernel_names';\nimport { makeTypesMatch } from '../../tensor_util';\nimport { convertToTensor } from '../../tensor_util_env';\nimport * as util from '../../util';\nimport { add } from '../add';\nimport * as broadcast_util from '../broadcast_util';\nimport * as conv_util from '../conv_util';\nimport { depthwiseConv2d as unfusedDepthwiseConv2d } from '../depthwise_conv2d';\nimport { depthwiseConv2dNativeBackpropFilter } from '../depthwise_conv2d_native_backprop_filter';\nimport { depthwiseConv2dNativeBackpropInput } from '../depthwise_conv2d_native_backprop_input';\nimport { applyActivation, getFusedBiasGradient, getFusedDyActivation, shouldFuse } from '../fused_util';\nimport { op } from '../operation';\nimport { reshape } from '../reshape';\n/**\n * Computes depthwise 2D convolution, optionally fused with adding a\n * bias and applying an activation.\n *\n * Given a 4D `input` array and a `filter` array of shape\n * `[filterHeight, filterWidth, inChannels, channelMultiplier]` containing\n * `inChannels` convolutional filters of depth 1, this op applies a\n * different filter to each input channel (expanding from 1 channel to\n * `channelMultiplier` channels for each), then concatenates the results\n * together. The output has `inChannels * channelMultiplier` channels.\n *\n * See\n * [https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d](\n *     https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d)\n * for more details.\n *\n * @param obj An object with the following properties:\n * @param x The input tensor, of rank 4 or rank 3, of shape\n *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is\n * assumed.\n * @param filter The filter tensor, rank 4, of shape\n *     `[filterHeight, filterWidth, inChannels, channelMultiplier]`.\n * @param strides The strides of the convolution: `[strideHeight,\n * strideWidth]`. If strides is a single number, then `strideHeight ==\n * strideWidth`.\n * @param pad The type of padding algorithm.\n *   - `same` and stride 1: output will be of same size as input,\n *       regardless of filter size.\n *   - `valid`: output will be smaller than input if filter is larger\n *       than 1x1.\n *   - For more info, see this guide:\n *     [https://www.tensorflow.org/api_docs/python/tf/nn/convolution](\n *          https://www.tensorflow.org/api_docs/python/tf/nn/convolution)\n * @param dilations The dilation rates: `[dilationHeight, dilationWidth]`\n *     in which we sample input values across the height and width dimensions\n *     in atrous convolution. Defaults to `[1, 1]`. If `rate` is a single\n *     number, then `dilationHeight == dilationWidth`. If it is greater than\n *     1, then all values of `strides` must be 1.\n * @param dataFormat: An optional string from: \"NHWC\", \"NCHW\". Defaults to\n *     \"NHWC\". Specify the data format of the input and output data. With the\n *     default format \"NHWC\", the data is stored in the order of: [batch,\n *     height, width, channels]. Only \"NHWC\" is currently supported.\n * @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is\n *     provided, it will default to truncate.\n * @param bias Tensor to be added to the result.\n * @param activation Name of activation kernel (defaults to `linear`).\n * @param preluActivationWeights Tensor of prelu weights to be applied as part\n *     of a `prelu` activation, typically the same shape as `x`.\n * @param leakyreluAlpha Optional. Alpha to be applied as part of a `leakyrelu`\n *     activation.\n */\n\nfunction fusedDepthwiseConv2d_({\n  x,\n  filter,\n  strides,\n  pad,\n  dataFormat = 'NHWC',\n  dilations = [1, 1],\n  dimRoundingMode,\n  bias,\n  activation = 'linear',\n  preluActivationWeights,\n  leakyreluAlpha\n}) {\n  if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {\n    let result = unfusedDepthwiseConv2d(x, filter, strides, pad, dataFormat, dilations, dimRoundingMode);\n\n    if (bias != null) {\n      result = add(result, bias);\n    }\n\n    return applyActivation(result, activation, preluActivationWeights, leakyreluAlpha);\n  }\n\n  const $x = convertToTensor(x, 'x', 'depthwiseConv2d', 'float32');\n  const $filter = convertToTensor(filter, 'filter', 'depthwiseConv2d', 'float32');\n  let x4D = $x;\n  let reshapedTo4D = false;\n\n  if ($x.rank === 3) {\n    reshapedTo4D = true;\n    x4D = reshape($x, [1, $x.shape[0], $x.shape[1], $x.shape[2]]);\n  }\n\n  util.assert(x4D.rank === 4, () => `Error in fused depthwiseConv2d: input must be rank 4, but got ` + `rank ${x4D.rank}.`);\n  util.assert($filter.rank === 4, () => `Error in fused depthwiseConv2d: filter must be rank 4, ` + `but got rank ${$filter.rank}.`);\n  util.assert(x4D.shape[3] === $filter.shape[2], () => `Error in fused depthwiseConv2d: number of input channels ` + `(${x4D.shape[3]}) must match the inChannels dimension in ` + `filter ${$filter.shape[2]}.`);\n\n  if (dilations == null) {\n    dilations = [1, 1];\n  }\n\n  util.assert(conv_util.eitherStridesOrDilationsAreOne(strides, dilations), () => 'Error in fused depthwiseConv2d: Either strides or dilations must ' + `be 1. Got strides ${strides} and dilations '${dilations}'`);\n  conv_util.checkPadOnDimRoundingMode('fused depthwiseConv2d', pad, dimRoundingMode);\n  const convInfo = conv_util.computeConv2DInfo(x4D.shape, $filter.shape, strides, dilations, pad, dimRoundingMode, true\n  /* depthwise */\n  );\n  let $bias;\n\n  if (bias != null) {\n    $bias = convertToTensor(bias, 'bias', 'fused conv2d');\n    [$bias] = makeTypesMatch($bias, $x);\n    broadcast_util.assertAndGetBroadcastShape(convInfo.outShape, $bias.shape);\n  }\n\n  let $preluActivationWeights;\n\n  if (preluActivationWeights != null) {\n    $preluActivationWeights = convertToTensor(preluActivationWeights, 'prelu weights', 'fused depthwiseConv2d');\n  }\n\n  const grad = (dy, saved) => {\n    util.assert(conv_util.tupleValuesAreOne(dilations), () => 'Error in gradient of fused depthwiseConv2d: dilation rates ' + `greater than 1 are not yet supported. Got dilations ` + `'${dilations}'`);\n    const [$filter, x4D, y, bias] = saved;\n    const dyActivation = getFusedDyActivation(dy, y, activation);\n    const xDer = depthwiseConv2dNativeBackpropInput(x4D.shape, dyActivation, $filter, strides, pad, dilations, dimRoundingMode);\n    const filterDer = depthwiseConv2dNativeBackpropFilter(x4D, dyActivation, $filter.shape, strides, pad, dilations, dimRoundingMode);\n\n    if (bias != null) {\n      const biasDer = getFusedBiasGradient($bias, dyActivation);\n      return [xDer, filterDer, biasDer];\n    }\n\n    return [xDer, filterDer];\n  };\n\n  const inputs = {\n    x: x4D,\n    filter: $filter,\n    bias: $bias,\n    preluActivationWeights: $preluActivationWeights\n  };\n  const attrs = {\n    strides,\n    pad,\n    dataFormat,\n    dilations,\n    dimRoundingMode,\n    activation,\n    leakyreluAlpha\n  }; // Depending on the the params passed in we will have different number of\n  // inputs and thus a a different number of elements in the gradient.\n\n  if (bias == null) {\n    const customOp = customGrad((x4D, filter, save) => {\n      // tslint:disable-next-line: no-unnecessary-type-assertion\n      let res = ENGINE.runKernel(FusedDepthwiseConv2D, inputs, attrs);\n      save([filter, x4D, res]);\n\n      if (reshapedTo4D) {\n        // tslint:disable-next-line: no-unnecessary-type-assertion\n        res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n      }\n\n      return {\n        value: res,\n        gradFunc: grad\n      };\n    });\n    return customOp(x4D, $filter);\n  } else {\n    const customOpWithBias = customGrad((x4D, filter, bias, save) => {\n      // tslint:disable-next-line: no-unnecessary-type-assertion\n      let res = ENGINE.runKernel(FusedDepthwiseConv2D, inputs, attrs);\n      save([filter, x4D, res, bias]);\n\n      if (reshapedTo4D) {\n        // tslint:disable-next-line: no-unnecessary-type-assertion\n        res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n      }\n\n      return {\n        value: res,\n        gradFunc: grad\n      };\n    });\n    return customOpWithBias(x4D, $filter, $bias);\n  }\n}\n\nexport const depthwiseConv2d = op({\n  fusedDepthwiseConv2d_\n});","map":{"version":3,"names":["ENGINE","customGrad","FusedDepthwiseConv2D","makeTypesMatch","convertToTensor","util","add","broadcast_util","conv_util","depthwiseConv2d","unfusedDepthwiseConv2d","depthwiseConv2dNativeBackpropFilter","depthwiseConv2dNativeBackpropInput","applyActivation","getFusedBiasGradient","getFusedDyActivation","shouldFuse","op","reshape","fusedDepthwiseConv2d_","x","filter","strides","pad","dataFormat","dilations","dimRoundingMode","bias","activation","preluActivationWeights","leakyreluAlpha","state","gradientDepth","result","$x","$filter","x4D","reshapedTo4D","rank","shape","assert","eitherStridesOrDilationsAreOne","checkPadOnDimRoundingMode","convInfo","computeConv2DInfo","$bias","assertAndGetBroadcastShape","outShape","$preluActivationWeights","grad","dy","saved","tupleValuesAreOne","y","dyActivation","xDer","filterDer","biasDer","inputs","attrs","customOp","save","res","runKernel","value","gradFunc","customOpWithBias"],"sources":["C:/Users/Anke/Documents/Feelix documents/Feelix2.0-dev/Feelix v2/node_modules/@tensorflow/tfjs-core/dist/ops/fused/depthwise_conv2d.js"],"sourcesContent":["/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { customGrad } from '../../gradients';\nimport { FusedDepthwiseConv2D } from '../../kernel_names';\nimport { makeTypesMatch } from '../../tensor_util';\nimport { convertToTensor } from '../../tensor_util_env';\nimport * as util from '../../util';\nimport { add } from '../add';\nimport * as broadcast_util from '../broadcast_util';\nimport * as conv_util from '../conv_util';\nimport { depthwiseConv2d as unfusedDepthwiseConv2d } from '../depthwise_conv2d';\nimport { depthwiseConv2dNativeBackpropFilter } from '../depthwise_conv2d_native_backprop_filter';\nimport { depthwiseConv2dNativeBackpropInput } from '../depthwise_conv2d_native_backprop_input';\nimport { applyActivation, getFusedBiasGradient, getFusedDyActivation, shouldFuse } from '../fused_util';\nimport { op } from '../operation';\nimport { reshape } from '../reshape';\n/**\n * Computes depthwise 2D convolution, optionally fused with adding a\n * bias and applying an activation.\n *\n * Given a 4D `input` array and a `filter` array of shape\n * `[filterHeight, filterWidth, inChannels, channelMultiplier]` containing\n * `inChannels` convolutional filters of depth 1, this op applies a\n * different filter to each input channel (expanding from 1 channel to\n * `channelMultiplier` channels for each), then concatenates the results\n * together. The output has `inChannels * channelMultiplier` channels.\n *\n * See\n * [https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d](\n *     https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d)\n * for more details.\n *\n * @param obj An object with the following properties:\n * @param x The input tensor, of rank 4 or rank 3, of shape\n *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is\n * assumed.\n * @param filter The filter tensor, rank 4, of shape\n *     `[filterHeight, filterWidth, inChannels, channelMultiplier]`.\n * @param strides The strides of the convolution: `[strideHeight,\n * strideWidth]`. If strides is a single number, then `strideHeight ==\n * strideWidth`.\n * @param pad The type of padding algorithm.\n *   - `same` and stride 1: output will be of same size as input,\n *       regardless of filter size.\n *   - `valid`: output will be smaller than input if filter is larger\n *       than 1x1.\n *   - For more info, see this guide:\n *     [https://www.tensorflow.org/api_docs/python/tf/nn/convolution](\n *          https://www.tensorflow.org/api_docs/python/tf/nn/convolution)\n * @param dilations The dilation rates: `[dilationHeight, dilationWidth]`\n *     in which we sample input values across the height and width dimensions\n *     in atrous convolution. Defaults to `[1, 1]`. If `rate` is a single\n *     number, then `dilationHeight == dilationWidth`. If it is greater than\n *     1, then all values of `strides` must be 1.\n * @param dataFormat: An optional string from: \"NHWC\", \"NCHW\". Defaults to\n *     \"NHWC\". Specify the data format of the input and output data. With the\n *     default format \"NHWC\", the data is stored in the order of: [batch,\n *     height, width, channels]. Only \"NHWC\" is currently supported.\n * @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is\n *     provided, it will default to truncate.\n * @param bias Tensor to be added to the result.\n * @param activation Name of activation kernel (defaults to `linear`).\n * @param preluActivationWeights Tensor of prelu weights to be applied as part\n *     of a `prelu` activation, typically the same shape as `x`.\n * @param leakyreluAlpha Optional. Alpha to be applied as part of a `leakyrelu`\n *     activation.\n */\nfunction fusedDepthwiseConv2d_({ x, filter, strides, pad, dataFormat = 'NHWC', dilations = [1, 1], dimRoundingMode, bias, activation = 'linear', preluActivationWeights, leakyreluAlpha }) {\n    if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {\n        let result = unfusedDepthwiseConv2d(x, filter, strides, pad, dataFormat, dilations, dimRoundingMode);\n        if (bias != null) {\n            result = add(result, bias);\n        }\n        return applyActivation(result, activation, preluActivationWeights, leakyreluAlpha);\n    }\n    const $x = convertToTensor(x, 'x', 'depthwiseConv2d', 'float32');\n    const $filter = convertToTensor(filter, 'filter', 'depthwiseConv2d', 'float32');\n    let x4D = $x;\n    let reshapedTo4D = false;\n    if ($x.rank === 3) {\n        reshapedTo4D = true;\n        x4D = reshape($x, [1, $x.shape[0], $x.shape[1], $x.shape[2]]);\n    }\n    util.assert(x4D.rank === 4, () => `Error in fused depthwiseConv2d: input must be rank 4, but got ` +\n        `rank ${x4D.rank}.`);\n    util.assert($filter.rank === 4, () => `Error in fused depthwiseConv2d: filter must be rank 4, ` +\n        `but got rank ${$filter.rank}.`);\n    util.assert(x4D.shape[3] === $filter.shape[2], () => `Error in fused depthwiseConv2d: number of input channels ` +\n        `(${x4D.shape[3]}) must match the inChannels dimension in ` +\n        `filter ${$filter.shape[2]}.`);\n    if (dilations == null) {\n        dilations = [1, 1];\n    }\n    util.assert(conv_util.eitherStridesOrDilationsAreOne(strides, dilations), () => 'Error in fused depthwiseConv2d: Either strides or dilations must ' +\n        `be 1. Got strides ${strides} and dilations '${dilations}'`);\n    conv_util.checkPadOnDimRoundingMode('fused depthwiseConv2d', pad, dimRoundingMode);\n    const convInfo = conv_util.computeConv2DInfo(x4D.shape, $filter.shape, strides, dilations, pad, dimRoundingMode, true /* depthwise */);\n    let $bias;\n    if (bias != null) {\n        $bias = convertToTensor(bias, 'bias', 'fused conv2d');\n        [$bias] = makeTypesMatch($bias, $x);\n        broadcast_util.assertAndGetBroadcastShape(convInfo.outShape, $bias.shape);\n    }\n    let $preluActivationWeights;\n    if (preluActivationWeights != null) {\n        $preluActivationWeights = convertToTensor(preluActivationWeights, 'prelu weights', 'fused depthwiseConv2d');\n    }\n    const grad = (dy, saved) => {\n        util.assert(conv_util.tupleValuesAreOne(dilations), () => 'Error in gradient of fused depthwiseConv2d: dilation rates ' +\n            `greater than 1 are not yet supported. Got dilations ` +\n            `'${dilations}'`);\n        const [$filter, x4D, y, bias] = saved;\n        const dyActivation = getFusedDyActivation(dy, y, activation);\n        const xDer = depthwiseConv2dNativeBackpropInput(x4D.shape, dyActivation, $filter, strides, pad, dilations, dimRoundingMode);\n        const filterDer = depthwiseConv2dNativeBackpropFilter(x4D, dyActivation, $filter.shape, strides, pad, dilations, dimRoundingMode);\n        if (bias != null) {\n            const biasDer = getFusedBiasGradient($bias, dyActivation);\n            return [xDer, filterDer, biasDer];\n        }\n        return [xDer, filterDer];\n    };\n    const inputs = {\n        x: x4D,\n        filter: $filter,\n        bias: $bias,\n        preluActivationWeights: $preluActivationWeights\n    };\n    const attrs = {\n        strides,\n        pad,\n        dataFormat,\n        dilations,\n        dimRoundingMode,\n        activation,\n        leakyreluAlpha\n    };\n    // Depending on the the params passed in we will have different number of\n    // inputs and thus a a different number of elements in the gradient.\n    if (bias == null) {\n        const customOp = customGrad((x4D, filter, save) => {\n            // tslint:disable-next-line: no-unnecessary-type-assertion\n            let res = ENGINE.runKernel(FusedDepthwiseConv2D, inputs, attrs);\n            save([filter, x4D, res]);\n            if (reshapedTo4D) {\n                // tslint:disable-next-line: no-unnecessary-type-assertion\n                res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n            }\n            return { value: res, gradFunc: grad };\n        });\n        return customOp(x4D, $filter);\n    }\n    else {\n        const customOpWithBias = customGrad((x4D, filter, bias, save) => {\n            // tslint:disable-next-line: no-unnecessary-type-assertion\n            let res = ENGINE.runKernel(FusedDepthwiseConv2D, inputs, attrs);\n            save([filter, x4D, res, bias]);\n            if (reshapedTo4D) {\n                // tslint:disable-next-line: no-unnecessary-type-assertion\n                res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n            }\n            return { value: res, gradFunc: grad };\n        });\n        return customOpWithBias(x4D, $filter, $bias);\n    }\n}\nexport const depthwiseConv2d = op({ fusedDepthwiseConv2d_ });\n"],"mappings":"AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAASA,MAAT,QAAuB,cAAvB;AACA,SAASC,UAAT,QAA2B,iBAA3B;AACA,SAASC,oBAAT,QAAqC,oBAArC;AACA,SAASC,cAAT,QAA+B,mBAA/B;AACA,SAASC,eAAT,QAAgC,uBAAhC;AACA,OAAO,KAAKC,IAAZ,MAAsB,YAAtB;AACA,SAASC,GAAT,QAAoB,QAApB;AACA,OAAO,KAAKC,cAAZ,MAAgC,mBAAhC;AACA,OAAO,KAAKC,SAAZ,MAA2B,cAA3B;AACA,SAASC,eAAe,IAAIC,sBAA5B,QAA0D,qBAA1D;AACA,SAASC,mCAAT,QAAoD,4CAApD;AACA,SAASC,kCAAT,QAAmD,2CAAnD;AACA,SAASC,eAAT,EAA0BC,oBAA1B,EAAgDC,oBAAhD,EAAsEC,UAAtE,QAAwF,eAAxF;AACA,SAASC,EAAT,QAAmB,cAAnB;AACA,SAASC,OAAT,QAAwB,YAAxB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AACA,SAASC,qBAAT,CAA+B;EAAEC,CAAF;EAAKC,MAAL;EAAaC,OAAb;EAAsBC,GAAtB;EAA2BC,UAAU,GAAG,MAAxC;EAAgDC,SAAS,GAAG,CAAC,CAAD,EAAI,CAAJ,CAA5D;EAAoEC,eAApE;EAAqFC,IAArF;EAA2FC,UAAU,GAAG,QAAxG;EAAkHC,sBAAlH;EAA0IC;AAA1I,CAA/B,EAA2L;EACvL,IAAId,UAAU,CAAChB,MAAM,CAAC+B,KAAP,CAAaC,aAAd,EAA6BJ,UAA7B,CAAV,KAAuD,KAA3D,EAAkE;IAC9D,IAAIK,MAAM,GAAGvB,sBAAsB,CAACU,CAAD,EAAIC,MAAJ,EAAYC,OAAZ,EAAqBC,GAArB,EAA0BC,UAA1B,EAAsCC,SAAtC,EAAiDC,eAAjD,CAAnC;;IACA,IAAIC,IAAI,IAAI,IAAZ,EAAkB;MACdM,MAAM,GAAG3B,GAAG,CAAC2B,MAAD,EAASN,IAAT,CAAZ;IACH;;IACD,OAAOd,eAAe,CAACoB,MAAD,EAASL,UAAT,EAAqBC,sBAArB,EAA6CC,cAA7C,CAAtB;EACH;;EACD,MAAMI,EAAE,GAAG9B,eAAe,CAACgB,CAAD,EAAI,GAAJ,EAAS,iBAAT,EAA4B,SAA5B,CAA1B;EACA,MAAMe,OAAO,GAAG/B,eAAe,CAACiB,MAAD,EAAS,QAAT,EAAmB,iBAAnB,EAAsC,SAAtC,CAA/B;EACA,IAAIe,GAAG,GAAGF,EAAV;EACA,IAAIG,YAAY,GAAG,KAAnB;;EACA,IAAIH,EAAE,CAACI,IAAH,KAAY,CAAhB,EAAmB;IACfD,YAAY,GAAG,IAAf;IACAD,GAAG,GAAGlB,OAAO,CAACgB,EAAD,EAAK,CAAC,CAAD,EAAIA,EAAE,CAACK,KAAH,CAAS,CAAT,CAAJ,EAAiBL,EAAE,CAACK,KAAH,CAAS,CAAT,CAAjB,EAA8BL,EAAE,CAACK,KAAH,CAAS,CAAT,CAA9B,CAAL,CAAb;EACH;;EACDlC,IAAI,CAACmC,MAAL,CAAYJ,GAAG,CAACE,IAAJ,KAAa,CAAzB,EAA4B,MAAO,gEAAD,GAC7B,QAAOF,GAAG,CAACE,IAAK,GADrB;EAEAjC,IAAI,CAACmC,MAAL,CAAYL,OAAO,CAACG,IAAR,KAAiB,CAA7B,EAAgC,MAAO,yDAAD,GACjC,gBAAeH,OAAO,CAACG,IAAK,GADjC;EAEAjC,IAAI,CAACmC,MAAL,CAAYJ,GAAG,CAACG,KAAJ,CAAU,CAAV,MAAiBJ,OAAO,CAACI,KAAR,CAAc,CAAd,CAA7B,EAA+C,MAAO,2DAAD,GAChD,IAAGH,GAAG,CAACG,KAAJ,CAAU,CAAV,CAAa,2CADgC,GAEhD,UAASJ,OAAO,CAACI,KAAR,CAAc,CAAd,CAAiB,GAF/B;;EAGA,IAAId,SAAS,IAAI,IAAjB,EAAuB;IACnBA,SAAS,GAAG,CAAC,CAAD,EAAI,CAAJ,CAAZ;EACH;;EACDpB,IAAI,CAACmC,MAAL,CAAYhC,SAAS,CAACiC,8BAAV,CAAyCnB,OAAzC,EAAkDG,SAAlD,CAAZ,EAA0E,MAAM,sEAC3E,qBAAoBH,OAAQ,mBAAkBG,SAAU,GAD7D;EAEAjB,SAAS,CAACkC,yBAAV,CAAoC,uBAApC,EAA6DnB,GAA7D,EAAkEG,eAAlE;EACA,MAAMiB,QAAQ,GAAGnC,SAAS,CAACoC,iBAAV,CAA4BR,GAAG,CAACG,KAAhC,EAAuCJ,OAAO,CAACI,KAA/C,EAAsDjB,OAAtD,EAA+DG,SAA/D,EAA0EF,GAA1E,EAA+EG,eAA/E,EAAgG;EAAK;EAArG,CAAjB;EACA,IAAImB,KAAJ;;EACA,IAAIlB,IAAI,IAAI,IAAZ,EAAkB;IACdkB,KAAK,GAAGzC,eAAe,CAACuB,IAAD,EAAO,MAAP,EAAe,cAAf,CAAvB;IACA,CAACkB,KAAD,IAAU1C,cAAc,CAAC0C,KAAD,EAAQX,EAAR,CAAxB;IACA3B,cAAc,CAACuC,0BAAf,CAA0CH,QAAQ,CAACI,QAAnD,EAA6DF,KAAK,CAACN,KAAnE;EACH;;EACD,IAAIS,uBAAJ;;EACA,IAAInB,sBAAsB,IAAI,IAA9B,EAAoC;IAChCmB,uBAAuB,GAAG5C,eAAe,CAACyB,sBAAD,EAAyB,eAAzB,EAA0C,uBAA1C,CAAzC;EACH;;EACD,MAAMoB,IAAI,GAAG,CAACC,EAAD,EAAKC,KAAL,KAAe;IACxB9C,IAAI,CAACmC,MAAL,CAAYhC,SAAS,CAAC4C,iBAAV,CAA4B3B,SAA5B,CAAZ,EAAoD,MAAM,gEACrD,sDADqD,GAErD,IAAGA,SAAU,GAFlB;IAGA,MAAM,CAACU,OAAD,EAAUC,GAAV,EAAeiB,CAAf,EAAkB1B,IAAlB,IAA0BwB,KAAhC;IACA,MAAMG,YAAY,GAAGvC,oBAAoB,CAACmC,EAAD,EAAKG,CAAL,EAAQzB,UAAR,CAAzC;IACA,MAAM2B,IAAI,GAAG3C,kCAAkC,CAACwB,GAAG,CAACG,KAAL,EAAYe,YAAZ,EAA0BnB,OAA1B,EAAmCb,OAAnC,EAA4CC,GAA5C,EAAiDE,SAAjD,EAA4DC,eAA5D,CAA/C;IACA,MAAM8B,SAAS,GAAG7C,mCAAmC,CAACyB,GAAD,EAAMkB,YAAN,EAAoBnB,OAAO,CAACI,KAA5B,EAAmCjB,OAAnC,EAA4CC,GAA5C,EAAiDE,SAAjD,EAA4DC,eAA5D,CAArD;;IACA,IAAIC,IAAI,IAAI,IAAZ,EAAkB;MACd,MAAM8B,OAAO,GAAG3C,oBAAoB,CAAC+B,KAAD,EAAQS,YAAR,CAApC;MACA,OAAO,CAACC,IAAD,EAAOC,SAAP,EAAkBC,OAAlB,CAAP;IACH;;IACD,OAAO,CAACF,IAAD,EAAOC,SAAP,CAAP;EACH,CAbD;;EAcA,MAAME,MAAM,GAAG;IACXtC,CAAC,EAAEgB,GADQ;IAEXf,MAAM,EAAEc,OAFG;IAGXR,IAAI,EAAEkB,KAHK;IAIXhB,sBAAsB,EAAEmB;EAJb,CAAf;EAMA,MAAMW,KAAK,GAAG;IACVrC,OADU;IAEVC,GAFU;IAGVC,UAHU;IAIVC,SAJU;IAKVC,eALU;IAMVE,UANU;IAOVE;EAPU,CAAd,CA5DuL,CAqEvL;EACA;;EACA,IAAIH,IAAI,IAAI,IAAZ,EAAkB;IACd,MAAMiC,QAAQ,GAAG3D,UAAU,CAAC,CAACmC,GAAD,EAAMf,MAAN,EAAcwC,IAAd,KAAuB;MAC/C;MACA,IAAIC,GAAG,GAAG9D,MAAM,CAAC+D,SAAP,CAAiB7D,oBAAjB,EAAuCwD,MAAvC,EAA+CC,KAA/C,CAAV;MACAE,IAAI,CAAC,CAACxC,MAAD,EAASe,GAAT,EAAc0B,GAAd,CAAD,CAAJ;;MACA,IAAIzB,YAAJ,EAAkB;QACd;QACAyB,GAAG,GAAG5C,OAAO,CAAC4C,GAAD,EAAM,CAACA,GAAG,CAACvB,KAAJ,CAAU,CAAV,CAAD,EAAeuB,GAAG,CAACvB,KAAJ,CAAU,CAAV,CAAf,EAA6BuB,GAAG,CAACvB,KAAJ,CAAU,CAAV,CAA7B,CAAN,CAAb;MACH;;MACD,OAAO;QAAEyB,KAAK,EAAEF,GAAT;QAAcG,QAAQ,EAAEhB;MAAxB,CAAP;IACH,CAT0B,CAA3B;IAUA,OAAOW,QAAQ,CAACxB,GAAD,EAAMD,OAAN,CAAf;EACH,CAZD,MAaK;IACD,MAAM+B,gBAAgB,GAAGjE,UAAU,CAAC,CAACmC,GAAD,EAAMf,MAAN,EAAcM,IAAd,EAAoBkC,IAApB,KAA6B;MAC7D;MACA,IAAIC,GAAG,GAAG9D,MAAM,CAAC+D,SAAP,CAAiB7D,oBAAjB,EAAuCwD,MAAvC,EAA+CC,KAA/C,CAAV;MACAE,IAAI,CAAC,CAACxC,MAAD,EAASe,GAAT,EAAc0B,GAAd,EAAmBnC,IAAnB,CAAD,CAAJ;;MACA,IAAIU,YAAJ,EAAkB;QACd;QACAyB,GAAG,GAAG5C,OAAO,CAAC4C,GAAD,EAAM,CAACA,GAAG,CAACvB,KAAJ,CAAU,CAAV,CAAD,EAAeuB,GAAG,CAACvB,KAAJ,CAAU,CAAV,CAAf,EAA6BuB,GAAG,CAACvB,KAAJ,CAAU,CAAV,CAA7B,CAAN,CAAb;MACH;;MACD,OAAO;QAAEyB,KAAK,EAAEF,GAAT;QAAcG,QAAQ,EAAEhB;MAAxB,CAAP;IACH,CATkC,CAAnC;IAUA,OAAOiB,gBAAgB,CAAC9B,GAAD,EAAMD,OAAN,EAAeU,KAAf,CAAvB;EACH;AACJ;;AACD,OAAO,MAAMpC,eAAe,GAAGQ,EAAE,CAAC;EAAEE;AAAF,CAAD,CAA1B"},"metadata":{},"sourceType":"module"}