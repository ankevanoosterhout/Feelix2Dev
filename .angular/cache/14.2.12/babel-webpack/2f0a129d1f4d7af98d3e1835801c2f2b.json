{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n// Layer activation functions\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { serialization, tidy } from '@tensorflow/tfjs-core';\nimport * as K from './backend/tfjs_backend';\nimport { deserializeKerasObject } from './utils/generic_utils';\n/**\n * Base class for Activations.\n *\n * Special note: due to cross-language compatibility reasons, the\n * static readonly className field in this family of classes must be set to\n * the initialLowerCamelCase name of the activation.\n */\n\nexport class Activation extends serialization.Serializable {\n  getConfig() {\n    return {};\n  }\n\n}\n/**\n * Exponential linear unit (ELU).\n * Reference: https://arxiv.org/abs/1511.07289\n */\n\nexport class Elu extends Activation {\n  /**\n   * Calculate the activation function.\n   *\n   * @param x: Input.\n   * @param alpha: Scaling factor the negative section.\n   * @return Output of the ELU activation.\n   */\n  apply(x, alpha = 1) {\n    return K.elu(x, alpha);\n  }\n\n}\n/** @nocollapse */\n\nElu.className = 'elu';\nserialization.registerClass(Elu);\n/**\n * Scaled Exponential Linear Unit. (Klambauer et al., 2017).\n * Reference: Self-Normalizing Neural Networks, https://arxiv.org/abs/1706.02515\n * Notes:\n *   - To be used together with the initialization \"lecunNormal\".\n *   - To be used together with the dropout variant \"AlphaDropout\".\n */\n\nexport class Selu extends Activation {\n  apply(x) {\n    return tfc.selu(x);\n  }\n\n}\n/** @nocollapse */\n\nSelu.className = 'selu';\nserialization.registerClass(Selu);\n/**\n *  Rectified linear unit\n */\n\nexport class Relu extends Activation {\n  apply(x) {\n    return tfc.relu(x);\n  }\n\n}\n/** @nocollapse */\n\nRelu.className = 'relu';\nserialization.registerClass(Relu);\n/**\n * Rectified linear unit activation maxing out at 6.0.\n */\n\nexport class Relu6 extends Activation {\n  apply(x) {\n    return tidy(() => tfc.minimum(6.0, tfc.relu(x)));\n  }\n\n}\n/** @nocollapse */\n\nRelu6.className = 'relu6';\nserialization.registerClass(Relu6); //* Linear activation (no-op) */\n\nexport class Linear extends Activation {\n  apply(x) {\n    return x;\n  }\n\n}\n/** @nocollapse */\n\nLinear.className = 'linear';\nserialization.registerClass(Linear);\n/**\n * Sigmoid activation function.\n */\n\nexport class Sigmoid extends Activation {\n  apply(x) {\n    return tfc.sigmoid(x);\n  }\n\n}\n/** @nocollapse */\n\nSigmoid.className = 'sigmoid';\nserialization.registerClass(Sigmoid);\n/**\n * Segment-wise linear approximation of sigmoid.\n */\n\nexport class HardSigmoid extends Activation {\n  apply(x) {\n    return K.hardSigmoid(x);\n  }\n\n}\n/** @nocollapse */\n\nHardSigmoid.className = 'hardSigmoid';\nserialization.registerClass(HardSigmoid);\n/**\n * Softplus activation function.\n */\n\nexport class Softplus extends Activation {\n  apply(x) {\n    return tfc.softplus(x);\n  }\n\n}\n/** @nocollapse */\n\nSoftplus.className = 'softplus';\nserialization.registerClass(Softplus);\n/**\n * Softsign activation function.\n */\n\nexport class Softsign extends Activation {\n  apply(x) {\n    return K.softsign(x);\n  }\n\n}\n/** @nocollapse */\n\nSoftsign.className = 'softsign';\nserialization.registerClass(Softsign);\n/**\n * Hyperbolic tangent function.\n */\n\nexport class Tanh extends Activation {\n  apply(x) {\n    return tfc.tanh(x);\n  }\n\n}\n/** @nocollapse */\n\nTanh.className = 'tanh';\nserialization.registerClass(Tanh);\n/**\n * Softmax activation function\n */\n\nexport class Softmax extends Activation {\n  /**\n   * Calculate the activation function.\n   *\n   * @param x Tensor.\n   * @param axis Integer, axis along which the softmax normalization is applied.\n   * Invalid if < 2, as softmax across 1 (the batch dimension) is assumed to be\n   * an error.\n   *\n   * @returns a Tensor of the same shape as x\n   *\n   * @throws ValueError: In case `dim(x) < 2`.\n   */\n  apply(x, axis = -1) {\n    return tfc.softmax(x, axis);\n  }\n\n}\n/** @nocollapse */\n\nSoftmax.className = 'softmax';\nserialization.registerClass(Softmax);\n/**\n * Log softmax activation function\n */\n\nexport class LogSoftmax extends Activation {\n  /**\n   * Calculate the activation function of log softmax:\n   * log( exp(x_i) / sum(exp(x)) )\n   *\n   * @param x Tensor.\n   * @param axis Integer, axis along which the softmax normalization is applied.\n   * Invalid if < 2, as softmax across 1 (the batch dimension) is assumed to be\n   * an error.\n   *\n   * @returns a Tensor of the same shape as x\n   *\n   * @throws ValueError: In case `dim(x) < 2`.\n   */\n  apply(x, axis = -1) {\n    return tfc.logSoftmax(x, axis);\n  }\n\n}\n/** @nocollapse */\n\nLogSoftmax.className = 'logSoftmax';\nserialization.registerClass(LogSoftmax);\n/**\n * Swish activation function\n */\n\nexport class Swish extends Activation {\n  /**\n   * Calculate the activation function.\n   *\n   * @param x Tensor.\n   * @param alpha Scaling factor for the sigmoid function.\n   * @returns a Tensor of the same shape as x\n   */\n  apply(x, alpha = 1) {\n    return tidy(() => tfc.mul(tfc.sigmoid(tfc.mul(x, alpha)), x));\n  }\n\n}\n/** @nocollapse */\n\nSwish.className = 'swish';\nserialization.registerClass(Swish);\n/**\n * Mish activation function\n */\n\nexport class Mish extends Activation {\n  /**\n   * Calculate the activation function.\n   *\n   * @param x Tensor.\n   * @returns a Tensor of the same shape as x\n   */\n  apply(x) {\n    return tidy(() => tfc.mul(x, tfc.tanh(tfc.softplus(x))));\n  }\n\n}\n/** @nocollapse */\n\nMish.className = 'mish';\nserialization.registerClass(Mish);\nexport function serializeActivation(activation) {\n  return activation.getClassName();\n}\nexport function deserializeActivation(config, customObjects = {}) {\n  return deserializeKerasObject(config, serialization.SerializationMap.getMap().classNameMap, customObjects, 'activation');\n}\nexport function getActivation(identifier) {\n  if (identifier == null) {\n    const config = {};\n    config['className'] = 'linear';\n    config['config'] = {};\n    return deserializeActivation(config);\n  }\n\n  if (typeof identifier === 'string') {\n    const config = {};\n    config['className'] = identifier;\n    config['config'] = {};\n    return deserializeActivation(config);\n  } else if (identifier instanceof Activation) {\n    return identifier;\n  } else {\n    return deserializeActivation(identifier);\n  }\n}","map":{"version":3,"names":["tfc","serialization","tidy","K","deserializeKerasObject","Activation","Serializable","getConfig","Elu","apply","x","alpha","elu","className","registerClass","Selu","selu","Relu","relu","Relu6","minimum","Linear","Sigmoid","sigmoid","HardSigmoid","hardSigmoid","Softplus","softplus","Softsign","softsign","Tanh","tanh","Softmax","axis","softmax","LogSoftmax","logSoftmax","Swish","mul","Mish","serializeActivation","activation","getClassName","deserializeActivation","config","customObjects","SerializationMap","getMap","classNameMap","getActivation","identifier"],"sources":["C:/Users/Anke/Documents/Feelix documents/Feelix2.0-dev/Feelix v2/node_modules/@tensorflow/tfjs-layers/dist/activations.js"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n// Layer activation functions\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { serialization, tidy } from '@tensorflow/tfjs-core';\nimport * as K from './backend/tfjs_backend';\nimport { deserializeKerasObject } from './utils/generic_utils';\n/**\n * Base class for Activations.\n *\n * Special note: due to cross-language compatibility reasons, the\n * static readonly className field in this family of classes must be set to\n * the initialLowerCamelCase name of the activation.\n */\nexport class Activation extends serialization.Serializable {\n    getConfig() {\n        return {};\n    }\n}\n/**\n * Exponential linear unit (ELU).\n * Reference: https://arxiv.org/abs/1511.07289\n */\nexport class Elu extends Activation {\n    /**\n     * Calculate the activation function.\n     *\n     * @param x: Input.\n     * @param alpha: Scaling factor the negative section.\n     * @return Output of the ELU activation.\n     */\n    apply(x, alpha = 1) {\n        return K.elu(x, alpha);\n    }\n}\n/** @nocollapse */\nElu.className = 'elu';\nserialization.registerClass(Elu);\n/**\n * Scaled Exponential Linear Unit. (Klambauer et al., 2017).\n * Reference: Self-Normalizing Neural Networks, https://arxiv.org/abs/1706.02515\n * Notes:\n *   - To be used together with the initialization \"lecunNormal\".\n *   - To be used together with the dropout variant \"AlphaDropout\".\n */\nexport class Selu extends Activation {\n    apply(x) {\n        return tfc.selu(x);\n    }\n}\n/** @nocollapse */\nSelu.className = 'selu';\nserialization.registerClass(Selu);\n/**\n *  Rectified linear unit\n */\nexport class Relu extends Activation {\n    apply(x) {\n        return tfc.relu(x);\n    }\n}\n/** @nocollapse */\nRelu.className = 'relu';\nserialization.registerClass(Relu);\n/**\n * Rectified linear unit activation maxing out at 6.0.\n */\nexport class Relu6 extends Activation {\n    apply(x) {\n        return tidy(() => tfc.minimum(6.0, tfc.relu(x)));\n    }\n}\n/** @nocollapse */\nRelu6.className = 'relu6';\nserialization.registerClass(Relu6);\n//* Linear activation (no-op) */\nexport class Linear extends Activation {\n    apply(x) {\n        return x;\n    }\n}\n/** @nocollapse */\nLinear.className = 'linear';\nserialization.registerClass(Linear);\n/**\n * Sigmoid activation function.\n */\nexport class Sigmoid extends Activation {\n    apply(x) {\n        return tfc.sigmoid(x);\n    }\n}\n/** @nocollapse */\nSigmoid.className = 'sigmoid';\nserialization.registerClass(Sigmoid);\n/**\n * Segment-wise linear approximation of sigmoid.\n */\nexport class HardSigmoid extends Activation {\n    apply(x) {\n        return K.hardSigmoid(x);\n    }\n}\n/** @nocollapse */\nHardSigmoid.className = 'hardSigmoid';\nserialization.registerClass(HardSigmoid);\n/**\n * Softplus activation function.\n */\nexport class Softplus extends Activation {\n    apply(x) {\n        return tfc.softplus(x);\n    }\n}\n/** @nocollapse */\nSoftplus.className = 'softplus';\nserialization.registerClass(Softplus);\n/**\n * Softsign activation function.\n */\nexport class Softsign extends Activation {\n    apply(x) {\n        return K.softsign(x);\n    }\n}\n/** @nocollapse */\nSoftsign.className = 'softsign';\nserialization.registerClass(Softsign);\n/**\n * Hyperbolic tangent function.\n */\nexport class Tanh extends Activation {\n    apply(x) {\n        return tfc.tanh(x);\n    }\n}\n/** @nocollapse */\nTanh.className = 'tanh';\nserialization.registerClass(Tanh);\n/**\n * Softmax activation function\n */\nexport class Softmax extends Activation {\n    /**\n     * Calculate the activation function.\n     *\n     * @param x Tensor.\n     * @param axis Integer, axis along which the softmax normalization is applied.\n     * Invalid if < 2, as softmax across 1 (the batch dimension) is assumed to be\n     * an error.\n     *\n     * @returns a Tensor of the same shape as x\n     *\n     * @throws ValueError: In case `dim(x) < 2`.\n     */\n    apply(x, axis = (-1)) {\n        return tfc.softmax(x, axis);\n    }\n}\n/** @nocollapse */\nSoftmax.className = 'softmax';\nserialization.registerClass(Softmax);\n/**\n * Log softmax activation function\n */\nexport class LogSoftmax extends Activation {\n    /**\n     * Calculate the activation function of log softmax:\n     * log( exp(x_i) / sum(exp(x)) )\n     *\n     * @param x Tensor.\n     * @param axis Integer, axis along which the softmax normalization is applied.\n     * Invalid if < 2, as softmax across 1 (the batch dimension) is assumed to be\n     * an error.\n     *\n     * @returns a Tensor of the same shape as x\n     *\n     * @throws ValueError: In case `dim(x) < 2`.\n     */\n    apply(x, axis = (-1)) {\n        return tfc.logSoftmax(x, axis);\n    }\n}\n/** @nocollapse */\nLogSoftmax.className = 'logSoftmax';\nserialization.registerClass(LogSoftmax);\n/**\n * Swish activation function\n */\nexport class Swish extends Activation {\n    /**\n     * Calculate the activation function.\n     *\n     * @param x Tensor.\n     * @param alpha Scaling factor for the sigmoid function.\n     * @returns a Tensor of the same shape as x\n     */\n    apply(x, alpha = 1) {\n        return tidy(() => tfc.mul(tfc.sigmoid(tfc.mul(x, alpha)), x));\n    }\n}\n/** @nocollapse */\nSwish.className = 'swish';\nserialization.registerClass(Swish);\n/**\n * Mish activation function\n */\nexport class Mish extends Activation {\n    /**\n     * Calculate the activation function.\n     *\n     * @param x Tensor.\n     * @returns a Tensor of the same shape as x\n     */\n    apply(x) {\n        return tidy(() => tfc.mul(x, tfc.tanh(tfc.softplus(x))));\n    }\n}\n/** @nocollapse */\nMish.className = 'mish';\nserialization.registerClass(Mish);\nexport function serializeActivation(activation) {\n    return activation.getClassName();\n}\nexport function deserializeActivation(config, customObjects = {}) {\n    return deserializeKerasObject(config, serialization.SerializationMap.getMap().classNameMap, customObjects, 'activation');\n}\nexport function getActivation(identifier) {\n    if (identifier == null) {\n        const config = {};\n        config['className'] = 'linear';\n        config['config'] = {};\n        return deserializeActivation(config);\n    }\n    if (typeof identifier === 'string') {\n        const config = {};\n        config['className'] = identifier;\n        config['config'] = {};\n        return deserializeActivation(config);\n    }\n    else if (identifier instanceof Activation) {\n        return identifier;\n    }\n    else {\n        return deserializeActivation(identifier);\n    }\n}\n"],"mappings":"AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,KAAKA,GAAZ,MAAqB,uBAArB;AACA,SAASC,aAAT,EAAwBC,IAAxB,QAAoC,uBAApC;AACA,OAAO,KAAKC,CAAZ,MAAmB,wBAAnB;AACA,SAASC,sBAAT,QAAuC,uBAAvC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AACA,OAAO,MAAMC,UAAN,SAAyBJ,aAAa,CAACK,YAAvC,CAAoD;EACvDC,SAAS,GAAG;IACR,OAAO,EAAP;EACH;;AAHsD;AAK3D;AACA;AACA;AACA;;AACA,OAAO,MAAMC,GAAN,SAAkBH,UAAlB,CAA6B;EAChC;AACJ;AACA;AACA;AACA;AACA;AACA;EACII,KAAK,CAACC,CAAD,EAAIC,KAAK,GAAG,CAAZ,EAAe;IAChB,OAAOR,CAAC,CAACS,GAAF,CAAMF,CAAN,EAASC,KAAT,CAAP;EACH;;AAV+B;AAYpC;;AACAH,GAAG,CAACK,SAAJ,GAAgB,KAAhB;AACAZ,aAAa,CAACa,aAAd,CAA4BN,GAA5B;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AACA,OAAO,MAAMO,IAAN,SAAmBV,UAAnB,CAA8B;EACjCI,KAAK,CAACC,CAAD,EAAI;IACL,OAAOV,GAAG,CAACgB,IAAJ,CAASN,CAAT,CAAP;EACH;;AAHgC;AAKrC;;AACAK,IAAI,CAACF,SAAL,GAAiB,MAAjB;AACAZ,aAAa,CAACa,aAAd,CAA4BC,IAA5B;AACA;AACA;AACA;;AACA,OAAO,MAAME,IAAN,SAAmBZ,UAAnB,CAA8B;EACjCI,KAAK,CAACC,CAAD,EAAI;IACL,OAAOV,GAAG,CAACkB,IAAJ,CAASR,CAAT,CAAP;EACH;;AAHgC;AAKrC;;AACAO,IAAI,CAACJ,SAAL,GAAiB,MAAjB;AACAZ,aAAa,CAACa,aAAd,CAA4BG,IAA5B;AACA;AACA;AACA;;AACA,OAAO,MAAME,KAAN,SAAoBd,UAApB,CAA+B;EAClCI,KAAK,CAACC,CAAD,EAAI;IACL,OAAOR,IAAI,CAAC,MAAMF,GAAG,CAACoB,OAAJ,CAAY,GAAZ,EAAiBpB,GAAG,CAACkB,IAAJ,CAASR,CAAT,CAAjB,CAAP,CAAX;EACH;;AAHiC;AAKtC;;AACAS,KAAK,CAACN,SAAN,GAAkB,OAAlB;AACAZ,aAAa,CAACa,aAAd,CAA4BK,KAA5B,E,CACA;;AACA,OAAO,MAAME,MAAN,SAAqBhB,UAArB,CAAgC;EACnCI,KAAK,CAACC,CAAD,EAAI;IACL,OAAOA,CAAP;EACH;;AAHkC;AAKvC;;AACAW,MAAM,CAACR,SAAP,GAAmB,QAAnB;AACAZ,aAAa,CAACa,aAAd,CAA4BO,MAA5B;AACA;AACA;AACA;;AACA,OAAO,MAAMC,OAAN,SAAsBjB,UAAtB,CAAiC;EACpCI,KAAK,CAACC,CAAD,EAAI;IACL,OAAOV,GAAG,CAACuB,OAAJ,CAAYb,CAAZ,CAAP;EACH;;AAHmC;AAKxC;;AACAY,OAAO,CAACT,SAAR,GAAoB,SAApB;AACAZ,aAAa,CAACa,aAAd,CAA4BQ,OAA5B;AACA;AACA;AACA;;AACA,OAAO,MAAME,WAAN,SAA0BnB,UAA1B,CAAqC;EACxCI,KAAK,CAACC,CAAD,EAAI;IACL,OAAOP,CAAC,CAACsB,WAAF,CAAcf,CAAd,CAAP;EACH;;AAHuC;AAK5C;;AACAc,WAAW,CAACX,SAAZ,GAAwB,aAAxB;AACAZ,aAAa,CAACa,aAAd,CAA4BU,WAA5B;AACA;AACA;AACA;;AACA,OAAO,MAAME,QAAN,SAAuBrB,UAAvB,CAAkC;EACrCI,KAAK,CAACC,CAAD,EAAI;IACL,OAAOV,GAAG,CAAC2B,QAAJ,CAAajB,CAAb,CAAP;EACH;;AAHoC;AAKzC;;AACAgB,QAAQ,CAACb,SAAT,GAAqB,UAArB;AACAZ,aAAa,CAACa,aAAd,CAA4BY,QAA5B;AACA;AACA;AACA;;AACA,OAAO,MAAME,QAAN,SAAuBvB,UAAvB,CAAkC;EACrCI,KAAK,CAACC,CAAD,EAAI;IACL,OAAOP,CAAC,CAAC0B,QAAF,CAAWnB,CAAX,CAAP;EACH;;AAHoC;AAKzC;;AACAkB,QAAQ,CAACf,SAAT,GAAqB,UAArB;AACAZ,aAAa,CAACa,aAAd,CAA4Bc,QAA5B;AACA;AACA;AACA;;AACA,OAAO,MAAME,IAAN,SAAmBzB,UAAnB,CAA8B;EACjCI,KAAK,CAACC,CAAD,EAAI;IACL,OAAOV,GAAG,CAAC+B,IAAJ,CAASrB,CAAT,CAAP;EACH;;AAHgC;AAKrC;;AACAoB,IAAI,CAACjB,SAAL,GAAiB,MAAjB;AACAZ,aAAa,CAACa,aAAd,CAA4BgB,IAA5B;AACA;AACA;AACA;;AACA,OAAO,MAAME,OAAN,SAAsB3B,UAAtB,CAAiC;EACpC;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACII,KAAK,CAACC,CAAD,EAAIuB,IAAI,GAAI,CAAC,CAAb,EAAiB;IAClB,OAAOjC,GAAG,CAACkC,OAAJ,CAAYxB,CAAZ,EAAeuB,IAAf,CAAP;EACH;;AAfmC;AAiBxC;;AACAD,OAAO,CAACnB,SAAR,GAAoB,SAApB;AACAZ,aAAa,CAACa,aAAd,CAA4BkB,OAA5B;AACA;AACA;AACA;;AACA,OAAO,MAAMG,UAAN,SAAyB9B,UAAzB,CAAoC;EACvC;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACII,KAAK,CAACC,CAAD,EAAIuB,IAAI,GAAI,CAAC,CAAb,EAAiB;IAClB,OAAOjC,GAAG,CAACoC,UAAJ,CAAe1B,CAAf,EAAkBuB,IAAlB,CAAP;EACH;;AAhBsC;AAkB3C;;AACAE,UAAU,CAACtB,SAAX,GAAuB,YAAvB;AACAZ,aAAa,CAACa,aAAd,CAA4BqB,UAA5B;AACA;AACA;AACA;;AACA,OAAO,MAAME,KAAN,SAAoBhC,UAApB,CAA+B;EAClC;AACJ;AACA;AACA;AACA;AACA;AACA;EACII,KAAK,CAACC,CAAD,EAAIC,KAAK,GAAG,CAAZ,EAAe;IAChB,OAAOT,IAAI,CAAC,MAAMF,GAAG,CAACsC,GAAJ,CAAQtC,GAAG,CAACuB,OAAJ,CAAYvB,GAAG,CAACsC,GAAJ,CAAQ5B,CAAR,EAAWC,KAAX,CAAZ,CAAR,EAAwCD,CAAxC,CAAP,CAAX;EACH;;AAViC;AAYtC;;AACA2B,KAAK,CAACxB,SAAN,GAAkB,OAAlB;AACAZ,aAAa,CAACa,aAAd,CAA4BuB,KAA5B;AACA;AACA;AACA;;AACA,OAAO,MAAME,IAAN,SAAmBlC,UAAnB,CAA8B;EACjC;AACJ;AACA;AACA;AACA;AACA;EACII,KAAK,CAACC,CAAD,EAAI;IACL,OAAOR,IAAI,CAAC,MAAMF,GAAG,CAACsC,GAAJ,CAAQ5B,CAAR,EAAWV,GAAG,CAAC+B,IAAJ,CAAS/B,GAAG,CAAC2B,QAAJ,CAAajB,CAAb,CAAT,CAAX,CAAP,CAAX;EACH;;AATgC;AAWrC;;AACA6B,IAAI,CAAC1B,SAAL,GAAiB,MAAjB;AACAZ,aAAa,CAACa,aAAd,CAA4ByB,IAA5B;AACA,OAAO,SAASC,mBAAT,CAA6BC,UAA7B,EAAyC;EAC5C,OAAOA,UAAU,CAACC,YAAX,EAAP;AACH;AACD,OAAO,SAASC,qBAAT,CAA+BC,MAA/B,EAAuCC,aAAa,GAAG,EAAvD,EAA2D;EAC9D,OAAOzC,sBAAsB,CAACwC,MAAD,EAAS3C,aAAa,CAAC6C,gBAAd,CAA+BC,MAA/B,GAAwCC,YAAjD,EAA+DH,aAA/D,EAA8E,YAA9E,CAA7B;AACH;AACD,OAAO,SAASI,aAAT,CAAuBC,UAAvB,EAAmC;EACtC,IAAIA,UAAU,IAAI,IAAlB,EAAwB;IACpB,MAAMN,MAAM,GAAG,EAAf;IACAA,MAAM,CAAC,WAAD,CAAN,GAAsB,QAAtB;IACAA,MAAM,CAAC,QAAD,CAAN,GAAmB,EAAnB;IACA,OAAOD,qBAAqB,CAACC,MAAD,CAA5B;EACH;;EACD,IAAI,OAAOM,UAAP,KAAsB,QAA1B,EAAoC;IAChC,MAAMN,MAAM,GAAG,EAAf;IACAA,MAAM,CAAC,WAAD,CAAN,GAAsBM,UAAtB;IACAN,MAAM,CAAC,QAAD,CAAN,GAAmB,EAAnB;IACA,OAAOD,qBAAqB,CAACC,MAAD,CAA5B;EACH,CALD,MAMK,IAAIM,UAAU,YAAY7C,UAA1B,EAAsC;IACvC,OAAO6C,UAAP;EACH,CAFI,MAGA;IACD,OAAOP,qBAAqB,CAACO,UAAD,CAA5B;EACH;AACJ"},"metadata":{},"sourceType":"module"}