{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * Normalization layers.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { moments, reshape, serialization, tidy, util } from '@tensorflow/tfjs-core';\nimport { getConstraint, serializeConstraint } from '../constraints';\nimport { InputSpec, Layer } from '../engine/topology';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { getInitializer, serializeInitializer } from '../initializers';\nimport { getRegularizer, serializeRegularizer } from '../regularizers';\nimport * as generic_utils from '../utils/generic_utils';\nimport * as math_utils from '../utils/math_utils';\nimport { getExactlyOneShape, getExactlyOneTensor } from '../utils/types_utils';\n/**\n * Applies batch normalization on x given mean, var, beta and gamma.\n *\n * I.e. returns:\n *   `output = (x - mean) / (sqrt(var) + epsilon) * gamma + beta`\n *\n * @param x Input tensor.\n * @param mean Mean of batch.\n * @param variance Variance of batch.\n * @param beta Tensor with which to center the input.\n * @param gamma Tensor by which to scale the input.\n * @param epsilon Fuzz factor.\n * @returns The result of the batch normalization.\n */\n\nexport function batchNormalization(x, mean, variance, beta, gamma, epsilon = 1e-3) {\n  let out;\n\n  if (x.rank === 2) {\n    out = tfc.batchNorm2d(x, mean, variance, beta, gamma, epsilon);\n  } else if (x.rank === 3) {\n    // TODO(cais): Check rank; give proper error message.\n    out = tfc.batchNorm3d(x, mean, variance, beta, gamma, epsilon);\n  } else if (x.rank === 4) {\n    out = tfc.batchNorm4d(x, mean, variance, beta, gamma, epsilon);\n  } else {\n    throw new NotImplementedError(`batchNormalization is not implemented for array of rank ${x.rank} ` + `yet`);\n  }\n\n  return out;\n}\n/**\n * Non-broadcasting batch normalization for use in training (not inference).\n *\n * The input is normalized to zero mean and unit variance along the\n * `reductionAxes`, followed by scaling with `gamma` and shifted by `beta`.\n * The result of that is returned as the first element\n * of the returned `Array`. The other two elements are the mean and variance,\n * respectively.\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\n\nfunction regularNormalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon = 1e-3) {\n  return tidy(() => {\n    const meanAndVariance = tfc.moments(x, reductionAxes);\n    const mean = meanAndVariance.mean;\n    const variance = meanAndVariance.variance;\n    const normed = batchNormalization(x, mean, variance, beta, gamma, epsilon);\n    return [normed, mean, variance];\n  });\n}\n/**\n * Broadcasting batch normalization for use in training (not inference).\n *\n * The input is normalized to zero mean and unit variance along the\n * `reductionAxes`, followed by scaling with `gamma` and shifted by `beta`.\n * The result of that is returned as the first element\n * of the returned `Array`. The other two elements are the mean and variance,\n * respectively.\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\n\n\nfunction broadcastNormalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon = 1e-3) {\n  return tidy(() => {\n    const meanAndVariance = tfc.moments(x, reductionAxes);\n    const mean = meanAndVariance.mean;\n    const variance = meanAndVariance.variance;\n    const targetShape = [];\n\n    for (const axis of math_utils.range(0, x.rank)) {\n      if (reductionAxes.indexOf(axis) !== -1) {\n        targetShape.push(1);\n      } else {\n        targetShape.push(x.shape[axis]);\n      }\n    }\n\n    const broadcastMean = reshape(mean, targetShape);\n    const broadcastVariance = reshape(variance, targetShape);\n    const broadcastGamma = gamma == null ? null : reshape(gamma, targetShape);\n    const broadcastBeta = beta == null ? null : reshape(beta, targetShape);\n    const normed = batchNormalization(x, broadcastMean, broadcastVariance, broadcastBeta, broadcastGamma, epsilon);\n    return [normed, mean, variance];\n  });\n}\n/**\n * Batch normalization for use in training (not inference).\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\n\n\nexport function normalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon = 1e-3) {\n  if (util.arraysEqual(reductionAxes.slice().sort(), math_utils.range(0, x.rank - 1))) {\n    return regularNormalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon);\n  } else {\n    return broadcastNormalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon);\n  }\n}\nexport class BatchNormalization extends Layer {\n  constructor(args) {\n    if (args == null) {\n      args = {};\n    }\n\n    super(args);\n    this.supportsMasking = true;\n    this.axis = args.axis == null ? -1 : args.axis;\n    this.momentum = args.momentum == null ? 0.99 : args.momentum;\n    this.epsilon = args.epsilon == null ? 1e-3 : args.epsilon;\n    this.center = args.center == null ? true : args.center;\n    this.scale = args.scale == null ? true : args.scale;\n    this.betaInitializer = getInitializer(args.betaInitializer || 'zeros');\n    this.gammaInitializer = getInitializer(args.gammaInitializer || 'ones');\n    this.movingMeanInitializer = getInitializer(args.movingMeanInitializer || 'zeros');\n    this.movingVarianceInitializer = getInitializer(args.movingVarianceInitializer || 'ones');\n    this.betaConstraint = getConstraint(args.betaConstraint);\n    this.gammaConstraint = getConstraint(args.gammaConstraint);\n    this.betaRegularizer = getRegularizer(args.betaRegularizer);\n    this.gammaRegularizer = getRegularizer(args.gammaRegularizer);\n  }\n\n  build(inputShape) {\n    inputShape = getExactlyOneShape(inputShape);\n    const axis = this.axis >= 0 ? this.axis : this.axis + inputShape.length;\n    const dim = inputShape[axis];\n\n    if (dim == null) {\n      throw new ValueError(`Axis ${axis} of input tensor should have a defined dimension but ` + `the layer received an input with shape ` + `${JSON.stringify(inputShape)}.`);\n    }\n\n    this.inputSpec = [new InputSpec({\n      ndim: inputShape.length,\n      axes: {\n        [axis]: dim\n      }\n    })];\n    const shape = [dim];\n\n    if (this.scale) {\n      this.gamma = this.addWeight('gamma', shape, null, this.gammaInitializer, this.gammaRegularizer, true, this.gammaConstraint);\n    }\n\n    if (this.center) {\n      this.beta = this.addWeight('beta', shape, null, this.betaInitializer, this.betaRegularizer, true, this.betaConstraint);\n    }\n\n    this.movingMean = this.addWeight('moving_mean', shape, null, this.movingMeanInitializer, null, false);\n    this.movingVariance = this.addWeight('moving_variance', shape, null, this.movingVarianceInitializer, null, false);\n    this.built = true;\n  }\n\n  call(inputs, kwargs) {\n    return tidy(() => {\n      const training = kwargs['training'] == null ? false : kwargs['training'];\n      const input = getExactlyOneTensor(inputs);\n      const inputShape = input.shape;\n      const ndim = inputShape.length;\n      const reductionAxes = math_utils.range(0, ndim);\n      const axis = this.axis >= 0 ? this.axis : this.axis + ndim;\n      reductionAxes.splice(axis, 1);\n      const broadcastShape = generic_utils.pyListRepeat(1, ndim);\n      broadcastShape[axis] = inputShape[axis];\n      const sortedReductionAxes = reductionAxes.slice();\n      sortedReductionAxes.sort();\n      const needsBroadcasting = !util.arraysEqual(sortedReductionAxes, math_utils.range(0, ndim).slice(0, ndim - 1));\n\n      const normalizeInference = () => {\n        if (needsBroadcasting) {\n          const broadcastMovingMean = reshape(this.movingMean.read(), broadcastShape);\n          const broadcastMovingVariance = reshape(this.movingVariance.read(), broadcastShape);\n          const broadcastBeta = this.center ? reshape(this.beta.read(), broadcastShape) : null;\n          const broadcastGamma = this.scale ? reshape(this.gamma.read(), broadcastShape) : null;\n          return batchNormalization(input, broadcastMovingMean, broadcastMovingVariance, broadcastBeta, broadcastGamma, this.epsilon);\n        } else {\n          return batchNormalization(input, this.movingMean.read(), this.movingVariance.read(), this.beta == null ? null : this.beta.read(), this.gamma == null ? null : this.gamma.read(), this.epsilon);\n        }\n      };\n\n      if (!training) {\n        return normalizeInference();\n      }\n\n      const [normedTraining, mean, variance] = normalizeBatchInTraining(input, this.gamma.read(), this.beta.read(), reductionAxes, this.epsilon);\n\n      const doMovingAverage = (variable, value, momentum) => {\n        tfc.tidy(() => {\n          const decay = 1 - momentum;\n          const origValue = variable.read();\n          const updateDelta = tfc.mul(tfc.sub(origValue, value), decay);\n          variable.write(tfc.sub(origValue, updateDelta));\n        });\n      }; // Perform updates to moving mean and moving variance for training.\n      // Porting Note: In PyKeras, these updates to `movingMean` and\n      //   `movingAverage` are done as a deferred Graph, added to the `Layer`'s\n      //   `update`s using the `add_update()` method. Here we do it imperatively\n      //   and encapsulate the updates in a function that is invoked\n      //   immediately.\n\n\n      const updateMovingMeanAndVariance = () => {\n        doMovingAverage(this.movingMean, mean, this.momentum);\n        doMovingAverage(this.movingVariance, variance, this.momentum);\n      };\n\n      updateMovingMeanAndVariance();\n      return normedTraining;\n    });\n  }\n\n  getConfig() {\n    const config = {\n      axis: this.axis,\n      momentum: this.momentum,\n      epsilon: this.epsilon,\n      center: this.center,\n      scale: this.scale,\n      betaInitializer: serializeInitializer(this.betaInitializer),\n      gammaInitializer: serializeInitializer(this.gammaInitializer),\n      movingMeanInitializer: serializeInitializer(this.movingMeanInitializer),\n      movingVarianceInitializer: serializeInitializer(this.movingVarianceInitializer),\n      betaRegularizer: serializeRegularizer(this.betaRegularizer),\n      gammaRegularizer: serializeRegularizer(this.gammaRegularizer),\n      betaConstraint: serializeConstraint(this.betaConstraint),\n      gammaConstraint: serializeConstraint(this.gammaConstraint)\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n}\n/** @nocollapse */\n\nBatchNormalization.className = 'BatchNormalization';\nserialization.registerClass(BatchNormalization);\nexport class LayerNormalization extends Layer {\n  constructor(args) {\n    if (args == null) {\n      args = {};\n    }\n\n    super(args);\n    this.axis = args.axis == null ? -1 : args.axis;\n\n    if (typeof this.axis === 'number') {\n      if (!Number.isInteger(this.axis)) {\n        throw new Error(`Expected axis to be an integer, but received ${this.axis}`);\n      }\n    } else if (Array.isArray(this.axis)) {\n      for (const axis of this.axis) {\n        if (!Number.isInteger(axis)) {\n          throw new Error(`Expected axis to be an array of integers, ` + `but received ${JSON.stringify(this.axis)}`);\n        }\n      }\n    } else {\n      throw new Error(`Expected axis to be an integer or an array of integers, ` + `but received ${JSON.stringify(this.axis)}`);\n    }\n\n    this.epsilon = args.epsilon == null ? 1e-3 : args.epsilon;\n    this.center = args.center == null ? true : args.center;\n    this.scale = args.scale == null ? true : args.scale;\n    this.betaInitializer = getInitializer(args.betaInitializer || 'zeros');\n    this.gammaInitializer = getInitializer(args.gammaInitializer || 'ones');\n    this.betaRegularizer = getRegularizer(args.betaRegularizer);\n    this.gammaRegularizer = getRegularizer(args.gammaRegularizer);\n    this.supportsMasking = true;\n  }\n\n  build(inputShape) {\n    inputShape = getExactlyOneShape(inputShape);\n    const nDims = inputShape.length; // Convert axis to array and resolve negatives.\n\n    if (typeof this.axis === 'number') {\n      this.axis = [this.axis];\n    }\n\n    for (let i = 0; i < this.axis.length; ++i) {\n      if (this.axis[i] < 0) {\n        this.axis[i] += nDims;\n      }\n    } // Further validate axes.\n\n\n    for (const axis of this.axis) {\n      if (axis < 0 || axis >= nDims) {\n        throw new Error(`Invalid axis: ${axis}`);\n      }\n    }\n\n    if (this.axis.length !== generic_utils.unique(this.axis).length) {\n      throw new Error(`Found duplicate axes in: ${this.axis}`);\n    }\n\n    const paramShape = this.axis.map(axis => inputShape[axis]);\n    const trainable = true;\n\n    if (this.scale) {\n      this.gamma = this.addWeight('gamma', paramShape, 'float32', this.gammaInitializer, this.gammaRegularizer, trainable);\n    } else {\n      this.gamma = null;\n    }\n\n    if (this.center) {\n      this.beta = this.addWeight('beta', paramShape, 'float32', this.betaInitializer, this.betaRegularizer, trainable);\n    } else {\n      this.beta = null;\n    }\n\n    this.built = true;\n  }\n\n  call(inputs, kwargs) {\n    const input = getExactlyOneTensor(inputs);\n    const inputShape = input.shape;\n    const nDims = inputShape.length;\n    return tidy(() => {\n      const keepDims = true;\n      let {\n        mean,\n        variance\n      } = moments(input, this.axis, keepDims);\n      const broadcastShape = generic_utils.pyListRepeat(1, nDims);\n\n      for (const dim of this.axis) {\n        broadcastShape[dim] = inputShape[dim];\n      }\n\n      const broadcast = v => {\n        if (v != null && v.shape.length !== nDims) {\n          return tfc.reshape(v, broadcastShape);\n        } else {\n          return v;\n        }\n      };\n\n      let scale = this.scale ? broadcast(this.gamma.read()) : null;\n      let offset = this.center ? broadcast(this.beta.read()) : null; // TODO(https://github.com/tensorflow/tfjs/issues/2120): The tiling below\n      // is a workaround for the limitation of core's batchNormalization?d don't\n      // support broadcasting in their gradients. In addition, the tiling is\n      // necessary to ensure correctness on the browser CPU backend regardless\n      // of forward or backward computation. Remove this workaround once the\n      // limitation is addressed. See .\n\n      const momentsTiling = [];\n      const scaleOffsetTiling = [];\n\n      for (let i = 0; i < nDims; ++i) {\n        if (this.axis.indexOf(i) !== -1) {\n          momentsTiling.push(inputShape[i]);\n          scaleOffsetTiling.push(1);\n        } else {\n          momentsTiling.push(1);\n          scaleOffsetTiling.push(inputShape[i]);\n        }\n      }\n\n      mean = tfc.tile(mean, momentsTiling);\n      variance = tfc.tile(variance, momentsTiling);\n\n      if (scale != null) {\n        scale = tfc.tile(scale, scaleOffsetTiling);\n      }\n\n      if (offset != null) {\n        offset = tfc.tile(offset, scaleOffsetTiling);\n      }\n\n      return batchNormalization(input, mean, variance, offset, scale, this.epsilon);\n    });\n  }\n\n  getConfig() {\n    const config = {\n      axis: this.axis,\n      epsilon: this.epsilon,\n      center: this.center,\n      scale: this.scale,\n      betaInitializer: serializeInitializer(this.betaInitializer),\n      gammaInitializer: serializeInitializer(this.gammaInitializer),\n      betaRegularizer: serializeRegularizer(this.betaRegularizer),\n      gammaRegularizer: serializeRegularizer(this.gammaRegularizer)\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n}\n/** @nocollapse */\n\nLayerNormalization.className = 'LayerNormalization';\nserialization.registerClass(LayerNormalization);","map":{"version":3,"names":["tfc","moments","reshape","serialization","tidy","util","getConstraint","serializeConstraint","InputSpec","Layer","NotImplementedError","ValueError","getInitializer","serializeInitializer","getRegularizer","serializeRegularizer","generic_utils","math_utils","getExactlyOneShape","getExactlyOneTensor","batchNormalization","x","mean","variance","beta","gamma","epsilon","out","rank","batchNorm2d","batchNorm3d","batchNorm4d","regularNormalizeBatchInTraining","reductionAxes","meanAndVariance","normed","broadcastNormalizeBatchInTraining","targetShape","axis","range","indexOf","push","shape","broadcastMean","broadcastVariance","broadcastGamma","broadcastBeta","normalizeBatchInTraining","arraysEqual","slice","sort","BatchNormalization","constructor","args","supportsMasking","momentum","center","scale","betaInitializer","gammaInitializer","movingMeanInitializer","movingVarianceInitializer","betaConstraint","gammaConstraint","betaRegularizer","gammaRegularizer","build","inputShape","length","dim","JSON","stringify","inputSpec","ndim","axes","addWeight","movingMean","movingVariance","built","call","inputs","kwargs","training","input","splice","broadcastShape","pyListRepeat","sortedReductionAxes","needsBroadcasting","normalizeInference","broadcastMovingMean","read","broadcastMovingVariance","normedTraining","doMovingAverage","variable","value","decay","origValue","updateDelta","mul","sub","write","updateMovingMeanAndVariance","getConfig","config","baseConfig","Object","assign","className","registerClass","LayerNormalization","Number","isInteger","Error","Array","isArray","nDims","i","unique","paramShape","map","trainable","keepDims","broadcast","v","offset","momentsTiling","scaleOffsetTiling","tile"],"sources":["C:/Users/Anke/Documents/Feelix documents/Feelix2.0-dev/Feelix v2/node_modules/@tensorflow/tfjs-layers/dist/layers/normalization.js"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n * Normalization layers.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { moments, reshape, serialization, tidy, util } from '@tensorflow/tfjs-core';\nimport { getConstraint, serializeConstraint } from '../constraints';\nimport { InputSpec, Layer } from '../engine/topology';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { getInitializer, serializeInitializer } from '../initializers';\nimport { getRegularizer, serializeRegularizer } from '../regularizers';\nimport * as generic_utils from '../utils/generic_utils';\nimport * as math_utils from '../utils/math_utils';\nimport { getExactlyOneShape, getExactlyOneTensor } from '../utils/types_utils';\n/**\n * Applies batch normalization on x given mean, var, beta and gamma.\n *\n * I.e. returns:\n *   `output = (x - mean) / (sqrt(var) + epsilon) * gamma + beta`\n *\n * @param x Input tensor.\n * @param mean Mean of batch.\n * @param variance Variance of batch.\n * @param beta Tensor with which to center the input.\n * @param gamma Tensor by which to scale the input.\n * @param epsilon Fuzz factor.\n * @returns The result of the batch normalization.\n */\nexport function batchNormalization(x, mean, variance, beta, gamma, epsilon = 1e-3) {\n    let out;\n    if (x.rank === 2) {\n        out = tfc.batchNorm2d(x, mean, variance, beta, gamma, epsilon);\n    }\n    else if (x.rank === 3) {\n        // TODO(cais): Check rank; give proper error message.\n        out = tfc.batchNorm3d(x, mean, variance, beta, gamma, epsilon);\n    }\n    else if (x.rank === 4) {\n        out = tfc.batchNorm4d(x, mean, variance, beta, gamma, epsilon);\n    }\n    else {\n        throw new NotImplementedError(`batchNormalization is not implemented for array of rank ${x.rank} ` +\n            `yet`);\n    }\n    return out;\n}\n/**\n * Non-broadcasting batch normalization for use in training (not inference).\n *\n * The input is normalized to zero mean and unit variance along the\n * `reductionAxes`, followed by scaling with `gamma` and shifted by `beta`.\n * The result of that is returned as the first element\n * of the returned `Array`. The other two elements are the mean and variance,\n * respectively.\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\nfunction regularNormalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon = 1e-3) {\n    return tidy(() => {\n        const meanAndVariance = tfc.moments(x, reductionAxes);\n        const mean = meanAndVariance.mean;\n        const variance = meanAndVariance.variance;\n        const normed = batchNormalization(x, mean, variance, beta, gamma, epsilon);\n        return [normed, mean, variance];\n    });\n}\n/**\n * Broadcasting batch normalization for use in training (not inference).\n *\n * The input is normalized to zero mean and unit variance along the\n * `reductionAxes`, followed by scaling with `gamma` and shifted by `beta`.\n * The result of that is returned as the first element\n * of the returned `Array`. The other two elements are the mean and variance,\n * respectively.\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\nfunction broadcastNormalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon = 1e-3) {\n    return tidy(() => {\n        const meanAndVariance = tfc.moments(x, reductionAxes);\n        const mean = meanAndVariance.mean;\n        const variance = meanAndVariance.variance;\n        const targetShape = [];\n        for (const axis of math_utils.range(0, x.rank)) {\n            if (reductionAxes.indexOf(axis) !== -1) {\n                targetShape.push(1);\n            }\n            else {\n                targetShape.push(x.shape[axis]);\n            }\n        }\n        const broadcastMean = reshape(mean, targetShape);\n        const broadcastVariance = reshape(variance, targetShape);\n        const broadcastGamma = gamma == null ? null : reshape(gamma, targetShape);\n        const broadcastBeta = beta == null ? null : reshape(beta, targetShape);\n        const normed = batchNormalization(x, broadcastMean, broadcastVariance, broadcastBeta, broadcastGamma, epsilon);\n        return [normed, mean, variance];\n    });\n}\n/**\n * Batch normalization for use in training (not inference).\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\nexport function normalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon = 1e-3) {\n    if (util.arraysEqual(reductionAxes.slice().sort(), math_utils.range(0, x.rank - 1))) {\n        return regularNormalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon);\n    }\n    else {\n        return broadcastNormalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon);\n    }\n}\nexport class BatchNormalization extends Layer {\n    constructor(args) {\n        if (args == null) {\n            args = {};\n        }\n        super(args);\n        this.supportsMasking = true;\n        this.axis = args.axis == null ? -1 : args.axis;\n        this.momentum = args.momentum == null ? 0.99 : args.momentum;\n        this.epsilon = args.epsilon == null ? 1e-3 : args.epsilon;\n        this.center = args.center == null ? true : args.center;\n        this.scale = args.scale == null ? true : args.scale;\n        this.betaInitializer = getInitializer(args.betaInitializer || 'zeros');\n        this.gammaInitializer = getInitializer(args.gammaInitializer || 'ones');\n        this.movingMeanInitializer =\n            getInitializer(args.movingMeanInitializer || 'zeros');\n        this.movingVarianceInitializer =\n            getInitializer(args.movingVarianceInitializer || 'ones');\n        this.betaConstraint = getConstraint(args.betaConstraint);\n        this.gammaConstraint = getConstraint(args.gammaConstraint);\n        this.betaRegularizer = getRegularizer(args.betaRegularizer);\n        this.gammaRegularizer = getRegularizer(args.gammaRegularizer);\n    }\n    build(inputShape) {\n        inputShape = getExactlyOneShape(inputShape);\n        const axis = this.axis >= 0 ? this.axis : (this.axis + inputShape.length);\n        const dim = inputShape[axis];\n        if (dim == null) {\n            throw new ValueError(`Axis ${axis} of input tensor should have a defined dimension but ` +\n                `the layer received an input with shape ` +\n                `${JSON.stringify(inputShape)}.`);\n        }\n        this.inputSpec =\n            [new InputSpec({ ndim: inputShape.length, axes: { [axis]: dim } })];\n        const shape = [dim];\n        if (this.scale) {\n            this.gamma = this.addWeight('gamma', shape, null, this.gammaInitializer, this.gammaRegularizer, true, this.gammaConstraint);\n        }\n        if (this.center) {\n            this.beta = this.addWeight('beta', shape, null, this.betaInitializer, this.betaRegularizer, true, this.betaConstraint);\n        }\n        this.movingMean = this.addWeight('moving_mean', shape, null, this.movingMeanInitializer, null, false);\n        this.movingVariance = this.addWeight('moving_variance', shape, null, this.movingVarianceInitializer, null, false);\n        this.built = true;\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            const training = kwargs['training'] == null ? false : kwargs['training'];\n            const input = getExactlyOneTensor(inputs);\n            const inputShape = input.shape;\n            const ndim = inputShape.length;\n            const reductionAxes = math_utils.range(0, ndim);\n            const axis = this.axis >= 0 ? this.axis : (this.axis + ndim);\n            reductionAxes.splice(axis, 1);\n            const broadcastShape = generic_utils.pyListRepeat(1, ndim);\n            broadcastShape[axis] = inputShape[axis];\n            const sortedReductionAxes = reductionAxes.slice();\n            sortedReductionAxes.sort();\n            const needsBroadcasting = !util.arraysEqual(sortedReductionAxes, math_utils.range(0, ndim).slice(0, ndim - 1));\n            const normalizeInference = () => {\n                if (needsBroadcasting) {\n                    const broadcastMovingMean = reshape(this.movingMean.read(), broadcastShape);\n                    const broadcastMovingVariance = reshape(this.movingVariance.read(), broadcastShape);\n                    const broadcastBeta = this.center ? reshape(this.beta.read(), broadcastShape) : null;\n                    const broadcastGamma = this.scale ? reshape(this.gamma.read(), broadcastShape) : null;\n                    return batchNormalization(input, broadcastMovingMean, broadcastMovingVariance, broadcastBeta, broadcastGamma, this.epsilon);\n                }\n                else {\n                    return batchNormalization(input, this.movingMean.read(), this.movingVariance.read(), this.beta == null ? null : this.beta.read(), this.gamma == null ? null : this.gamma.read(), this.epsilon);\n                }\n            };\n            if (!training) {\n                return normalizeInference();\n            }\n            const [normedTraining, mean, variance] = normalizeBatchInTraining(input, this.gamma.read(), this.beta.read(), reductionAxes, this.epsilon);\n            const doMovingAverage = (variable, value, momentum) => {\n                tfc.tidy(() => {\n                    const decay = 1 - momentum;\n                    const origValue = variable.read();\n                    const updateDelta = tfc.mul(tfc.sub(origValue, value), decay);\n                    variable.write(tfc.sub(origValue, updateDelta));\n                });\n            };\n            // Perform updates to moving mean and moving variance for training.\n            // Porting Note: In PyKeras, these updates to `movingMean` and\n            //   `movingAverage` are done as a deferred Graph, added to the `Layer`'s\n            //   `update`s using the `add_update()` method. Here we do it imperatively\n            //   and encapsulate the updates in a function that is invoked\n            //   immediately.\n            const updateMovingMeanAndVariance = () => {\n                doMovingAverage(this.movingMean, mean, this.momentum);\n                doMovingAverage(this.movingVariance, variance, this.momentum);\n            };\n            updateMovingMeanAndVariance();\n            return normedTraining;\n        });\n    }\n    getConfig() {\n        const config = {\n            axis: this.axis,\n            momentum: this.momentum,\n            epsilon: this.epsilon,\n            center: this.center,\n            scale: this.scale,\n            betaInitializer: serializeInitializer(this.betaInitializer),\n            gammaInitializer: serializeInitializer(this.gammaInitializer),\n            movingMeanInitializer: serializeInitializer(this.movingMeanInitializer),\n            movingVarianceInitializer: serializeInitializer(this.movingVarianceInitializer),\n            betaRegularizer: serializeRegularizer(this.betaRegularizer),\n            gammaRegularizer: serializeRegularizer(this.gammaRegularizer),\n            betaConstraint: serializeConstraint(this.betaConstraint),\n            gammaConstraint: serializeConstraint(this.gammaConstraint)\n        };\n        const baseConfig = super.getConfig();\n        Object.assign(config, baseConfig);\n        return config;\n    }\n}\n/** @nocollapse */\nBatchNormalization.className = 'BatchNormalization';\nserialization.registerClass(BatchNormalization);\nexport class LayerNormalization extends Layer {\n    constructor(args) {\n        if (args == null) {\n            args = {};\n        }\n        super(args);\n        this.axis = args.axis == null ? -1 : args.axis;\n        if (typeof this.axis === 'number') {\n            if (!Number.isInteger(this.axis)) {\n                throw new Error(`Expected axis to be an integer, but received ${this.axis}`);\n            }\n        }\n        else if (Array.isArray(this.axis)) {\n            for (const axis of this.axis) {\n                if (!Number.isInteger(axis)) {\n                    throw new Error(`Expected axis to be an array of integers, ` +\n                        `but received ${JSON.stringify(this.axis)}`);\n                }\n            }\n        }\n        else {\n            throw new Error(`Expected axis to be an integer or an array of integers, ` +\n                `but received ${JSON.stringify(this.axis)}`);\n        }\n        this.epsilon = args.epsilon == null ? 1e-3 : args.epsilon;\n        this.center = args.center == null ? true : args.center;\n        this.scale = args.scale == null ? true : args.scale;\n        this.betaInitializer = getInitializer(args.betaInitializer || 'zeros');\n        this.gammaInitializer = getInitializer(args.gammaInitializer || 'ones');\n        this.betaRegularizer = getRegularizer(args.betaRegularizer);\n        this.gammaRegularizer = getRegularizer(args.gammaRegularizer);\n        this.supportsMasking = true;\n    }\n    build(inputShape) {\n        inputShape = getExactlyOneShape(inputShape);\n        const nDims = inputShape.length;\n        // Convert axis to array and resolve negatives.\n        if (typeof this.axis === 'number') {\n            this.axis = [this.axis];\n        }\n        for (let i = 0; i < this.axis.length; ++i) {\n            if (this.axis[i] < 0) {\n                this.axis[i] += nDims;\n            }\n        }\n        // Further validate axes.\n        for (const axis of this.axis) {\n            if (axis < 0 || axis >= nDims) {\n                throw new Error(`Invalid axis: ${axis}`);\n            }\n        }\n        if (this.axis.length !== generic_utils.unique(this.axis).length) {\n            throw new Error(`Found duplicate axes in: ${this.axis}`);\n        }\n        const paramShape = this.axis.map(axis => inputShape[axis]);\n        const trainable = true;\n        if (this.scale) {\n            this.gamma = this.addWeight('gamma', paramShape, 'float32', this.gammaInitializer, this.gammaRegularizer, trainable);\n        }\n        else {\n            this.gamma = null;\n        }\n        if (this.center) {\n            this.beta = this.addWeight('beta', paramShape, 'float32', this.betaInitializer, this.betaRegularizer, trainable);\n        }\n        else {\n            this.beta = null;\n        }\n        this.built = true;\n    }\n    call(inputs, kwargs) {\n        const input = getExactlyOneTensor(inputs);\n        const inputShape = input.shape;\n        const nDims = inputShape.length;\n        return tidy(() => {\n            const keepDims = true;\n            let { mean, variance } = moments(input, this.axis, keepDims);\n            const broadcastShape = generic_utils.pyListRepeat(1, nDims);\n            for (const dim of this.axis) {\n                broadcastShape[dim] = inputShape[dim];\n            }\n            const broadcast = (v) => {\n                if (v != null && v.shape.length !== nDims) {\n                    return tfc.reshape(v, broadcastShape);\n                }\n                else {\n                    return v;\n                }\n            };\n            let scale = this.scale ? broadcast(this.gamma.read()) : null;\n            let offset = this.center ? broadcast(this.beta.read()) : null;\n            // TODO(https://github.com/tensorflow/tfjs/issues/2120): The tiling below\n            // is a workaround for the limitation of core's batchNormalization?d don't\n            // support broadcasting in their gradients. In addition, the tiling is\n            // necessary to ensure correctness on the browser CPU backend regardless\n            // of forward or backward computation. Remove this workaround once the\n            // limitation is addressed. See .\n            const momentsTiling = [];\n            const scaleOffsetTiling = [];\n            for (let i = 0; i < nDims; ++i) {\n                if (this.axis.indexOf(i) !== -1) {\n                    momentsTiling.push(inputShape[i]);\n                    scaleOffsetTiling.push(1);\n                }\n                else {\n                    momentsTiling.push(1);\n                    scaleOffsetTiling.push(inputShape[i]);\n                }\n            }\n            mean = tfc.tile(mean, momentsTiling);\n            variance = tfc.tile(variance, momentsTiling);\n            if (scale != null) {\n                scale = tfc.tile(scale, scaleOffsetTiling);\n            }\n            if (offset != null) {\n                offset = tfc.tile(offset, scaleOffsetTiling);\n            }\n            return batchNormalization(input, mean, variance, offset, scale, this.epsilon);\n        });\n    }\n    getConfig() {\n        const config = {\n            axis: this.axis,\n            epsilon: this.epsilon,\n            center: this.center,\n            scale: this.scale,\n            betaInitializer: serializeInitializer(this.betaInitializer),\n            gammaInitializer: serializeInitializer(this.gammaInitializer),\n            betaRegularizer: serializeRegularizer(this.betaRegularizer),\n            gammaRegularizer: serializeRegularizer(this.gammaRegularizer)\n        };\n        const baseConfig = super.getConfig();\n        Object.assign(config, baseConfig);\n        return config;\n    }\n}\n/** @nocollapse */\nLayerNormalization.className = 'LayerNormalization';\nserialization.registerClass(LayerNormalization);\n"],"mappings":"AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AACA;AACA;AACA;AACA,OAAO,KAAKA,GAAZ,MAAqB,uBAArB;AACA,SAASC,OAAT,EAAkBC,OAAlB,EAA2BC,aAA3B,EAA0CC,IAA1C,EAAgDC,IAAhD,QAA4D,uBAA5D;AACA,SAASC,aAAT,EAAwBC,mBAAxB,QAAmD,gBAAnD;AACA,SAASC,SAAT,EAAoBC,KAApB,QAAiC,oBAAjC;AACA,SAASC,mBAAT,EAA8BC,UAA9B,QAAgD,WAAhD;AACA,SAASC,cAAT,EAAyBC,oBAAzB,QAAqD,iBAArD;AACA,SAASC,cAAT,EAAyBC,oBAAzB,QAAqD,iBAArD;AACA,OAAO,KAAKC,aAAZ,MAA+B,wBAA/B;AACA,OAAO,KAAKC,UAAZ,MAA4B,qBAA5B;AACA,SAASC,kBAAT,EAA6BC,mBAA7B,QAAwD,sBAAxD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AACA,OAAO,SAASC,kBAAT,CAA4BC,CAA5B,EAA+BC,IAA/B,EAAqCC,QAArC,EAA+CC,IAA/C,EAAqDC,KAArD,EAA4DC,OAAO,GAAG,IAAtE,EAA4E;EAC/E,IAAIC,GAAJ;;EACA,IAAIN,CAAC,CAACO,IAAF,KAAW,CAAf,EAAkB;IACdD,GAAG,GAAG3B,GAAG,CAAC6B,WAAJ,CAAgBR,CAAhB,EAAmBC,IAAnB,EAAyBC,QAAzB,EAAmCC,IAAnC,EAAyCC,KAAzC,EAAgDC,OAAhD,CAAN;EACH,CAFD,MAGK,IAAIL,CAAC,CAACO,IAAF,KAAW,CAAf,EAAkB;IACnB;IACAD,GAAG,GAAG3B,GAAG,CAAC8B,WAAJ,CAAgBT,CAAhB,EAAmBC,IAAnB,EAAyBC,QAAzB,EAAmCC,IAAnC,EAAyCC,KAAzC,EAAgDC,OAAhD,CAAN;EACH,CAHI,MAIA,IAAIL,CAAC,CAACO,IAAF,KAAW,CAAf,EAAkB;IACnBD,GAAG,GAAG3B,GAAG,CAAC+B,WAAJ,CAAgBV,CAAhB,EAAmBC,IAAnB,EAAyBC,QAAzB,EAAmCC,IAAnC,EAAyCC,KAAzC,EAAgDC,OAAhD,CAAN;EACH,CAFI,MAGA;IACD,MAAM,IAAIhB,mBAAJ,CAAyB,2DAA0DW,CAAC,CAACO,IAAK,GAAlE,GACzB,KADC,CAAN;EAEH;;EACD,OAAOD,GAAP;AACH;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AACA,SAASK,+BAAT,CAAyCX,CAAzC,EAA4CI,KAA5C,EAAmDD,IAAnD,EAAyDS,aAAzD,EAAwEP,OAAO,GAAG,IAAlF,EAAwF;EACpF,OAAOtB,IAAI,CAAC,MAAM;IACd,MAAM8B,eAAe,GAAGlC,GAAG,CAACC,OAAJ,CAAYoB,CAAZ,EAAeY,aAAf,CAAxB;IACA,MAAMX,IAAI,GAAGY,eAAe,CAACZ,IAA7B;IACA,MAAMC,QAAQ,GAAGW,eAAe,CAACX,QAAjC;IACA,MAAMY,MAAM,GAAGf,kBAAkB,CAACC,CAAD,EAAIC,IAAJ,EAAUC,QAAV,EAAoBC,IAApB,EAA0BC,KAA1B,EAAiCC,OAAjC,CAAjC;IACA,OAAO,CAACS,MAAD,EAASb,IAAT,EAAeC,QAAf,CAAP;EACH,CANU,CAAX;AAOH;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;AACA,SAASa,iCAAT,CAA2Cf,CAA3C,EAA8CI,KAA9C,EAAqDD,IAArD,EAA2DS,aAA3D,EAA0EP,OAAO,GAAG,IAApF,EAA0F;EACtF,OAAOtB,IAAI,CAAC,MAAM;IACd,MAAM8B,eAAe,GAAGlC,GAAG,CAACC,OAAJ,CAAYoB,CAAZ,EAAeY,aAAf,CAAxB;IACA,MAAMX,IAAI,GAAGY,eAAe,CAACZ,IAA7B;IACA,MAAMC,QAAQ,GAAGW,eAAe,CAACX,QAAjC;IACA,MAAMc,WAAW,GAAG,EAApB;;IACA,KAAK,MAAMC,IAAX,IAAmBrB,UAAU,CAACsB,KAAX,CAAiB,CAAjB,EAAoBlB,CAAC,CAACO,IAAtB,CAAnB,EAAgD;MAC5C,IAAIK,aAAa,CAACO,OAAd,CAAsBF,IAAtB,MAAgC,CAAC,CAArC,EAAwC;QACpCD,WAAW,CAACI,IAAZ,CAAiB,CAAjB;MACH,CAFD,MAGK;QACDJ,WAAW,CAACI,IAAZ,CAAiBpB,CAAC,CAACqB,KAAF,CAAQJ,IAAR,CAAjB;MACH;IACJ;;IACD,MAAMK,aAAa,GAAGzC,OAAO,CAACoB,IAAD,EAAOe,WAAP,CAA7B;IACA,MAAMO,iBAAiB,GAAG1C,OAAO,CAACqB,QAAD,EAAWc,WAAX,CAAjC;IACA,MAAMQ,cAAc,GAAGpB,KAAK,IAAI,IAAT,GAAgB,IAAhB,GAAuBvB,OAAO,CAACuB,KAAD,EAAQY,WAAR,CAArD;IACA,MAAMS,aAAa,GAAGtB,IAAI,IAAI,IAAR,GAAe,IAAf,GAAsBtB,OAAO,CAACsB,IAAD,EAAOa,WAAP,CAAnD;IACA,MAAMF,MAAM,GAAGf,kBAAkB,CAACC,CAAD,EAAIsB,aAAJ,EAAmBC,iBAAnB,EAAsCE,aAAtC,EAAqDD,cAArD,EAAqEnB,OAArE,CAAjC;IACA,OAAO,CAACS,MAAD,EAASb,IAAT,EAAeC,QAAf,CAAP;EACH,CAnBU,CAAX;AAoBH;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;AACA,OAAO,SAASwB,wBAAT,CAAkC1B,CAAlC,EAAqCI,KAArC,EAA4CD,IAA5C,EAAkDS,aAAlD,EAAiEP,OAAO,GAAG,IAA3E,EAAiF;EACpF,IAAIrB,IAAI,CAAC2C,WAAL,CAAiBf,aAAa,CAACgB,KAAd,GAAsBC,IAAtB,EAAjB,EAA+CjC,UAAU,CAACsB,KAAX,CAAiB,CAAjB,EAAoBlB,CAAC,CAACO,IAAF,GAAS,CAA7B,CAA/C,CAAJ,EAAqF;IACjF,OAAOI,+BAA+B,CAACX,CAAD,EAAII,KAAJ,EAAWD,IAAX,EAAiBS,aAAjB,EAAgCP,OAAhC,CAAtC;EACH,CAFD,MAGK;IACD,OAAOU,iCAAiC,CAACf,CAAD,EAAII,KAAJ,EAAWD,IAAX,EAAiBS,aAAjB,EAAgCP,OAAhC,CAAxC;EACH;AACJ;AACD,OAAO,MAAMyB,kBAAN,SAAiC1C,KAAjC,CAAuC;EAC1C2C,WAAW,CAACC,IAAD,EAAO;IACd,IAAIA,IAAI,IAAI,IAAZ,EAAkB;MACdA,IAAI,GAAG,EAAP;IACH;;IACD,MAAMA,IAAN;IACA,KAAKC,eAAL,GAAuB,IAAvB;IACA,KAAKhB,IAAL,GAAYe,IAAI,CAACf,IAAL,IAAa,IAAb,GAAoB,CAAC,CAArB,GAAyBe,IAAI,CAACf,IAA1C;IACA,KAAKiB,QAAL,GAAgBF,IAAI,CAACE,QAAL,IAAiB,IAAjB,GAAwB,IAAxB,GAA+BF,IAAI,CAACE,QAApD;IACA,KAAK7B,OAAL,GAAe2B,IAAI,CAAC3B,OAAL,IAAgB,IAAhB,GAAuB,IAAvB,GAA8B2B,IAAI,CAAC3B,OAAlD;IACA,KAAK8B,MAAL,GAAcH,IAAI,CAACG,MAAL,IAAe,IAAf,GAAsB,IAAtB,GAA6BH,IAAI,CAACG,MAAhD;IACA,KAAKC,KAAL,GAAaJ,IAAI,CAACI,KAAL,IAAc,IAAd,GAAqB,IAArB,GAA4BJ,IAAI,CAACI,KAA9C;IACA,KAAKC,eAAL,GAAuB9C,cAAc,CAACyC,IAAI,CAACK,eAAL,IAAwB,OAAzB,CAArC;IACA,KAAKC,gBAAL,GAAwB/C,cAAc,CAACyC,IAAI,CAACM,gBAAL,IAAyB,MAA1B,CAAtC;IACA,KAAKC,qBAAL,GACIhD,cAAc,CAACyC,IAAI,CAACO,qBAAL,IAA8B,OAA/B,CADlB;IAEA,KAAKC,yBAAL,GACIjD,cAAc,CAACyC,IAAI,CAACQ,yBAAL,IAAkC,MAAnC,CADlB;IAEA,KAAKC,cAAL,GAAsBxD,aAAa,CAAC+C,IAAI,CAACS,cAAN,CAAnC;IACA,KAAKC,eAAL,GAAuBzD,aAAa,CAAC+C,IAAI,CAACU,eAAN,CAApC;IACA,KAAKC,eAAL,GAAuBlD,cAAc,CAACuC,IAAI,CAACW,eAAN,CAArC;IACA,KAAKC,gBAAL,GAAwBnD,cAAc,CAACuC,IAAI,CAACY,gBAAN,CAAtC;EACH;;EACDC,KAAK,CAACC,UAAD,EAAa;IACdA,UAAU,GAAGjD,kBAAkB,CAACiD,UAAD,CAA/B;IACA,MAAM7B,IAAI,GAAG,KAAKA,IAAL,IAAa,CAAb,GAAiB,KAAKA,IAAtB,GAA8B,KAAKA,IAAL,GAAY6B,UAAU,CAACC,MAAlE;IACA,MAAMC,GAAG,GAAGF,UAAU,CAAC7B,IAAD,CAAtB;;IACA,IAAI+B,GAAG,IAAI,IAAX,EAAiB;MACb,MAAM,IAAI1D,UAAJ,CAAgB,QAAO2B,IAAK,uDAAb,GAChB,yCADgB,GAEhB,GAAEgC,IAAI,CAACC,SAAL,CAAeJ,UAAf,CAA2B,GAF5B,CAAN;IAGH;;IACD,KAAKK,SAAL,GACI,CAAC,IAAIhE,SAAJ,CAAc;MAAEiE,IAAI,EAAEN,UAAU,CAACC,MAAnB;MAA2BM,IAAI,EAAE;QAAE,CAACpC,IAAD,GAAQ+B;MAAV;IAAjC,CAAd,CAAD,CADJ;IAEA,MAAM3B,KAAK,GAAG,CAAC2B,GAAD,CAAd;;IACA,IAAI,KAAKZ,KAAT,EAAgB;MACZ,KAAKhC,KAAL,GAAa,KAAKkD,SAAL,CAAe,OAAf,EAAwBjC,KAAxB,EAA+B,IAA/B,EAAqC,KAAKiB,gBAA1C,EAA4D,KAAKM,gBAAjE,EAAmF,IAAnF,EAAyF,KAAKF,eAA9F,CAAb;IACH;;IACD,IAAI,KAAKP,MAAT,EAAiB;MACb,KAAKhC,IAAL,GAAY,KAAKmD,SAAL,CAAe,MAAf,EAAuBjC,KAAvB,EAA8B,IAA9B,EAAoC,KAAKgB,eAAzC,EAA0D,KAAKM,eAA/D,EAAgF,IAAhF,EAAsF,KAAKF,cAA3F,CAAZ;IACH;;IACD,KAAKc,UAAL,GAAkB,KAAKD,SAAL,CAAe,aAAf,EAA8BjC,KAA9B,EAAqC,IAArC,EAA2C,KAAKkB,qBAAhD,EAAuE,IAAvE,EAA6E,KAA7E,CAAlB;IACA,KAAKiB,cAAL,GAAsB,KAAKF,SAAL,CAAe,iBAAf,EAAkCjC,KAAlC,EAAyC,IAAzC,EAA+C,KAAKmB,yBAApD,EAA+E,IAA/E,EAAqF,KAArF,CAAtB;IACA,KAAKiB,KAAL,GAAa,IAAb;EACH;;EACDC,IAAI,CAACC,MAAD,EAASC,MAAT,EAAiB;IACjB,OAAO7E,IAAI,CAAC,MAAM;MACd,MAAM8E,QAAQ,GAAGD,MAAM,CAAC,UAAD,CAAN,IAAsB,IAAtB,GAA6B,KAA7B,GAAqCA,MAAM,CAAC,UAAD,CAA5D;MACA,MAAME,KAAK,GAAGhE,mBAAmB,CAAC6D,MAAD,CAAjC;MACA,MAAMb,UAAU,GAAGgB,KAAK,CAACzC,KAAzB;MACA,MAAM+B,IAAI,GAAGN,UAAU,CAACC,MAAxB;MACA,MAAMnC,aAAa,GAAGhB,UAAU,CAACsB,KAAX,CAAiB,CAAjB,EAAoBkC,IAApB,CAAtB;MACA,MAAMnC,IAAI,GAAG,KAAKA,IAAL,IAAa,CAAb,GAAiB,KAAKA,IAAtB,GAA8B,KAAKA,IAAL,GAAYmC,IAAvD;MACAxC,aAAa,CAACmD,MAAd,CAAqB9C,IAArB,EAA2B,CAA3B;MACA,MAAM+C,cAAc,GAAGrE,aAAa,CAACsE,YAAd,CAA2B,CAA3B,EAA8Bb,IAA9B,CAAvB;MACAY,cAAc,CAAC/C,IAAD,CAAd,GAAuB6B,UAAU,CAAC7B,IAAD,CAAjC;MACA,MAAMiD,mBAAmB,GAAGtD,aAAa,CAACgB,KAAd,EAA5B;MACAsC,mBAAmB,CAACrC,IAApB;MACA,MAAMsC,iBAAiB,GAAG,CAACnF,IAAI,CAAC2C,WAAL,CAAiBuC,mBAAjB,EAAsCtE,UAAU,CAACsB,KAAX,CAAiB,CAAjB,EAAoBkC,IAApB,EAA0BxB,KAA1B,CAAgC,CAAhC,EAAmCwB,IAAI,GAAG,CAA1C,CAAtC,CAA3B;;MACA,MAAMgB,kBAAkB,GAAG,MAAM;QAC7B,IAAID,iBAAJ,EAAuB;UACnB,MAAME,mBAAmB,GAAGxF,OAAO,CAAC,KAAK0E,UAAL,CAAgBe,IAAhB,EAAD,EAAyBN,cAAzB,CAAnC;UACA,MAAMO,uBAAuB,GAAG1F,OAAO,CAAC,KAAK2E,cAAL,CAAoBc,IAApB,EAAD,EAA6BN,cAA7B,CAAvC;UACA,MAAMvC,aAAa,GAAG,KAAKU,MAAL,GAActD,OAAO,CAAC,KAAKsB,IAAL,CAAUmE,IAAV,EAAD,EAAmBN,cAAnB,CAArB,GAA0D,IAAhF;UACA,MAAMxC,cAAc,GAAG,KAAKY,KAAL,GAAavD,OAAO,CAAC,KAAKuB,KAAL,CAAWkE,IAAX,EAAD,EAAoBN,cAApB,CAApB,GAA0D,IAAjF;UACA,OAAOjE,kBAAkB,CAAC+D,KAAD,EAAQO,mBAAR,EAA6BE,uBAA7B,EAAsD9C,aAAtD,EAAqED,cAArE,EAAqF,KAAKnB,OAA1F,CAAzB;QACH,CAND,MAOK;UACD,OAAON,kBAAkB,CAAC+D,KAAD,EAAQ,KAAKP,UAAL,CAAgBe,IAAhB,EAAR,EAAgC,KAAKd,cAAL,CAAoBc,IAApB,EAAhC,EAA4D,KAAKnE,IAAL,IAAa,IAAb,GAAoB,IAApB,GAA2B,KAAKA,IAAL,CAAUmE,IAAV,EAAvF,EAAyG,KAAKlE,KAAL,IAAc,IAAd,GAAqB,IAArB,GAA4B,KAAKA,KAAL,CAAWkE,IAAX,EAArI,EAAwJ,KAAKjE,OAA7J,CAAzB;QACH;MACJ,CAXD;;MAYA,IAAI,CAACwD,QAAL,EAAe;QACX,OAAOO,kBAAkB,EAAzB;MACH;;MACD,MAAM,CAACI,cAAD,EAAiBvE,IAAjB,EAAuBC,QAAvB,IAAmCwB,wBAAwB,CAACoC,KAAD,EAAQ,KAAK1D,KAAL,CAAWkE,IAAX,EAAR,EAA2B,KAAKnE,IAAL,CAAUmE,IAAV,EAA3B,EAA6C1D,aAA7C,EAA4D,KAAKP,OAAjE,CAAjE;;MACA,MAAMoE,eAAe,GAAG,CAACC,QAAD,EAAWC,KAAX,EAAkBzC,QAAlB,KAA+B;QACnDvD,GAAG,CAACI,IAAJ,CAAS,MAAM;UACX,MAAM6F,KAAK,GAAG,IAAI1C,QAAlB;UACA,MAAM2C,SAAS,GAAGH,QAAQ,CAACJ,IAAT,EAAlB;UACA,MAAMQ,WAAW,GAAGnG,GAAG,CAACoG,GAAJ,CAAQpG,GAAG,CAACqG,GAAJ,CAAQH,SAAR,EAAmBF,KAAnB,CAAR,EAAmCC,KAAnC,CAApB;UACAF,QAAQ,CAACO,KAAT,CAAetG,GAAG,CAACqG,GAAJ,CAAQH,SAAR,EAAmBC,WAAnB,CAAf;QACH,CALD;MAMH,CAPD,CA7Bc,CAqCd;MACA;MACA;MACA;MACA;MACA;;;MACA,MAAMI,2BAA2B,GAAG,MAAM;QACtCT,eAAe,CAAC,KAAKlB,UAAN,EAAkBtD,IAAlB,EAAwB,KAAKiC,QAA7B,CAAf;QACAuC,eAAe,CAAC,KAAKjB,cAAN,EAAsBtD,QAAtB,EAAgC,KAAKgC,QAArC,CAAf;MACH,CAHD;;MAIAgD,2BAA2B;MAC3B,OAAOV,cAAP;IACH,CAjDU,CAAX;EAkDH;;EACDW,SAAS,GAAG;IACR,MAAMC,MAAM,GAAG;MACXnE,IAAI,EAAE,KAAKA,IADA;MAEXiB,QAAQ,EAAE,KAAKA,QAFJ;MAGX7B,OAAO,EAAE,KAAKA,OAHH;MAIX8B,MAAM,EAAE,KAAKA,MAJF;MAKXC,KAAK,EAAE,KAAKA,KALD;MAMXC,eAAe,EAAE7C,oBAAoB,CAAC,KAAK6C,eAAN,CAN1B;MAOXC,gBAAgB,EAAE9C,oBAAoB,CAAC,KAAK8C,gBAAN,CAP3B;MAQXC,qBAAqB,EAAE/C,oBAAoB,CAAC,KAAK+C,qBAAN,CARhC;MASXC,yBAAyB,EAAEhD,oBAAoB,CAAC,KAAKgD,yBAAN,CATpC;MAUXG,eAAe,EAAEjD,oBAAoB,CAAC,KAAKiD,eAAN,CAV1B;MAWXC,gBAAgB,EAAElD,oBAAoB,CAAC,KAAKkD,gBAAN,CAX3B;MAYXH,cAAc,EAAEvD,mBAAmB,CAAC,KAAKuD,cAAN,CAZxB;MAaXC,eAAe,EAAExD,mBAAmB,CAAC,KAAKwD,eAAN;IAbzB,CAAf;IAeA,MAAM2C,UAAU,GAAG,MAAMF,SAAN,EAAnB;IACAG,MAAM,CAACC,MAAP,CAAcH,MAAd,EAAsBC,UAAtB;IACA,OAAOD,MAAP;EACH;;AApHyC;AAsH9C;;AACAtD,kBAAkB,CAAC0D,SAAnB,GAA+B,oBAA/B;AACA1G,aAAa,CAAC2G,aAAd,CAA4B3D,kBAA5B;AACA,OAAO,MAAM4D,kBAAN,SAAiCtG,KAAjC,CAAuC;EAC1C2C,WAAW,CAACC,IAAD,EAAO;IACd,IAAIA,IAAI,IAAI,IAAZ,EAAkB;MACdA,IAAI,GAAG,EAAP;IACH;;IACD,MAAMA,IAAN;IACA,KAAKf,IAAL,GAAYe,IAAI,CAACf,IAAL,IAAa,IAAb,GAAoB,CAAC,CAArB,GAAyBe,IAAI,CAACf,IAA1C;;IACA,IAAI,OAAO,KAAKA,IAAZ,KAAqB,QAAzB,EAAmC;MAC/B,IAAI,CAAC0E,MAAM,CAACC,SAAP,CAAiB,KAAK3E,IAAtB,CAAL,EAAkC;QAC9B,MAAM,IAAI4E,KAAJ,CAAW,gDAA+C,KAAK5E,IAAK,EAApE,CAAN;MACH;IACJ,CAJD,MAKK,IAAI6E,KAAK,CAACC,OAAN,CAAc,KAAK9E,IAAnB,CAAJ,EAA8B;MAC/B,KAAK,MAAMA,IAAX,IAAmB,KAAKA,IAAxB,EAA8B;QAC1B,IAAI,CAAC0E,MAAM,CAACC,SAAP,CAAiB3E,IAAjB,CAAL,EAA6B;UACzB,MAAM,IAAI4E,KAAJ,CAAW,4CAAD,GACX,gBAAe5C,IAAI,CAACC,SAAL,CAAe,KAAKjC,IAApB,CAA0B,EADxC,CAAN;QAEH;MACJ;IACJ,CAPI,MAQA;MACD,MAAM,IAAI4E,KAAJ,CAAW,0DAAD,GACX,gBAAe5C,IAAI,CAACC,SAAL,CAAe,KAAKjC,IAApB,CAA0B,EADxC,CAAN;IAEH;;IACD,KAAKZ,OAAL,GAAe2B,IAAI,CAAC3B,OAAL,IAAgB,IAAhB,GAAuB,IAAvB,GAA8B2B,IAAI,CAAC3B,OAAlD;IACA,KAAK8B,MAAL,GAAcH,IAAI,CAACG,MAAL,IAAe,IAAf,GAAsB,IAAtB,GAA6BH,IAAI,CAACG,MAAhD;IACA,KAAKC,KAAL,GAAaJ,IAAI,CAACI,KAAL,IAAc,IAAd,GAAqB,IAArB,GAA4BJ,IAAI,CAACI,KAA9C;IACA,KAAKC,eAAL,GAAuB9C,cAAc,CAACyC,IAAI,CAACK,eAAL,IAAwB,OAAzB,CAArC;IACA,KAAKC,gBAAL,GAAwB/C,cAAc,CAACyC,IAAI,CAACM,gBAAL,IAAyB,MAA1B,CAAtC;IACA,KAAKK,eAAL,GAAuBlD,cAAc,CAACuC,IAAI,CAACW,eAAN,CAArC;IACA,KAAKC,gBAAL,GAAwBnD,cAAc,CAACuC,IAAI,CAACY,gBAAN,CAAtC;IACA,KAAKX,eAAL,GAAuB,IAAvB;EACH;;EACDY,KAAK,CAACC,UAAD,EAAa;IACdA,UAAU,GAAGjD,kBAAkB,CAACiD,UAAD,CAA/B;IACA,MAAMkD,KAAK,GAAGlD,UAAU,CAACC,MAAzB,CAFc,CAGd;;IACA,IAAI,OAAO,KAAK9B,IAAZ,KAAqB,QAAzB,EAAmC;MAC/B,KAAKA,IAAL,GAAY,CAAC,KAAKA,IAAN,CAAZ;IACH;;IACD,KAAK,IAAIgF,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAG,KAAKhF,IAAL,CAAU8B,MAA9B,EAAsC,EAAEkD,CAAxC,EAA2C;MACvC,IAAI,KAAKhF,IAAL,CAAUgF,CAAV,IAAe,CAAnB,EAAsB;QAClB,KAAKhF,IAAL,CAAUgF,CAAV,KAAgBD,KAAhB;MACH;IACJ,CAXa,CAYd;;;IACA,KAAK,MAAM/E,IAAX,IAAmB,KAAKA,IAAxB,EAA8B;MAC1B,IAAIA,IAAI,GAAG,CAAP,IAAYA,IAAI,IAAI+E,KAAxB,EAA+B;QAC3B,MAAM,IAAIH,KAAJ,CAAW,iBAAgB5E,IAAK,EAAhC,CAAN;MACH;IACJ;;IACD,IAAI,KAAKA,IAAL,CAAU8B,MAAV,KAAqBpD,aAAa,CAACuG,MAAd,CAAqB,KAAKjF,IAA1B,EAAgC8B,MAAzD,EAAiE;MAC7D,MAAM,IAAI8C,KAAJ,CAAW,4BAA2B,KAAK5E,IAAK,EAAhD,CAAN;IACH;;IACD,MAAMkF,UAAU,GAAG,KAAKlF,IAAL,CAAUmF,GAAV,CAAcnF,IAAI,IAAI6B,UAAU,CAAC7B,IAAD,CAAhC,CAAnB;IACA,MAAMoF,SAAS,GAAG,IAAlB;;IACA,IAAI,KAAKjE,KAAT,EAAgB;MACZ,KAAKhC,KAAL,GAAa,KAAKkD,SAAL,CAAe,OAAf,EAAwB6C,UAAxB,EAAoC,SAApC,EAA+C,KAAK7D,gBAApD,EAAsE,KAAKM,gBAA3E,EAA6FyD,SAA7F,CAAb;IACH,CAFD,MAGK;MACD,KAAKjG,KAAL,GAAa,IAAb;IACH;;IACD,IAAI,KAAK+B,MAAT,EAAiB;MACb,KAAKhC,IAAL,GAAY,KAAKmD,SAAL,CAAe,MAAf,EAAuB6C,UAAvB,EAAmC,SAAnC,EAA8C,KAAK9D,eAAnD,EAAoE,KAAKM,eAAzE,EAA0F0D,SAA1F,CAAZ;IACH,CAFD,MAGK;MACD,KAAKlG,IAAL,GAAY,IAAZ;IACH;;IACD,KAAKsD,KAAL,GAAa,IAAb;EACH;;EACDC,IAAI,CAACC,MAAD,EAASC,MAAT,EAAiB;IACjB,MAAME,KAAK,GAAGhE,mBAAmB,CAAC6D,MAAD,CAAjC;IACA,MAAMb,UAAU,GAAGgB,KAAK,CAACzC,KAAzB;IACA,MAAM2E,KAAK,GAAGlD,UAAU,CAACC,MAAzB;IACA,OAAOhE,IAAI,CAAC,MAAM;MACd,MAAMuH,QAAQ,GAAG,IAAjB;MACA,IAAI;QAAErG,IAAF;QAAQC;MAAR,IAAqBtB,OAAO,CAACkF,KAAD,EAAQ,KAAK7C,IAAb,EAAmBqF,QAAnB,CAAhC;MACA,MAAMtC,cAAc,GAAGrE,aAAa,CAACsE,YAAd,CAA2B,CAA3B,EAA8B+B,KAA9B,CAAvB;;MACA,KAAK,MAAMhD,GAAX,IAAkB,KAAK/B,IAAvB,EAA6B;QACzB+C,cAAc,CAAChB,GAAD,CAAd,GAAsBF,UAAU,CAACE,GAAD,CAAhC;MACH;;MACD,MAAMuD,SAAS,GAAIC,CAAD,IAAO;QACrB,IAAIA,CAAC,IAAI,IAAL,IAAaA,CAAC,CAACnF,KAAF,CAAQ0B,MAAR,KAAmBiD,KAApC,EAA2C;UACvC,OAAOrH,GAAG,CAACE,OAAJ,CAAY2H,CAAZ,EAAexC,cAAf,CAAP;QACH,CAFD,MAGK;UACD,OAAOwC,CAAP;QACH;MACJ,CAPD;;MAQA,IAAIpE,KAAK,GAAG,KAAKA,KAAL,GAAamE,SAAS,CAAC,KAAKnG,KAAL,CAAWkE,IAAX,EAAD,CAAtB,GAA4C,IAAxD;MACA,IAAImC,MAAM,GAAG,KAAKtE,MAAL,GAAcoE,SAAS,CAAC,KAAKpG,IAAL,CAAUmE,IAAV,EAAD,CAAvB,GAA4C,IAAzD,CAhBc,CAiBd;MACA;MACA;MACA;MACA;MACA;;MACA,MAAMoC,aAAa,GAAG,EAAtB;MACA,MAAMC,iBAAiB,GAAG,EAA1B;;MACA,KAAK,IAAIV,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGD,KAApB,EAA2B,EAAEC,CAA7B,EAAgC;QAC5B,IAAI,KAAKhF,IAAL,CAAUE,OAAV,CAAkB8E,CAAlB,MAAyB,CAAC,CAA9B,EAAiC;UAC7BS,aAAa,CAACtF,IAAd,CAAmB0B,UAAU,CAACmD,CAAD,CAA7B;UACAU,iBAAiB,CAACvF,IAAlB,CAAuB,CAAvB;QACH,CAHD,MAIK;UACDsF,aAAa,CAACtF,IAAd,CAAmB,CAAnB;UACAuF,iBAAiB,CAACvF,IAAlB,CAAuB0B,UAAU,CAACmD,CAAD,CAAjC;QACH;MACJ;;MACDhG,IAAI,GAAGtB,GAAG,CAACiI,IAAJ,CAAS3G,IAAT,EAAeyG,aAAf,CAAP;MACAxG,QAAQ,GAAGvB,GAAG,CAACiI,IAAJ,CAAS1G,QAAT,EAAmBwG,aAAnB,CAAX;;MACA,IAAItE,KAAK,IAAI,IAAb,EAAmB;QACfA,KAAK,GAAGzD,GAAG,CAACiI,IAAJ,CAASxE,KAAT,EAAgBuE,iBAAhB,CAAR;MACH;;MACD,IAAIF,MAAM,IAAI,IAAd,EAAoB;QAChBA,MAAM,GAAG9H,GAAG,CAACiI,IAAJ,CAASH,MAAT,EAAiBE,iBAAjB,CAAT;MACH;;MACD,OAAO5G,kBAAkB,CAAC+D,KAAD,EAAQ7D,IAAR,EAAcC,QAAd,EAAwBuG,MAAxB,EAAgCrE,KAAhC,EAAuC,KAAK/B,OAA5C,CAAzB;IACH,CA5CU,CAAX;EA6CH;;EACD8E,SAAS,GAAG;IACR,MAAMC,MAAM,GAAG;MACXnE,IAAI,EAAE,KAAKA,IADA;MAEXZ,OAAO,EAAE,KAAKA,OAFH;MAGX8B,MAAM,EAAE,KAAKA,MAHF;MAIXC,KAAK,EAAE,KAAKA,KAJD;MAKXC,eAAe,EAAE7C,oBAAoB,CAAC,KAAK6C,eAAN,CAL1B;MAMXC,gBAAgB,EAAE9C,oBAAoB,CAAC,KAAK8C,gBAAN,CAN3B;MAOXK,eAAe,EAAEjD,oBAAoB,CAAC,KAAKiD,eAAN,CAP1B;MAQXC,gBAAgB,EAAElD,oBAAoB,CAAC,KAAKkD,gBAAN;IAR3B,CAAf;IAUA,MAAMyC,UAAU,GAAG,MAAMF,SAAN,EAAnB;IACAG,MAAM,CAACC,MAAP,CAAcH,MAAd,EAAsBC,UAAtB;IACA,OAAOD,MAAP;EACH;;AAtIyC;AAwI9C;;AACAM,kBAAkB,CAACF,SAAnB,GAA+B,oBAA/B;AACA1G,aAAa,CAAC2G,aAAd,CAA4BC,kBAA5B"},"metadata":{},"sourceType":"module"}