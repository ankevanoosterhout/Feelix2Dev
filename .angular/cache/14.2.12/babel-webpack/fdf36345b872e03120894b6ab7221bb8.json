{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * TensorFlow.js Layers: Noise Layers.\n */\nimport { add, greaterEqual, mul, randomUniform, serialization, tidy } from '@tensorflow/tfjs-core';\nimport * as K from '../backend/tfjs_backend';\nimport { Layer } from '../engine/topology';\nimport { getExactlyOneTensor } from '../utils/types_utils';\nexport class GaussianNoise extends Layer {\n  constructor(args) {\n    super(args);\n    this.supportsMasking = true;\n    this.stddev = args.stddev;\n  }\n\n  computeOutputShape(inputShape) {\n    return inputShape;\n  }\n\n  getConfig() {\n    const baseConfig = super.getConfig();\n    const config = {\n      stddev: this.stddev\n    };\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n  call(inputs, kwargs) {\n    return tidy(() => {\n      this.invokeCallHook(inputs, kwargs);\n      const input = getExactlyOneTensor(inputs);\n\n      const noised = () => add(K.randomNormal(input.shape, 0, this.stddev), input);\n\n      const output = K.inTrainPhase(noised, () => input, kwargs['training'] || false);\n      return output;\n    });\n  }\n\n}\n/** @nocollapse */\n\nGaussianNoise.className = 'GaussianNoise';\nserialization.registerClass(GaussianNoise);\nexport class GaussianDropout extends Layer {\n  constructor(args) {\n    super(args);\n    this.supportsMasking = true;\n    this.rate = args.rate;\n  }\n\n  computeOutputShape(inputShape) {\n    return inputShape;\n  }\n\n  getConfig() {\n    const baseConfig = super.getConfig();\n    const config = {\n      rate: this.rate\n    };\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n  call(inputs, kwargs) {\n    return tidy(() => {\n      this.invokeCallHook(inputs, kwargs);\n      const input = getExactlyOneTensor(inputs);\n\n      if (this.rate > 0 && this.rate < 1) {\n        const noised = () => {\n          const stddev = Math.sqrt(this.rate / (1 - this.rate));\n          return mul(input, K.randomNormal(input.shape, 1, stddev));\n        };\n\n        return K.inTrainPhase(noised, () => input, kwargs['training'] || false);\n      }\n\n      return input;\n    });\n  }\n\n}\n/** @nocollapse */\n\nGaussianDropout.className = 'GaussianDropout';\nserialization.registerClass(GaussianDropout);\n/**\n * Applies Alpha Dropout to the input.\n *\n * As it is a regularization layer, it is only active at training time.\n *\n * Alpha Dropout is a `Dropout` that keeps mean and variance of inputs\n * to their original values, in order to ensure the self-normalizing property\n * even after this dropout.\n * Alpha Dropout fits well to Scaled Exponential Linear Units\n * by randomly setting activations to the negative saturation value.\n *\n * Arguments:\n *   - `rate`: float, drop probability (as with `Dropout`).\n *     The multiplicative noise will have\n *     standard deviation `sqrt(rate / (1 - rate))`.\n *   - `noise_shape`: A 1-D `Tensor` of type `int32`, representing the\n *     shape for randomly generated keep/drop flags.\n *\n * Input shape:\n *   Arbitrary. Use the keyword argument `inputShape`\n *   (tuple of integers, does not include the samples axis)\n *   when using this layer as the first layer in a model.\n *\n * Output shape:\n *   Same shape as input.\n *\n * References:\n *   - [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)\n */\n\nexport class AlphaDropout extends Layer {\n  constructor(args) {\n    super(args);\n    this.supportsMasking = true;\n    this.rate = args.rate;\n    this.noiseShape = args.noiseShape;\n  }\n\n  _getNoiseShape(inputs) {\n    return this.noiseShape || getExactlyOneTensor(inputs).shape;\n  }\n\n  computeOutputShape(inputShape) {\n    return inputShape;\n  }\n\n  getConfig() {\n    const baseConfig = super.getConfig();\n    const config = {\n      rate: this.rate\n    };\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n  call(inputs, kwargs) {\n    return tidy(() => {\n      if (this.rate < 1 && this.rate > 0) {\n        const noiseShape = this._getNoiseShape(inputs);\n\n        const droppedInputs = () => {\n          const input = getExactlyOneTensor(inputs);\n          const alpha = 1.6732632423543772848170429916717;\n          const scale = 1.0507009873554804934193349852946;\n          const alphaP = -alpha * scale;\n          let keptIdx = greaterEqual(randomUniform(noiseShape), this.rate);\n          keptIdx = K.cast(keptIdx, 'float32'); // get default dtype.\n          // Get affine transformation params.\n\n          const a = ((1 - this.rate) * (1 + this.rate * alphaP ** 2)) ** -0.5;\n          const b = -a * alphaP * this.rate; // Apply mask.\n\n          const x = add(mul(input, keptIdx), mul(add(keptIdx, -1), alphaP));\n          return add(mul(x, a), b);\n        };\n\n        return K.inTrainPhase(droppedInputs, () => getExactlyOneTensor(inputs), kwargs['training'] || false);\n      }\n\n      return inputs;\n    });\n  }\n\n}\n/** @nocollapse */\n\nAlphaDropout.className = 'AlphaDropout';\nserialization.registerClass(AlphaDropout);","map":{"version":3,"names":["add","greaterEqual","mul","randomUniform","serialization","tidy","K","Layer","getExactlyOneTensor","GaussianNoise","constructor","args","supportsMasking","stddev","computeOutputShape","inputShape","getConfig","baseConfig","config","Object","assign","call","inputs","kwargs","invokeCallHook","input","noised","randomNormal","shape","output","inTrainPhase","className","registerClass","GaussianDropout","rate","Math","sqrt","AlphaDropout","noiseShape","_getNoiseShape","droppedInputs","alpha","scale","alphaP","keptIdx","cast","a","b","x"],"sources":["C:/Users/Anke/Documents/Feelix documents/Feelix2.0-dev/Feelix v2/node_modules/@tensorflow/tfjs-layers/dist/layers/noise.js"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n * TensorFlow.js Layers: Noise Layers.\n */\nimport { add, greaterEqual, mul, randomUniform, serialization, tidy } from '@tensorflow/tfjs-core';\nimport * as K from '../backend/tfjs_backend';\nimport { Layer } from '../engine/topology';\nimport { getExactlyOneTensor } from '../utils/types_utils';\nexport class GaussianNoise extends Layer {\n    constructor(args) {\n        super(args);\n        this.supportsMasking = true;\n        this.stddev = args.stddev;\n    }\n    computeOutputShape(inputShape) {\n        return inputShape;\n    }\n    getConfig() {\n        const baseConfig = super.getConfig();\n        const config = { stddev: this.stddev };\n        Object.assign(config, baseConfig);\n        return config;\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            this.invokeCallHook(inputs, kwargs);\n            const input = getExactlyOneTensor(inputs);\n            const noised = () => add(K.randomNormal(input.shape, 0, this.stddev), input);\n            const output = K.inTrainPhase(noised, () => input, kwargs['training'] || false);\n            return output;\n        });\n    }\n}\n/** @nocollapse */\nGaussianNoise.className = 'GaussianNoise';\nserialization.registerClass(GaussianNoise);\nexport class GaussianDropout extends Layer {\n    constructor(args) {\n        super(args);\n        this.supportsMasking = true;\n        this.rate = args.rate;\n    }\n    computeOutputShape(inputShape) {\n        return inputShape;\n    }\n    getConfig() {\n        const baseConfig = super.getConfig();\n        const config = { rate: this.rate };\n        Object.assign(config, baseConfig);\n        return config;\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            this.invokeCallHook(inputs, kwargs);\n            const input = getExactlyOneTensor(inputs);\n            if (this.rate > 0 && this.rate < 1) {\n                const noised = () => {\n                    const stddev = Math.sqrt(this.rate / (1 - this.rate));\n                    return mul(input, K.randomNormal(input.shape, 1, stddev));\n                };\n                return K.inTrainPhase(noised, () => input, kwargs['training'] || false);\n            }\n            return input;\n        });\n    }\n}\n/** @nocollapse */\nGaussianDropout.className = 'GaussianDropout';\nserialization.registerClass(GaussianDropout);\n/**\n * Applies Alpha Dropout to the input.\n *\n * As it is a regularization layer, it is only active at training time.\n *\n * Alpha Dropout is a `Dropout` that keeps mean and variance of inputs\n * to their original values, in order to ensure the self-normalizing property\n * even after this dropout.\n * Alpha Dropout fits well to Scaled Exponential Linear Units\n * by randomly setting activations to the negative saturation value.\n *\n * Arguments:\n *   - `rate`: float, drop probability (as with `Dropout`).\n *     The multiplicative noise will have\n *     standard deviation `sqrt(rate / (1 - rate))`.\n *   - `noise_shape`: A 1-D `Tensor` of type `int32`, representing the\n *     shape for randomly generated keep/drop flags.\n *\n * Input shape:\n *   Arbitrary. Use the keyword argument `inputShape`\n *   (tuple of integers, does not include the samples axis)\n *   when using this layer as the first layer in a model.\n *\n * Output shape:\n *   Same shape as input.\n *\n * References:\n *   - [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)\n */\nexport class AlphaDropout extends Layer {\n    constructor(args) {\n        super(args);\n        this.supportsMasking = true;\n        this.rate = args.rate;\n        this.noiseShape = args.noiseShape;\n    }\n    _getNoiseShape(inputs) {\n        return this.noiseShape || getExactlyOneTensor(inputs).shape;\n    }\n    computeOutputShape(inputShape) {\n        return inputShape;\n    }\n    getConfig() {\n        const baseConfig = super.getConfig();\n        const config = { rate: this.rate };\n        Object.assign(config, baseConfig);\n        return config;\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            if (this.rate < 1 && this.rate > 0) {\n                const noiseShape = this._getNoiseShape(inputs);\n                const droppedInputs = () => {\n                    const input = getExactlyOneTensor(inputs);\n                    const alpha = 1.6732632423543772848170429916717;\n                    const scale = 1.0507009873554804934193349852946;\n                    const alphaP = -alpha * scale;\n                    let keptIdx = greaterEqual(randomUniform(noiseShape), this.rate);\n                    keptIdx = K.cast(keptIdx, 'float32'); // get default dtype.\n                    // Get affine transformation params.\n                    const a = ((1 - this.rate) * (1 + this.rate * alphaP ** 2)) ** -0.5;\n                    const b = -a * alphaP * this.rate;\n                    // Apply mask.\n                    const x = add(mul(input, keptIdx), mul(add(keptIdx, -1), alphaP));\n                    return add(mul(x, a), b);\n                };\n                return K.inTrainPhase(droppedInputs, () => getExactlyOneTensor(inputs), kwargs['training'] || false);\n            }\n            return inputs;\n        });\n    }\n}\n/** @nocollapse */\nAlphaDropout.className = 'AlphaDropout';\nserialization.registerClass(AlphaDropout);\n"],"mappings":"AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AACA;AACA;AACA;AACA,SAASA,GAAT,EAAcC,YAAd,EAA4BC,GAA5B,EAAiCC,aAAjC,EAAgDC,aAAhD,EAA+DC,IAA/D,QAA2E,uBAA3E;AACA,OAAO,KAAKC,CAAZ,MAAmB,yBAAnB;AACA,SAASC,KAAT,QAAsB,oBAAtB;AACA,SAASC,mBAAT,QAAoC,sBAApC;AACA,OAAO,MAAMC,aAAN,SAA4BF,KAA5B,CAAkC;EACrCG,WAAW,CAACC,IAAD,EAAO;IACd,MAAMA,IAAN;IACA,KAAKC,eAAL,GAAuB,IAAvB;IACA,KAAKC,MAAL,GAAcF,IAAI,CAACE,MAAnB;EACH;;EACDC,kBAAkB,CAACC,UAAD,EAAa;IAC3B,OAAOA,UAAP;EACH;;EACDC,SAAS,GAAG;IACR,MAAMC,UAAU,GAAG,MAAMD,SAAN,EAAnB;IACA,MAAME,MAAM,GAAG;MAAEL,MAAM,EAAE,KAAKA;IAAf,CAAf;IACAM,MAAM,CAACC,MAAP,CAAcF,MAAd,EAAsBD,UAAtB;IACA,OAAOC,MAAP;EACH;;EACDG,IAAI,CAACC,MAAD,EAASC,MAAT,EAAiB;IACjB,OAAOlB,IAAI,CAAC,MAAM;MACd,KAAKmB,cAAL,CAAoBF,MAApB,EAA4BC,MAA5B;MACA,MAAME,KAAK,GAAGjB,mBAAmB,CAACc,MAAD,CAAjC;;MACA,MAAMI,MAAM,GAAG,MAAM1B,GAAG,CAACM,CAAC,CAACqB,YAAF,CAAeF,KAAK,CAACG,KAArB,EAA4B,CAA5B,EAA+B,KAAKf,MAApC,CAAD,EAA8CY,KAA9C,CAAxB;;MACA,MAAMI,MAAM,GAAGvB,CAAC,CAACwB,YAAF,CAAeJ,MAAf,EAAuB,MAAMD,KAA7B,EAAoCF,MAAM,CAAC,UAAD,CAAN,IAAsB,KAA1D,CAAf;MACA,OAAOM,MAAP;IACH,CANU,CAAX;EAOH;;AAvBoC;AAyBzC;;AACApB,aAAa,CAACsB,SAAd,GAA0B,eAA1B;AACA3B,aAAa,CAAC4B,aAAd,CAA4BvB,aAA5B;AACA,OAAO,MAAMwB,eAAN,SAA8B1B,KAA9B,CAAoC;EACvCG,WAAW,CAACC,IAAD,EAAO;IACd,MAAMA,IAAN;IACA,KAAKC,eAAL,GAAuB,IAAvB;IACA,KAAKsB,IAAL,GAAYvB,IAAI,CAACuB,IAAjB;EACH;;EACDpB,kBAAkB,CAACC,UAAD,EAAa;IAC3B,OAAOA,UAAP;EACH;;EACDC,SAAS,GAAG;IACR,MAAMC,UAAU,GAAG,MAAMD,SAAN,EAAnB;IACA,MAAME,MAAM,GAAG;MAAEgB,IAAI,EAAE,KAAKA;IAAb,CAAf;IACAf,MAAM,CAACC,MAAP,CAAcF,MAAd,EAAsBD,UAAtB;IACA,OAAOC,MAAP;EACH;;EACDG,IAAI,CAACC,MAAD,EAASC,MAAT,EAAiB;IACjB,OAAOlB,IAAI,CAAC,MAAM;MACd,KAAKmB,cAAL,CAAoBF,MAApB,EAA4BC,MAA5B;MACA,MAAME,KAAK,GAAGjB,mBAAmB,CAACc,MAAD,CAAjC;;MACA,IAAI,KAAKY,IAAL,GAAY,CAAZ,IAAiB,KAAKA,IAAL,GAAY,CAAjC,EAAoC;QAChC,MAAMR,MAAM,GAAG,MAAM;UACjB,MAAMb,MAAM,GAAGsB,IAAI,CAACC,IAAL,CAAU,KAAKF,IAAL,IAAa,IAAI,KAAKA,IAAtB,CAAV,CAAf;UACA,OAAOhC,GAAG,CAACuB,KAAD,EAAQnB,CAAC,CAACqB,YAAF,CAAeF,KAAK,CAACG,KAArB,EAA4B,CAA5B,EAA+Bf,MAA/B,CAAR,CAAV;QACH,CAHD;;QAIA,OAAOP,CAAC,CAACwB,YAAF,CAAeJ,MAAf,EAAuB,MAAMD,KAA7B,EAAoCF,MAAM,CAAC,UAAD,CAAN,IAAsB,KAA1D,CAAP;MACH;;MACD,OAAOE,KAAP;IACH,CAXU,CAAX;EAYH;;AA5BsC;AA8B3C;;AACAQ,eAAe,CAACF,SAAhB,GAA4B,iBAA5B;AACA3B,aAAa,CAAC4B,aAAd,CAA4BC,eAA5B;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AACA,OAAO,MAAMI,YAAN,SAA2B9B,KAA3B,CAAiC;EACpCG,WAAW,CAACC,IAAD,EAAO;IACd,MAAMA,IAAN;IACA,KAAKC,eAAL,GAAuB,IAAvB;IACA,KAAKsB,IAAL,GAAYvB,IAAI,CAACuB,IAAjB;IACA,KAAKI,UAAL,GAAkB3B,IAAI,CAAC2B,UAAvB;EACH;;EACDC,cAAc,CAACjB,MAAD,EAAS;IACnB,OAAO,KAAKgB,UAAL,IAAmB9B,mBAAmB,CAACc,MAAD,CAAnB,CAA4BM,KAAtD;EACH;;EACDd,kBAAkB,CAACC,UAAD,EAAa;IAC3B,OAAOA,UAAP;EACH;;EACDC,SAAS,GAAG;IACR,MAAMC,UAAU,GAAG,MAAMD,SAAN,EAAnB;IACA,MAAME,MAAM,GAAG;MAAEgB,IAAI,EAAE,KAAKA;IAAb,CAAf;IACAf,MAAM,CAACC,MAAP,CAAcF,MAAd,EAAsBD,UAAtB;IACA,OAAOC,MAAP;EACH;;EACDG,IAAI,CAACC,MAAD,EAASC,MAAT,EAAiB;IACjB,OAAOlB,IAAI,CAAC,MAAM;MACd,IAAI,KAAK6B,IAAL,GAAY,CAAZ,IAAiB,KAAKA,IAAL,GAAY,CAAjC,EAAoC;QAChC,MAAMI,UAAU,GAAG,KAAKC,cAAL,CAAoBjB,MAApB,CAAnB;;QACA,MAAMkB,aAAa,GAAG,MAAM;UACxB,MAAMf,KAAK,GAAGjB,mBAAmB,CAACc,MAAD,CAAjC;UACA,MAAMmB,KAAK,GAAG,iCAAd;UACA,MAAMC,KAAK,GAAG,iCAAd;UACA,MAAMC,MAAM,GAAG,CAACF,KAAD,GAASC,KAAxB;UACA,IAAIE,OAAO,GAAG3C,YAAY,CAACE,aAAa,CAACmC,UAAD,CAAd,EAA4B,KAAKJ,IAAjC,CAA1B;UACAU,OAAO,GAAGtC,CAAC,CAACuC,IAAF,CAAOD,OAAP,EAAgB,SAAhB,CAAV,CANwB,CAMc;UACtC;;UACA,MAAME,CAAC,GAAG,CAAC,CAAC,IAAI,KAAKZ,IAAV,KAAmB,IAAI,KAAKA,IAAL,GAAYS,MAAM,IAAI,CAA7C,CAAD,KAAqD,CAAC,GAAhE;UACA,MAAMI,CAAC,GAAG,CAACD,CAAD,GAAKH,MAAL,GAAc,KAAKT,IAA7B,CATwB,CAUxB;;UACA,MAAMc,CAAC,GAAGhD,GAAG,CAACE,GAAG,CAACuB,KAAD,EAAQmB,OAAR,CAAJ,EAAsB1C,GAAG,CAACF,GAAG,CAAC4C,OAAD,EAAU,CAAC,CAAX,CAAJ,EAAmBD,MAAnB,CAAzB,CAAb;UACA,OAAO3C,GAAG,CAACE,GAAG,CAAC8C,CAAD,EAAIF,CAAJ,CAAJ,EAAYC,CAAZ,CAAV;QACH,CAbD;;QAcA,OAAOzC,CAAC,CAACwB,YAAF,CAAeU,aAAf,EAA8B,MAAMhC,mBAAmB,CAACc,MAAD,CAAvD,EAAiEC,MAAM,CAAC,UAAD,CAAN,IAAsB,KAAvF,CAAP;MACH;;MACD,OAAOD,MAAP;IACH,CApBU,CAAX;EAqBH;;AAzCmC;AA2CxC;;AACAe,YAAY,CAACN,SAAb,GAAyB,cAAzB;AACA3B,aAAa,CAAC4B,aAAd,CAA4BK,YAA5B"},"metadata":{},"sourceType":"module"}