{"ast":null,"code":"import _asyncToGenerator from \"C:/Users/Anke/Documents/Feelix documents/Feelix2.0-dev/Feelix v2/node_modules/@babel/runtime/helpers/esm/asyncToGenerator.js\";\n\n/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { dispose, tidy } from '../globals';\nimport { abs } from '../ops/abs';\nimport { add } from '../ops/add';\nimport { div } from '../ops/div';\nimport { maximum } from '../ops/maximum';\nimport { mul } from '../ops/mul';\nimport { scalar } from '../ops/scalar';\nimport { sub } from '../ops/sub';\nimport { zerosLike } from '../ops/zeros_like';\nimport { registerClass } from '../serialization';\nimport { Optimizer } from './optimizer';\nexport class AdamaxOptimizer extends Optimizer {\n  constructor(learningRate, beta1, beta2, epsilon = null, decay = 0.0) {\n    super();\n    this.learningRate = learningRate;\n    this.beta1 = beta1;\n    this.beta2 = beta2;\n    this.epsilon = epsilon;\n    this.decay = decay;\n    this.accumulatedFirstMoment = [];\n    this.accumulatedWeightedInfNorm = [];\n    tidy(() => {\n      this.iteration = scalar(0).variable();\n      this.accBeta1 = scalar(beta1).variable();\n    });\n\n    if (epsilon == null) {\n      this.epsilon = ENGINE.backend.epsilon();\n    }\n  }\n\n  applyGradients(variableGradients) {\n    const variableNames = Array.isArray(variableGradients) ? variableGradients.map(item => item.name) : Object.keys(variableGradients);\n    tidy(() => {\n      const oneMinusAccBeta1 = sub(1, this.accBeta1);\n      const lr = div(-this.learningRate, add(mul(this.iteration, this.decay), 1));\n      variableNames.forEach((name, i) => {\n        const value = ENGINE.registeredVariables[name];\n        const trainable = false;\n\n        if (this.accumulatedFirstMoment[i] == null) {\n          this.accumulatedFirstMoment[i] = {\n            originalName: `${name}/m`,\n            variable: zerosLike(value).variable(trainable)\n          };\n        }\n\n        if (this.accumulatedWeightedInfNorm[i] == null) {\n          this.accumulatedWeightedInfNorm[i] = {\n            originalName: `${name}/v`,\n            variable: zerosLike(value).variable(trainable)\n          };\n        }\n\n        const gradient = Array.isArray(variableGradients) ? variableGradients[i].tensor : variableGradients[name];\n\n        if (gradient == null) {\n          return;\n        }\n\n        const firstMoment = this.accumulatedFirstMoment[i].variable;\n        const weightedInfNorm = this.accumulatedWeightedInfNorm[i].variable;\n        const newFirstMoment = add(mul(firstMoment, this.beta1), mul(gradient, 1 - this.beta1));\n        const ut0 = mul(weightedInfNorm, this.beta2);\n        const ut1 = abs(gradient);\n        const newWeightedInfNorm = maximum(ut0, ut1);\n        firstMoment.assign(newFirstMoment);\n        weightedInfNorm.assign(newWeightedInfNorm);\n        const newValue = add(mul(div(lr, oneMinusAccBeta1), div(newFirstMoment, add(newWeightedInfNorm, this.epsilon))), value);\n        value.assign(newValue);\n      });\n      this.iteration.assign(add(this.iteration, 1));\n      this.accBeta1.assign(mul(this.accBeta1, this.beta1));\n    });\n    this.incrementIterations();\n  }\n\n  dispose() {\n    this.accBeta1.dispose();\n    this.iteration.dispose();\n\n    if (this.accumulatedFirstMoment != null) {\n      dispose(this.accumulatedFirstMoment.map(v => v.variable));\n    }\n\n    if (this.accumulatedWeightedInfNorm != null) {\n      dispose(this.accumulatedWeightedInfNorm.map(v => v.variable));\n    }\n  }\n\n  getWeights() {\n    return _asyncToGenerator(function* () {\n      throw new Error('getWeights() is not implemented for Adamax yet.');\n    })();\n  }\n\n  setWeights(weightValues) {\n    return _asyncToGenerator(function* () {\n      throw new Error('setWeights() is not implemented for Adamax yet.');\n    })();\n  }\n\n  getConfig() {\n    return {\n      'learningRate': this.learningRate,\n      'beta1': this.beta1,\n      'beta2': this.beta2,\n      'epsilon': this.epsilon,\n      'decay': this.decay\n    };\n  }\n  /** @nocollapse */\n\n\n  static fromConfig(cls, config) {\n    return new cls(config['learningRate'], config['beta1'], config['beta2'], config['epsilon'], config['decay']);\n  }\n\n}\n/** @nocollapse */\n\nAdamaxOptimizer.className = 'Adamax'; // Note: Name matters for Python compatbility.\n\nregisterClass(AdamaxOptimizer);","map":{"version":3,"names":["ENGINE","dispose","tidy","abs","add","div","maximum","mul","scalar","sub","zerosLike","registerClass","Optimizer","AdamaxOptimizer","constructor","learningRate","beta1","beta2","epsilon","decay","accumulatedFirstMoment","accumulatedWeightedInfNorm","iteration","variable","accBeta1","backend","applyGradients","variableGradients","variableNames","Array","isArray","map","item","name","Object","keys","oneMinusAccBeta1","lr","forEach","i","value","registeredVariables","trainable","originalName","gradient","tensor","firstMoment","weightedInfNorm","newFirstMoment","ut0","ut1","newWeightedInfNorm","assign","newValue","incrementIterations","v","getWeights","Error","setWeights","weightValues","getConfig","fromConfig","cls","config","className"],"sources":["C:/Users/Anke/Documents/Feelix documents/Feelix2.0-dev/Feelix v2/node_modules/@tensorflow/tfjs-core/dist/optimizers/adamax_optimizer.js"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { dispose, tidy } from '../globals';\nimport { abs } from '../ops/abs';\nimport { add } from '../ops/add';\nimport { div } from '../ops/div';\nimport { maximum } from '../ops/maximum';\nimport { mul } from '../ops/mul';\nimport { scalar } from '../ops/scalar';\nimport { sub } from '../ops/sub';\nimport { zerosLike } from '../ops/zeros_like';\nimport { registerClass } from '../serialization';\nimport { Optimizer } from './optimizer';\nexport class AdamaxOptimizer extends Optimizer {\n    constructor(learningRate, beta1, beta2, epsilon = null, decay = 0.0) {\n        super();\n        this.learningRate = learningRate;\n        this.beta1 = beta1;\n        this.beta2 = beta2;\n        this.epsilon = epsilon;\n        this.decay = decay;\n        this.accumulatedFirstMoment = [];\n        this.accumulatedWeightedInfNorm = [];\n        tidy(() => {\n            this.iteration = scalar(0).variable();\n            this.accBeta1 = scalar(beta1).variable();\n        });\n        if (epsilon == null) {\n            this.epsilon = ENGINE.backend.epsilon();\n        }\n    }\n    applyGradients(variableGradients) {\n        const variableNames = Array.isArray(variableGradients) ?\n            variableGradients.map(item => item.name) :\n            Object.keys(variableGradients);\n        tidy(() => {\n            const oneMinusAccBeta1 = sub(1, this.accBeta1);\n            const lr = div(-this.learningRate, add(mul(this.iteration, this.decay), 1));\n            variableNames.forEach((name, i) => {\n                const value = ENGINE.registeredVariables[name];\n                const trainable = false;\n                if (this.accumulatedFirstMoment[i] == null) {\n                    this.accumulatedFirstMoment[i] = {\n                        originalName: `${name}/m`,\n                        variable: zerosLike(value).variable(trainable)\n                    };\n                }\n                if (this.accumulatedWeightedInfNorm[i] == null) {\n                    this.accumulatedWeightedInfNorm[i] = {\n                        originalName: `${name}/v`,\n                        variable: zerosLike(value).variable(trainable)\n                    };\n                }\n                const gradient = Array.isArray(variableGradients) ?\n                    variableGradients[i].tensor :\n                    variableGradients[name];\n                if (gradient == null) {\n                    return;\n                }\n                const firstMoment = this.accumulatedFirstMoment[i].variable;\n                const weightedInfNorm = this.accumulatedWeightedInfNorm[i].variable;\n                const newFirstMoment = add(mul(firstMoment, this.beta1), mul(gradient, 1 - this.beta1));\n                const ut0 = mul(weightedInfNorm, this.beta2);\n                const ut1 = abs(gradient);\n                const newWeightedInfNorm = maximum(ut0, ut1);\n                firstMoment.assign(newFirstMoment);\n                weightedInfNorm.assign(newWeightedInfNorm);\n                const newValue = add(mul(div(lr, oneMinusAccBeta1), div(newFirstMoment, add(newWeightedInfNorm, this.epsilon))), value);\n                value.assign(newValue);\n            });\n            this.iteration.assign(add(this.iteration, 1));\n            this.accBeta1.assign(mul(this.accBeta1, this.beta1));\n        });\n        this.incrementIterations();\n    }\n    dispose() {\n        this.accBeta1.dispose();\n        this.iteration.dispose();\n        if (this.accumulatedFirstMoment != null) {\n            dispose(this.accumulatedFirstMoment.map(v => v.variable));\n        }\n        if (this.accumulatedWeightedInfNorm != null) {\n            dispose(this.accumulatedWeightedInfNorm.map(v => v.variable));\n        }\n    }\n    async getWeights() {\n        throw new Error('getWeights() is not implemented for Adamax yet.');\n    }\n    async setWeights(weightValues) {\n        throw new Error('setWeights() is not implemented for Adamax yet.');\n    }\n    getConfig() {\n        return {\n            'learningRate': this.learningRate,\n            'beta1': this.beta1,\n            'beta2': this.beta2,\n            'epsilon': this.epsilon,\n            'decay': this.decay\n        };\n    }\n    /** @nocollapse */\n    static fromConfig(cls, config) {\n        return new cls(config['learningRate'], config['beta1'], config['beta2'], config['epsilon'], config['decay']);\n    }\n}\n/** @nocollapse */\nAdamaxOptimizer.className = 'Adamax'; // Note: Name matters for Python compatbility.\nregisterClass(AdamaxOptimizer);\n"],"mappings":";;AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAASA,MAAT,QAAuB,WAAvB;AACA,SAASC,OAAT,EAAkBC,IAAlB,QAA8B,YAA9B;AACA,SAASC,GAAT,QAAoB,YAApB;AACA,SAASC,GAAT,QAAoB,YAApB;AACA,SAASC,GAAT,QAAoB,YAApB;AACA,SAASC,OAAT,QAAwB,gBAAxB;AACA,SAASC,GAAT,QAAoB,YAApB;AACA,SAASC,MAAT,QAAuB,eAAvB;AACA,SAASC,GAAT,QAAoB,YAApB;AACA,SAASC,SAAT,QAA0B,mBAA1B;AACA,SAASC,aAAT,QAA8B,kBAA9B;AACA,SAASC,SAAT,QAA0B,aAA1B;AACA,OAAO,MAAMC,eAAN,SAA8BD,SAA9B,CAAwC;EAC3CE,WAAW,CAACC,YAAD,EAAeC,KAAf,EAAsBC,KAAtB,EAA6BC,OAAO,GAAG,IAAvC,EAA6CC,KAAK,GAAG,GAArD,EAA0D;IACjE;IACA,KAAKJ,YAAL,GAAoBA,YAApB;IACA,KAAKC,KAAL,GAAaA,KAAb;IACA,KAAKC,KAAL,GAAaA,KAAb;IACA,KAAKC,OAAL,GAAeA,OAAf;IACA,KAAKC,KAAL,GAAaA,KAAb;IACA,KAAKC,sBAAL,GAA8B,EAA9B;IACA,KAAKC,0BAAL,GAAkC,EAAlC;IACAnB,IAAI,CAAC,MAAM;MACP,KAAKoB,SAAL,GAAiBd,MAAM,CAAC,CAAD,CAAN,CAAUe,QAAV,EAAjB;MACA,KAAKC,QAAL,GAAgBhB,MAAM,CAACQ,KAAD,CAAN,CAAcO,QAAd,EAAhB;IACH,CAHG,CAAJ;;IAIA,IAAIL,OAAO,IAAI,IAAf,EAAqB;MACjB,KAAKA,OAAL,GAAelB,MAAM,CAACyB,OAAP,CAAeP,OAAf,EAAf;IACH;EACJ;;EACDQ,cAAc,CAACC,iBAAD,EAAoB;IAC9B,MAAMC,aAAa,GAAGC,KAAK,CAACC,OAAN,CAAcH,iBAAd,IAClBA,iBAAiB,CAACI,GAAlB,CAAsBC,IAAI,IAAIA,IAAI,CAACC,IAAnC,CADkB,GAElBC,MAAM,CAACC,IAAP,CAAYR,iBAAZ,CAFJ;IAGAzB,IAAI,CAAC,MAAM;MACP,MAAMkC,gBAAgB,GAAG3B,GAAG,CAAC,CAAD,EAAI,KAAKe,QAAT,CAA5B;MACA,MAAMa,EAAE,GAAGhC,GAAG,CAAC,CAAC,KAAKU,YAAP,EAAqBX,GAAG,CAACG,GAAG,CAAC,KAAKe,SAAN,EAAiB,KAAKH,KAAtB,CAAJ,EAAkC,CAAlC,CAAxB,CAAd;MACAS,aAAa,CAACU,OAAd,CAAsB,CAACL,IAAD,EAAOM,CAAP,KAAa;QAC/B,MAAMC,KAAK,GAAGxC,MAAM,CAACyC,mBAAP,CAA2BR,IAA3B,CAAd;QACA,MAAMS,SAAS,GAAG,KAAlB;;QACA,IAAI,KAAKtB,sBAAL,CAA4BmB,CAA5B,KAAkC,IAAtC,EAA4C;UACxC,KAAKnB,sBAAL,CAA4BmB,CAA5B,IAAiC;YAC7BI,YAAY,EAAG,GAAEV,IAAK,IADO;YAE7BV,QAAQ,EAAEb,SAAS,CAAC8B,KAAD,CAAT,CAAiBjB,QAAjB,CAA0BmB,SAA1B;UAFmB,CAAjC;QAIH;;QACD,IAAI,KAAKrB,0BAAL,CAAgCkB,CAAhC,KAAsC,IAA1C,EAAgD;UAC5C,KAAKlB,0BAAL,CAAgCkB,CAAhC,IAAqC;YACjCI,YAAY,EAAG,GAAEV,IAAK,IADW;YAEjCV,QAAQ,EAAEb,SAAS,CAAC8B,KAAD,CAAT,CAAiBjB,QAAjB,CAA0BmB,SAA1B;UAFuB,CAArC;QAIH;;QACD,MAAME,QAAQ,GAAGf,KAAK,CAACC,OAAN,CAAcH,iBAAd,IACbA,iBAAiB,CAACY,CAAD,CAAjB,CAAqBM,MADR,GAEblB,iBAAiB,CAACM,IAAD,CAFrB;;QAGA,IAAIW,QAAQ,IAAI,IAAhB,EAAsB;UAClB;QACH;;QACD,MAAME,WAAW,GAAG,KAAK1B,sBAAL,CAA4BmB,CAA5B,EAA+BhB,QAAnD;QACA,MAAMwB,eAAe,GAAG,KAAK1B,0BAAL,CAAgCkB,CAAhC,EAAmChB,QAA3D;QACA,MAAMyB,cAAc,GAAG5C,GAAG,CAACG,GAAG,CAACuC,WAAD,EAAc,KAAK9B,KAAnB,CAAJ,EAA+BT,GAAG,CAACqC,QAAD,EAAW,IAAI,KAAK5B,KAApB,CAAlC,CAA1B;QACA,MAAMiC,GAAG,GAAG1C,GAAG,CAACwC,eAAD,EAAkB,KAAK9B,KAAvB,CAAf;QACA,MAAMiC,GAAG,GAAG/C,GAAG,CAACyC,QAAD,CAAf;QACA,MAAMO,kBAAkB,GAAG7C,OAAO,CAAC2C,GAAD,EAAMC,GAAN,CAAlC;QACAJ,WAAW,CAACM,MAAZ,CAAmBJ,cAAnB;QACAD,eAAe,CAACK,MAAhB,CAAuBD,kBAAvB;QACA,MAAME,QAAQ,GAAGjD,GAAG,CAACG,GAAG,CAACF,GAAG,CAACgC,EAAD,EAAKD,gBAAL,CAAJ,EAA4B/B,GAAG,CAAC2C,cAAD,EAAiB5C,GAAG,CAAC+C,kBAAD,EAAqB,KAAKjC,OAA1B,CAApB,CAA/B,CAAJ,EAA6FsB,KAA7F,CAApB;QACAA,KAAK,CAACY,MAAN,CAAaC,QAAb;MACH,CA/BD;MAgCA,KAAK/B,SAAL,CAAe8B,MAAf,CAAsBhD,GAAG,CAAC,KAAKkB,SAAN,EAAiB,CAAjB,CAAzB;MACA,KAAKE,QAAL,CAAc4B,MAAd,CAAqB7C,GAAG,CAAC,KAAKiB,QAAN,EAAgB,KAAKR,KAArB,CAAxB;IACH,CArCG,CAAJ;IAsCA,KAAKsC,mBAAL;EACH;;EACDrD,OAAO,GAAG;IACN,KAAKuB,QAAL,CAAcvB,OAAd;IACA,KAAKqB,SAAL,CAAerB,OAAf;;IACA,IAAI,KAAKmB,sBAAL,IAA+B,IAAnC,EAAyC;MACrCnB,OAAO,CAAC,KAAKmB,sBAAL,CAA4BW,GAA5B,CAAgCwB,CAAC,IAAIA,CAAC,CAAChC,QAAvC,CAAD,CAAP;IACH;;IACD,IAAI,KAAKF,0BAAL,IAAmC,IAAvC,EAA6C;MACzCpB,OAAO,CAAC,KAAKoB,0BAAL,CAAgCU,GAAhC,CAAoCwB,CAAC,IAAIA,CAAC,CAAChC,QAA3C,CAAD,CAAP;IACH;EACJ;;EACKiC,UAAU,GAAG;IAAA;MACf,MAAM,IAAIC,KAAJ,CAAU,iDAAV,CAAN;IADe;EAElB;;EACKC,UAAU,CAACC,YAAD,EAAe;IAAA;MAC3B,MAAM,IAAIF,KAAJ,CAAU,iDAAV,CAAN;IAD2B;EAE9B;;EACDG,SAAS,GAAG;IACR,OAAO;MACH,gBAAgB,KAAK7C,YADlB;MAEH,SAAS,KAAKC,KAFX;MAGH,SAAS,KAAKC,KAHX;MAIH,WAAW,KAAKC,OAJb;MAKH,SAAS,KAAKC;IALX,CAAP;EAOH;EACD;;;EACiB,OAAV0C,UAAU,CAACC,GAAD,EAAMC,MAAN,EAAc;IAC3B,OAAO,IAAID,GAAJ,CAAQC,MAAM,CAAC,cAAD,CAAd,EAAgCA,MAAM,CAAC,OAAD,CAAtC,EAAiDA,MAAM,CAAC,OAAD,CAAvD,EAAkEA,MAAM,CAAC,SAAD,CAAxE,EAAqFA,MAAM,CAAC,OAAD,CAA3F,CAAP;EACH;;AA1F0C;AA4F/C;;AACAlD,eAAe,CAACmD,SAAhB,GAA4B,QAA5B,C,CAAsC;;AACtCrD,aAAa,CAACE,eAAD,CAAb"},"metadata":{},"sourceType":"module"}