{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * Layers that augment the functionality of a base layer.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { serialization, tidy } from '@tensorflow/tfjs-core';\nimport * as K from '../backend/tfjs_backend';\nimport { nameScope } from '../common';\nimport { InputSpec, Layer, SymbolicTensor } from '../engine/topology';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { VALID_BIDIRECTIONAL_MERGE_MODES } from '../keras_format/common';\nimport * as generic_utils from '../utils/generic_utils';\nimport { getExactlyOneShape, getExactlyOneTensor } from '../utils/types_utils';\nimport { rnn, standardizeArgs } from './recurrent';\nimport { deserialize } from './serialization';\n/**\n * Abstract wrapper base class.\n *\n * Wrappers take another layer and augment it in various ways.\n * Do not use this class as a layer, it is only an abstract base class.\n * Two usable wrappers are the `TimeDistributed` and `Bidirectional` wrappers.\n */\n\nexport class Wrapper extends Layer {\n  constructor(args) {\n    // Porting Note: In PyKeras, `self.layer` is set prior to the calling\n    //   `super()`. But we can't do that here due to TypeScript's restriction.\n    //   See: https://github.com/Microsoft/TypeScript/issues/8277\n    //   As a result, we have to add checks in `get trainable()` and\n    //   `set trainable()` below in order to prevent using `this.layer` when\n    //   its value is `undefined`. The super constructor does use the getter\n    //   and the setter of `this.layer`.\n    super(args);\n    this.layer = args.layer;\n  }\n\n  build(inputShape) {\n    this.built = true;\n  } // TODO(cais): Implement activityRegularizer getter.\n\n\n  get trainable() {\n    // Porting Note: the check of `this.layer` here is necessary due to the\n    //   way the `constructor` of this class is written (see Porting Note\n    //   above).\n    if (this.layer != null) {\n      return this.layer.trainable;\n    } else {\n      return false;\n    }\n  }\n\n  set trainable(value) {\n    // Porting Note: the check of `this.layer` here is necessary due to the\n    //   way the `constructor` of this class is written (see Porting Note\n    //   above).\n    if (this.layer != null) {\n      this.layer.trainable = value;\n    }\n  }\n\n  get trainableWeights() {\n    return this.layer.trainableWeights;\n  } // TODO(cais): Implement setter for trainableWeights.\n\n\n  get nonTrainableWeights() {\n    return this.layer.nonTrainableWeights;\n  } // TODO(cais): Implement setter for nonTrainableWeights.\n\n\n  get updates() {\n    // tslint:disable-next-line:no-any\n    return this.layer._updates;\n  } // TODO(cais): Implement getUpdatesFor().\n\n\n  get losses() {\n    return this.layer.losses;\n  } // TODO(cais): Implement getLossesFor().\n\n\n  getWeights() {\n    return this.layer.getWeights();\n  }\n\n  setWeights(weights) {\n    this.layer.setWeights(weights);\n  }\n\n  getConfig() {\n    const config = {\n      'layer': {\n        'className': this.layer.getClassName(),\n        'config': this.layer.getConfig()\n      }\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n  setFastWeightInitDuringBuild(value) {\n    super.setFastWeightInitDuringBuild(value);\n\n    if (this.layer != null) {\n      this.layer.setFastWeightInitDuringBuild(value);\n    }\n  }\n  /** @nocollapse */\n\n\n  static fromConfig(cls, config, customObjects = {}) {\n    const layerConfig = config['layer'];\n    const layer = deserialize(layerConfig, customObjects);\n    delete config['layer'];\n    const newConfig = {\n      layer\n    };\n    Object.assign(newConfig, config);\n    return new cls(newConfig);\n  }\n\n}\nexport class TimeDistributed extends Wrapper {\n  constructor(args) {\n    super(args);\n    this.supportsMasking = true;\n  }\n\n  build(inputShape) {\n    inputShape = getExactlyOneShape(inputShape);\n\n    if (inputShape.length < 3) {\n      throw new ValueError(`TimeDistributed layer expects an input shape >= 3D, but received ` + `input shape ${JSON.stringify(inputShape)}`);\n    }\n\n    this.inputSpec = [{\n      shape: inputShape\n    }];\n    const childInputShape = [inputShape[0]].concat(inputShape.slice(2));\n\n    if (!this.layer.built) {\n      this.layer.build(childInputShape);\n      this.layer.built = true;\n    }\n\n    super.build(inputShape);\n  }\n\n  computeOutputShape(inputShape) {\n    inputShape = getExactlyOneShape(inputShape);\n    const childInputShape = [inputShape[0]].concat(inputShape.slice(2));\n    const childOutputShape = this.layer.computeOutputShape(childInputShape);\n    const timesteps = inputShape[1];\n    return [childOutputShape[0], timesteps].concat(childOutputShape.slice(1));\n  }\n\n  call(inputs, kwargs) {\n    return tidy(() => {\n      // TODO(cais): Add 'training' and 'useLearningPhase' to kwargs.\n      inputs = getExactlyOneTensor(inputs); // Porting Note: In tfjs-layers, `inputs` are always concrete tensor\n      // values. Hence the inputs can't have an undetermined first (batch)\n      // dimension, which is why we always use the K.rnn approach here.\n\n      const step = (inputs, states) => {\n        // TODO(cais): Add useLearningPhase.\n        // NOTE(cais): `layer.call` may return a length-1 array of Tensor in\n        //   some cases (e.g., `layer` is a `Sequential` instance), which is\n        //   why `getExactlyOneTensor` is used below.\n        const output = getExactlyOneTensor(this.layer.call(inputs, kwargs));\n        return [output, []];\n      };\n\n      const rnnOutputs = rnn(step, inputs, [], false\n      /* goBackwards */\n      , null\n      /* mask */\n      , null\n      /* constants */\n      , false\n      /* unroll */\n      , true\n      /* needPerStepOutputs */\n      );\n      const y = rnnOutputs[1]; // TODO(cais): Add activity regularization.\n      // TODO(cais): Add useLearningPhase.\n\n      return y;\n    });\n  }\n\n}\n/** @nocollapse */\n\nTimeDistributed.className = 'TimeDistributed';\nserialization.registerClass(TimeDistributed);\nexport function checkBidirectionalMergeMode(value) {\n  generic_utils.checkStringTypeUnionValue(VALID_BIDIRECTIONAL_MERGE_MODES, 'BidirectionalMergeMode', value);\n}\nconst DEFAULT_BIDIRECTIONAL_MERGE_MODE = 'concat';\nexport class Bidirectional extends Wrapper {\n  constructor(args) {\n    super(args); // Note: When creating `this.forwardLayer`, the original Layer object\n    //   (`config.layer`) ought to be cloned. This is why we call\n    //   `getConfig()` followed by `deserialize()`. Without this cloning,\n    //   the layer names saved during serialization will incorrectly contain\n    //   the 'forward_' prefix. In Python Keras, this is done using\n    //   `copy.copy` (shallow copy), which does not have a simple equivalent\n    //   in JavaScript. JavaScript's `Object.assign()` does not copy\n    //   methods.\n\n    const layerConfig = args.layer.getConfig();\n    const forwDict = {};\n    forwDict['className'] = args.layer.getClassName();\n    forwDict['config'] = layerConfig;\n    this.forwardLayer = deserialize(forwDict);\n    layerConfig['goBackwards'] = layerConfig['goBackwards'] === true ? false : true;\n    const backDict = {};\n    backDict['className'] = args.layer.getClassName();\n    backDict['config'] = layerConfig;\n    this.backwardLayer = deserialize(backDict);\n    this.forwardLayer.name = 'forward_' + this.forwardLayer.name;\n    this.backwardLayer.name = 'backward_' + this.backwardLayer.name;\n    this.mergeMode = args.mergeMode === undefined ? DEFAULT_BIDIRECTIONAL_MERGE_MODE : args.mergeMode;\n    checkBidirectionalMergeMode(this.mergeMode);\n\n    if (args.weights) {\n      throw new NotImplementedError('weights support is not implemented for Bidirectional layer yet.');\n    }\n\n    this._stateful = args.layer.stateful;\n    this.returnSequences = args.layer.returnSequences;\n    this.returnState = args.layer.returnState;\n    this.supportsMasking = true;\n    this._trainable = true;\n    this.inputSpec = args.layer.inputSpec;\n    this.numConstants = null;\n  }\n\n  get trainable() {\n    return this._trainable;\n  }\n\n  set trainable(value) {\n    // Porting Note: the check of `this.layer` here is necessary due to the\n    //   way the `constructor` of this class is written (see Porting Note\n    //   above).\n    this._trainable = value;\n\n    if (this.forwardLayer != null) {\n      this.forwardLayer.trainable = value;\n    }\n\n    if (this.backwardLayer != null) {\n      this.backwardLayer.trainable = value;\n    }\n  }\n\n  getWeights() {\n    return this.forwardLayer.getWeights().concat(this.backwardLayer.getWeights());\n  }\n\n  setWeights(weights) {\n    const numWeights = weights.length;\n    const numeightsOver2 = Math.floor(numWeights / 2);\n    this.forwardLayer.setWeights(weights.slice(0, numeightsOver2));\n    this.backwardLayer.setWeights(weights.slice(numeightsOver2));\n  }\n\n  computeOutputShape(inputShape) {\n    let layerShapes = this.forwardLayer.computeOutputShape(inputShape);\n\n    if (!(Array.isArray(layerShapes) && Array.isArray(layerShapes[0]))) {\n      layerShapes = [layerShapes];\n    }\n\n    layerShapes = layerShapes;\n    let outputShape;\n    let outputShapes;\n    let stateShape;\n\n    if (this.returnState) {\n      stateShape = layerShapes.slice(1);\n      outputShape = layerShapes[0];\n    } else {\n      outputShape = layerShapes[0];\n    }\n\n    outputShape = outputShape;\n\n    if (this.mergeMode === 'concat') {\n      outputShape[outputShape.length - 1] *= 2;\n      outputShapes = [outputShape];\n    } else if (this.mergeMode == null) {\n      outputShapes = [outputShape, outputShape.slice()];\n    } else {\n      outputShapes = [outputShape];\n    }\n\n    if (this.returnState) {\n      if (this.mergeMode == null) {\n        return outputShapes.concat(stateShape).concat(stateShape.slice());\n      }\n\n      return [outputShape].concat(stateShape).concat(stateShape.slice());\n    }\n\n    return generic_utils.singletonOrArray(outputShapes);\n  }\n\n  apply(inputs, kwargs) {\n    let initialState = kwargs == null ? null : kwargs['initialState'];\n    let constants = kwargs == null ? null : kwargs['constants'];\n\n    if (kwargs == null) {\n      kwargs = {};\n    }\n\n    const standardized = standardizeArgs(inputs, initialState, constants, this.numConstants);\n    inputs = standardized.inputs;\n    initialState = standardized.initialState;\n    constants = standardized.constants;\n\n    if (Array.isArray(inputs)) {\n      initialState = inputs.slice(1);\n      inputs = inputs[0];\n    }\n\n    if ((initialState == null || initialState.length === 0) && constants == null) {\n      return super.apply(inputs, kwargs);\n    }\n\n    const additionalInputs = [];\n    const additionalSpecs = [];\n\n    if (initialState != null) {\n      const numStates = initialState.length;\n\n      if (numStates % 2 > 0) {\n        throw new ValueError('When passing `initialState` to a Bidrectional RNN, ' + 'the state should be an Array containing the states of ' + 'the underlying RNNs.');\n      }\n\n      kwargs['initialState'] = initialState;\n      additionalInputs.push(...initialState);\n      const stateSpecs = initialState.map(state => new InputSpec({\n        shape: state.shape\n      }));\n      this.forwardLayer.stateSpec = stateSpecs.slice(0, numStates / 2);\n      this.backwardLayer.stateSpec = stateSpecs.slice(numStates / 2);\n      additionalSpecs.push(...stateSpecs);\n    }\n\n    if (constants != null) {\n      throw new NotImplementedError('Support for constants in Bidirectional layers is not ' + 'implemented yet.');\n    }\n\n    const isSymbolicTensor = additionalInputs[0] instanceof SymbolicTensor;\n\n    for (const tensor of additionalInputs) {\n      if (tensor instanceof SymbolicTensor !== isSymbolicTensor) {\n        throw new ValueError('The initial state of a Bidirectional layer cannot be ' + 'specified as a mix of symbolic and non-symbolic tensors');\n      }\n    }\n\n    if (isSymbolicTensor) {\n      // Compute the full input and specs, including the states.\n      const fullInput = [inputs].concat(additionalInputs);\n      const fullInputSpec = this.inputSpec.concat(additionalSpecs); // Perform the call temporarily and replace inputSpec.\n      // Note: with initial states symbolic calls and non-symbolic calls to\n      // this method differ in how the initial states are passed. For\n      // symbolic calls, the initial states are passed in the first arg, as\n      // an Array of SymbolicTensors; for non-symbolic calls, they are\n      // passed in the second arg as a part of the kwargs. Hence the need to\n      // temporarily modify inputSpec here.\n      // TODO(cais): Make refactoring so that this hacky code below is no\n      // longer needed.\n\n      const originalInputSpec = this.inputSpec;\n      this.inputSpec = fullInputSpec;\n      const output = super.apply(fullInput, kwargs);\n      this.inputSpec = originalInputSpec;\n      return output;\n    } else {\n      return super.apply(inputs, kwargs);\n    }\n  }\n\n  call(inputs, kwargs) {\n    return tidy(() => {\n      const initialState = kwargs['initialState'];\n      let y;\n      let yRev;\n\n      if (initialState == null) {\n        y = this.forwardLayer.call(inputs, kwargs);\n        yRev = this.backwardLayer.call(inputs, kwargs);\n      } else {\n        const forwardState = initialState.slice(0, initialState.length / 2);\n        const backwardState = initialState.slice(initialState.length / 2);\n        y = this.forwardLayer.call(inputs, Object.assign(kwargs, {\n          initialState: forwardState\n        }));\n        yRev = this.backwardLayer.call(inputs, Object.assign(kwargs, {\n          initialState: backwardState\n        }));\n      }\n\n      let states;\n\n      if (this.returnState) {\n        if (Array.isArray(y)) {\n          states = y.slice(1).concat(yRev.slice(1));\n        } else {}\n\n        y = y[0];\n        yRev = yRev[0];\n      }\n\n      if (this.returnSequences) {\n        yRev = tfc.reverse(yRev, 1);\n      }\n\n      let output;\n\n      if (this.mergeMode === 'concat') {\n        output = K.concatenate([y, yRev]);\n      } else if (this.mergeMode === 'sum') {\n        output = tfc.add(y, yRev);\n      } else if (this.mergeMode === 'ave') {\n        output = tfc.mul(.5, tfc.add(y, yRev));\n      } else if (this.mergeMode === 'mul') {\n        output = tfc.mul(y, yRev);\n      } else if (this.mergeMode == null) {\n        output = [y, yRev];\n      } // TODO(cais): Properly set learning phase.\n\n\n      if (this.returnState) {\n        if (this.mergeMode == null) {\n          return output.concat(states);\n        }\n\n        return [output].concat(states);\n      }\n\n      return output;\n    });\n  }\n\n  resetStates(states) {\n    this.forwardLayer.resetStates();\n    this.backwardLayer.resetStates();\n  }\n\n  build(inputShape) {\n    nameScope(this.forwardLayer.name, () => {\n      this.forwardLayer.build(inputShape);\n    });\n    nameScope(this.backwardLayer.name, () => {\n      this.backwardLayer.build(inputShape);\n    });\n    this.built = true;\n  }\n\n  computeMask(inputs, mask) {\n    if (Array.isArray(mask)) {\n      mask = mask[0];\n    }\n\n    let outputMask;\n\n    if (this.returnSequences) {\n      if (this.mergeMode == null) {\n        outputMask = [mask, mask];\n      } else {\n        outputMask = mask;\n      }\n    } else {\n      if (this.mergeMode == null) {\n        outputMask = [null, null];\n      } else {\n        outputMask = null;\n      }\n    }\n\n    if (this.returnState) {\n      const states = this.forwardLayer.states;\n      const stateMask = states.map(state => null);\n\n      if (Array.isArray(outputMask)) {\n        return outputMask.concat(stateMask).concat(stateMask);\n      } else {\n        return [outputMask].concat(stateMask).concat(stateMask);\n      }\n    } else {\n      return outputMask;\n    }\n  }\n\n  get trainableWeights() {\n    return this.forwardLayer.trainableWeights.concat(this.backwardLayer.trainableWeights);\n  }\n\n  get nonTrainableWeights() {\n    return this.forwardLayer.nonTrainableWeights.concat(this.backwardLayer.nonTrainableWeights);\n  } // TODO(cais): Implement constraints().\n\n\n  setFastWeightInitDuringBuild(value) {\n    super.setFastWeightInitDuringBuild(value);\n\n    if (this.forwardLayer != null) {\n      this.forwardLayer.setFastWeightInitDuringBuild(value);\n    }\n\n    if (this.backwardLayer != null) {\n      this.backwardLayer.setFastWeightInitDuringBuild(value);\n    }\n  }\n\n  getConfig() {\n    const config = {\n      'mergeMode': this.mergeMode\n    }; // TODO(cais): Add logic for `numConstants` once the property is added.\n\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n  /** @nocollapse */\n\n\n  static fromConfig(cls, config) {\n    const rnnLayer = deserialize(config['layer']);\n    delete config['layer']; // TODO(cais): Add logic for `numConstants` once the property is added.\n\n    if (config['numConstants'] != null) {\n      throw new NotImplementedError(`Deserialization of a Bidirectional layer with numConstants ` + `present is not supported yet.`);\n    } // tslint:disable-next-line:no-any\n\n\n    const newConfig = config;\n    newConfig['layer'] = rnnLayer;\n    return new cls(newConfig);\n  }\n\n}\n/** @nocollapse */\n\nBidirectional.className = 'Bidirectional';\nserialization.registerClass(Bidirectional);","map":{"version":3,"names":["tfc","serialization","tidy","K","nameScope","InputSpec","Layer","SymbolicTensor","NotImplementedError","ValueError","VALID_BIDIRECTIONAL_MERGE_MODES","generic_utils","getExactlyOneShape","getExactlyOneTensor","rnn","standardizeArgs","deserialize","Wrapper","constructor","args","layer","build","inputShape","built","trainable","value","trainableWeights","nonTrainableWeights","updates","_updates","losses","getWeights","setWeights","weights","getConfig","config","getClassName","baseConfig","Object","assign","setFastWeightInitDuringBuild","fromConfig","cls","customObjects","layerConfig","newConfig","TimeDistributed","supportsMasking","length","JSON","stringify","inputSpec","shape","childInputShape","concat","slice","computeOutputShape","childOutputShape","timesteps","call","inputs","kwargs","step","states","output","rnnOutputs","y","className","registerClass","checkBidirectionalMergeMode","checkStringTypeUnionValue","DEFAULT_BIDIRECTIONAL_MERGE_MODE","Bidirectional","forwDict","forwardLayer","backDict","backwardLayer","name","mergeMode","undefined","_stateful","stateful","returnSequences","returnState","_trainable","numConstants","numWeights","numeightsOver2","Math","floor","layerShapes","Array","isArray","outputShape","outputShapes","stateShape","singletonOrArray","apply","initialState","constants","standardized","additionalInputs","additionalSpecs","numStates","push","stateSpecs","map","state","stateSpec","isSymbolicTensor","tensor","fullInput","fullInputSpec","originalInputSpec","yRev","forwardState","backwardState","reverse","concatenate","add","mul","resetStates","computeMask","mask","outputMask","stateMask","rnnLayer"],"sources":["C:/Users/Anke/Documents/Feelix documents/Feelix2.0-dev/Feelix v2/node_modules/@tensorflow/tfjs-layers/dist/layers/wrappers.js"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n * Layers that augment the functionality of a base layer.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { serialization, tidy } from '@tensorflow/tfjs-core';\nimport * as K from '../backend/tfjs_backend';\nimport { nameScope } from '../common';\nimport { InputSpec, Layer, SymbolicTensor } from '../engine/topology';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { VALID_BIDIRECTIONAL_MERGE_MODES } from '../keras_format/common';\nimport * as generic_utils from '../utils/generic_utils';\nimport { getExactlyOneShape, getExactlyOneTensor } from '../utils/types_utils';\nimport { rnn, standardizeArgs } from './recurrent';\nimport { deserialize } from './serialization';\n/**\n * Abstract wrapper base class.\n *\n * Wrappers take another layer and augment it in various ways.\n * Do not use this class as a layer, it is only an abstract base class.\n * Two usable wrappers are the `TimeDistributed` and `Bidirectional` wrappers.\n */\nexport class Wrapper extends Layer {\n    constructor(args) {\n        // Porting Note: In PyKeras, `self.layer` is set prior to the calling\n        //   `super()`. But we can't do that here due to TypeScript's restriction.\n        //   See: https://github.com/Microsoft/TypeScript/issues/8277\n        //   As a result, we have to add checks in `get trainable()` and\n        //   `set trainable()` below in order to prevent using `this.layer` when\n        //   its value is `undefined`. The super constructor does use the getter\n        //   and the setter of `this.layer`.\n        super(args);\n        this.layer = args.layer;\n    }\n    build(inputShape) {\n        this.built = true;\n    }\n    // TODO(cais): Implement activityRegularizer getter.\n    get trainable() {\n        // Porting Note: the check of `this.layer` here is necessary due to the\n        //   way the `constructor` of this class is written (see Porting Note\n        //   above).\n        if (this.layer != null) {\n            return this.layer.trainable;\n        }\n        else {\n            return false;\n        }\n    }\n    set trainable(value) {\n        // Porting Note: the check of `this.layer` here is necessary due to the\n        //   way the `constructor` of this class is written (see Porting Note\n        //   above).\n        if (this.layer != null) {\n            this.layer.trainable = value;\n        }\n    }\n    get trainableWeights() {\n        return this.layer.trainableWeights;\n    }\n    // TODO(cais): Implement setter for trainableWeights.\n    get nonTrainableWeights() {\n        return this.layer.nonTrainableWeights;\n    }\n    // TODO(cais): Implement setter for nonTrainableWeights.\n    get updates() {\n        // tslint:disable-next-line:no-any\n        return this.layer._updates;\n    }\n    // TODO(cais): Implement getUpdatesFor().\n    get losses() {\n        return this.layer.losses;\n    }\n    // TODO(cais): Implement getLossesFor().\n    getWeights() {\n        return this.layer.getWeights();\n    }\n    setWeights(weights) {\n        this.layer.setWeights(weights);\n    }\n    getConfig() {\n        const config = {\n            'layer': {\n                'className': this.layer.getClassName(),\n                'config': this.layer.getConfig(),\n            }\n        };\n        const baseConfig = super.getConfig();\n        Object.assign(config, baseConfig);\n        return config;\n    }\n    setFastWeightInitDuringBuild(value) {\n        super.setFastWeightInitDuringBuild(value);\n        if (this.layer != null) {\n            this.layer.setFastWeightInitDuringBuild(value);\n        }\n    }\n    /** @nocollapse */\n    static fromConfig(cls, config, customObjects = {}) {\n        const layerConfig = config['layer'];\n        const layer = deserialize(layerConfig, customObjects);\n        delete config['layer'];\n        const newConfig = { layer };\n        Object.assign(newConfig, config);\n        return new cls(newConfig);\n    }\n}\nexport class TimeDistributed extends Wrapper {\n    constructor(args) {\n        super(args);\n        this.supportsMasking = true;\n    }\n    build(inputShape) {\n        inputShape = getExactlyOneShape(inputShape);\n        if (inputShape.length < 3) {\n            throw new ValueError(`TimeDistributed layer expects an input shape >= 3D, but received ` +\n                `input shape ${JSON.stringify(inputShape)}`);\n        }\n        this.inputSpec = [{ shape: inputShape }];\n        const childInputShape = [inputShape[0]].concat(inputShape.slice(2));\n        if (!this.layer.built) {\n            this.layer.build(childInputShape);\n            this.layer.built = true;\n        }\n        super.build(inputShape);\n    }\n    computeOutputShape(inputShape) {\n        inputShape = getExactlyOneShape(inputShape);\n        const childInputShape = [inputShape[0]].concat(inputShape.slice(2));\n        const childOutputShape = this.layer.computeOutputShape(childInputShape);\n        const timesteps = inputShape[1];\n        return [childOutputShape[0], timesteps].concat(childOutputShape.slice(1));\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            // TODO(cais): Add 'training' and 'useLearningPhase' to kwargs.\n            inputs = getExactlyOneTensor(inputs);\n            // Porting Note: In tfjs-layers, `inputs` are always concrete tensor\n            // values. Hence the inputs can't have an undetermined first (batch)\n            // dimension, which is why we always use the K.rnn approach here.\n            const step = (inputs, states) => {\n                // TODO(cais): Add useLearningPhase.\n                // NOTE(cais): `layer.call` may return a length-1 array of Tensor in\n                //   some cases (e.g., `layer` is a `Sequential` instance), which is\n                //   why `getExactlyOneTensor` is used below.\n                const output = getExactlyOneTensor(this.layer.call(inputs, kwargs));\n                return [output, []];\n            };\n            const rnnOutputs = rnn(step, inputs, [], false /* goBackwards */, null /* mask */, null /* constants */, false /* unroll */, true /* needPerStepOutputs */);\n            const y = rnnOutputs[1];\n            // TODO(cais): Add activity regularization.\n            // TODO(cais): Add useLearningPhase.\n            return y;\n        });\n    }\n}\n/** @nocollapse */\nTimeDistributed.className = 'TimeDistributed';\nserialization.registerClass(TimeDistributed);\nexport function checkBidirectionalMergeMode(value) {\n    generic_utils.checkStringTypeUnionValue(VALID_BIDIRECTIONAL_MERGE_MODES, 'BidirectionalMergeMode', value);\n}\nconst DEFAULT_BIDIRECTIONAL_MERGE_MODE = 'concat';\nexport class Bidirectional extends Wrapper {\n    constructor(args) {\n        super(args);\n        // Note: When creating `this.forwardLayer`, the original Layer object\n        //   (`config.layer`) ought to be cloned. This is why we call\n        //   `getConfig()` followed by `deserialize()`. Without this cloning,\n        //   the layer names saved during serialization will incorrectly contain\n        //   the 'forward_' prefix. In Python Keras, this is done using\n        //   `copy.copy` (shallow copy), which does not have a simple equivalent\n        //   in JavaScript. JavaScript's `Object.assign()` does not copy\n        //   methods.\n        const layerConfig = args.layer.getConfig();\n        const forwDict = {};\n        forwDict['className'] = args.layer.getClassName();\n        forwDict['config'] = layerConfig;\n        this.forwardLayer = deserialize(forwDict);\n        layerConfig['goBackwards'] =\n            layerConfig['goBackwards'] === true ? false : true;\n        const backDict = {};\n        backDict['className'] = args.layer.getClassName();\n        backDict['config'] = layerConfig;\n        this.backwardLayer = deserialize(backDict);\n        this.forwardLayer.name = 'forward_' + this.forwardLayer.name;\n        this.backwardLayer.name = 'backward_' + this.backwardLayer.name;\n        this.mergeMode = args.mergeMode === undefined ?\n            DEFAULT_BIDIRECTIONAL_MERGE_MODE :\n            args.mergeMode;\n        checkBidirectionalMergeMode(this.mergeMode);\n        if (args.weights) {\n            throw new NotImplementedError('weights support is not implemented for Bidirectional layer yet.');\n        }\n        this._stateful = args.layer.stateful;\n        this.returnSequences = args.layer.returnSequences;\n        this.returnState = args.layer.returnState;\n        this.supportsMasking = true;\n        this._trainable = true;\n        this.inputSpec = args.layer.inputSpec;\n        this.numConstants = null;\n    }\n    get trainable() {\n        return this._trainable;\n    }\n    set trainable(value) {\n        // Porting Note: the check of `this.layer` here is necessary due to the\n        //   way the `constructor` of this class is written (see Porting Note\n        //   above).\n        this._trainable = value;\n        if (this.forwardLayer != null) {\n            this.forwardLayer.trainable = value;\n        }\n        if (this.backwardLayer != null) {\n            this.backwardLayer.trainable = value;\n        }\n    }\n    getWeights() {\n        return this.forwardLayer.getWeights().concat(this.backwardLayer.getWeights());\n    }\n    setWeights(weights) {\n        const numWeights = weights.length;\n        const numeightsOver2 = Math.floor(numWeights / 2);\n        this.forwardLayer.setWeights(weights.slice(0, numeightsOver2));\n        this.backwardLayer.setWeights(weights.slice(numeightsOver2));\n    }\n    computeOutputShape(inputShape) {\n        let layerShapes = this.forwardLayer.computeOutputShape(inputShape);\n        if (!(Array.isArray(layerShapes) && Array.isArray(layerShapes[0]))) {\n            layerShapes = [layerShapes];\n        }\n        layerShapes = layerShapes;\n        let outputShape;\n        let outputShapes;\n        let stateShape;\n        if (this.returnState) {\n            stateShape = layerShapes.slice(1);\n            outputShape = layerShapes[0];\n        }\n        else {\n            outputShape = layerShapes[0];\n        }\n        outputShape = outputShape;\n        if (this.mergeMode === 'concat') {\n            outputShape[outputShape.length - 1] *= 2;\n            outputShapes = [outputShape];\n        }\n        else if (this.mergeMode == null) {\n            outputShapes = [outputShape, outputShape.slice()];\n        }\n        else {\n            outputShapes = [outputShape];\n        }\n        if (this.returnState) {\n            if (this.mergeMode == null) {\n                return outputShapes.concat(stateShape).concat(stateShape.slice());\n            }\n            return [outputShape].concat(stateShape).concat(stateShape.slice());\n        }\n        return generic_utils.singletonOrArray(outputShapes);\n    }\n    apply(inputs, kwargs) {\n        let initialState = kwargs == null ? null : kwargs['initialState'];\n        let constants = kwargs == null ? null : kwargs['constants'];\n        if (kwargs == null) {\n            kwargs = {};\n        }\n        const standardized = standardizeArgs(inputs, initialState, constants, this.numConstants);\n        inputs = standardized.inputs;\n        initialState = standardized.initialState;\n        constants = standardized.constants;\n        if (Array.isArray(inputs)) {\n            initialState = inputs.slice(1);\n            inputs = inputs[0];\n        }\n        if ((initialState == null || initialState.length === 0) &&\n            constants == null) {\n            return super.apply(inputs, kwargs);\n        }\n        const additionalInputs = [];\n        const additionalSpecs = [];\n        if (initialState != null) {\n            const numStates = initialState.length;\n            if (numStates % 2 > 0) {\n                throw new ValueError('When passing `initialState` to a Bidrectional RNN, ' +\n                    'the state should be an Array containing the states of ' +\n                    'the underlying RNNs.');\n            }\n            kwargs['initialState'] = initialState;\n            additionalInputs.push(...initialState);\n            const stateSpecs = initialState\n                .map(state => new InputSpec({ shape: state.shape }));\n            this.forwardLayer.stateSpec = stateSpecs.slice(0, numStates / 2);\n            this.backwardLayer.stateSpec = stateSpecs.slice(numStates / 2);\n            additionalSpecs.push(...stateSpecs);\n        }\n        if (constants != null) {\n            throw new NotImplementedError('Support for constants in Bidirectional layers is not ' +\n                'implemented yet.');\n        }\n        const isSymbolicTensor = additionalInputs[0] instanceof SymbolicTensor;\n        for (const tensor of additionalInputs) {\n            if (tensor instanceof SymbolicTensor !== isSymbolicTensor) {\n                throw new ValueError('The initial state of a Bidirectional layer cannot be ' +\n                    'specified as a mix of symbolic and non-symbolic tensors');\n            }\n        }\n        if (isSymbolicTensor) {\n            // Compute the full input and specs, including the states.\n            const fullInput = [inputs].concat(additionalInputs);\n            const fullInputSpec = this.inputSpec.concat(additionalSpecs);\n            // Perform the call temporarily and replace inputSpec.\n            // Note: with initial states symbolic calls and non-symbolic calls to\n            // this method differ in how the initial states are passed. For\n            // symbolic calls, the initial states are passed in the first arg, as\n            // an Array of SymbolicTensors; for non-symbolic calls, they are\n            // passed in the second arg as a part of the kwargs. Hence the need to\n            // temporarily modify inputSpec here.\n            // TODO(cais): Make refactoring so that this hacky code below is no\n            // longer needed.\n            const originalInputSpec = this.inputSpec;\n            this.inputSpec = fullInputSpec;\n            const output = super.apply(fullInput, kwargs);\n            this.inputSpec = originalInputSpec;\n            return output;\n        }\n        else {\n            return super.apply(inputs, kwargs);\n        }\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            const initialState = kwargs['initialState'];\n            let y;\n            let yRev;\n            if (initialState == null) {\n                y = this.forwardLayer.call(inputs, kwargs);\n                yRev = this.backwardLayer.call(inputs, kwargs);\n            }\n            else {\n                const forwardState = initialState.slice(0, initialState.length / 2);\n                const backwardState = initialState.slice(initialState.length / 2);\n                y = this.forwardLayer.call(inputs, Object.assign(kwargs, { initialState: forwardState }));\n                yRev = this.backwardLayer.call(inputs, Object.assign(kwargs, { initialState: backwardState }));\n            }\n            let states;\n            if (this.returnState) {\n                if (Array.isArray(y)) {\n                    states = y.slice(1).concat(yRev.slice(1));\n                }\n                else {\n                }\n                y = y[0];\n                yRev = yRev[0];\n            }\n            if (this.returnSequences) {\n                yRev = tfc.reverse(yRev, 1);\n            }\n            let output;\n            if (this.mergeMode === 'concat') {\n                output = K.concatenate([y, yRev]);\n            }\n            else if (this.mergeMode === 'sum') {\n                output = tfc.add(y, yRev);\n            }\n            else if (this.mergeMode === 'ave') {\n                output = tfc.mul(.5, tfc.add(y, yRev));\n            }\n            else if (this.mergeMode === 'mul') {\n                output = tfc.mul(y, yRev);\n            }\n            else if (this.mergeMode == null) {\n                output = [y, yRev];\n            }\n            // TODO(cais): Properly set learning phase.\n            if (this.returnState) {\n                if (this.mergeMode == null) {\n                    return output.concat(states);\n                }\n                return [output].concat(states);\n            }\n            return output;\n        });\n    }\n    resetStates(states) {\n        this.forwardLayer.resetStates();\n        this.backwardLayer.resetStates();\n    }\n    build(inputShape) {\n        nameScope(this.forwardLayer.name, () => {\n            this.forwardLayer.build(inputShape);\n        });\n        nameScope(this.backwardLayer.name, () => {\n            this.backwardLayer.build(inputShape);\n        });\n        this.built = true;\n    }\n    computeMask(inputs, mask) {\n        if (Array.isArray(mask)) {\n            mask = mask[0];\n        }\n        let outputMask;\n        if (this.returnSequences) {\n            if (this.mergeMode == null) {\n                outputMask = [mask, mask];\n            }\n            else {\n                outputMask = mask;\n            }\n        }\n        else {\n            if (this.mergeMode == null) {\n                outputMask = [null, null];\n            }\n            else {\n                outputMask = null;\n            }\n        }\n        if (this.returnState) {\n            const states = this.forwardLayer.states;\n            const stateMask = states.map(state => null);\n            if (Array.isArray(outputMask)) {\n                return outputMask.concat(stateMask).concat(stateMask);\n            }\n            else {\n                return [outputMask].concat(stateMask).concat(stateMask);\n            }\n        }\n        else {\n            return outputMask;\n        }\n    }\n    get trainableWeights() {\n        return this.forwardLayer.trainableWeights.concat(this.backwardLayer.trainableWeights);\n    }\n    get nonTrainableWeights() {\n        return this.forwardLayer.nonTrainableWeights.concat(this.backwardLayer.nonTrainableWeights);\n    }\n    // TODO(cais): Implement constraints().\n    setFastWeightInitDuringBuild(value) {\n        super.setFastWeightInitDuringBuild(value);\n        if (this.forwardLayer != null) {\n            this.forwardLayer.setFastWeightInitDuringBuild(value);\n        }\n        if (this.backwardLayer != null) {\n            this.backwardLayer.setFastWeightInitDuringBuild(value);\n        }\n    }\n    getConfig() {\n        const config = {\n            'mergeMode': this.mergeMode,\n        };\n        // TODO(cais): Add logic for `numConstants` once the property is added.\n        const baseConfig = super.getConfig();\n        Object.assign(config, baseConfig);\n        return config;\n    }\n    /** @nocollapse */\n    static fromConfig(cls, config) {\n        const rnnLayer = deserialize(config['layer']);\n        delete config['layer'];\n        // TODO(cais): Add logic for `numConstants` once the property is added.\n        if (config['numConstants'] != null) {\n            throw new NotImplementedError(`Deserialization of a Bidirectional layer with numConstants ` +\n                `present is not supported yet.`);\n        }\n        // tslint:disable-next-line:no-any\n        const newConfig = config;\n        newConfig['layer'] = rnnLayer;\n        return new cls(newConfig);\n    }\n}\n/** @nocollapse */\nBidirectional.className = 'Bidirectional';\nserialization.registerClass(Bidirectional);\n"],"mappings":"AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AACA;AACA;AACA;AACA,OAAO,KAAKA,GAAZ,MAAqB,uBAArB;AACA,SAASC,aAAT,EAAwBC,IAAxB,QAAoC,uBAApC;AACA,OAAO,KAAKC,CAAZ,MAAmB,yBAAnB;AACA,SAASC,SAAT,QAA0B,WAA1B;AACA,SAASC,SAAT,EAAoBC,KAApB,EAA2BC,cAA3B,QAAiD,oBAAjD;AACA,SAASC,mBAAT,EAA8BC,UAA9B,QAAgD,WAAhD;AACA,SAASC,+BAAT,QAAgD,wBAAhD;AACA,OAAO,KAAKC,aAAZ,MAA+B,wBAA/B;AACA,SAASC,kBAAT,EAA6BC,mBAA7B,QAAwD,sBAAxD;AACA,SAASC,GAAT,EAAcC,eAAd,QAAqC,aAArC;AACA,SAASC,WAAT,QAA4B,iBAA5B;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AACA,OAAO,MAAMC,OAAN,SAAsBX,KAAtB,CAA4B;EAC/BY,WAAW,CAACC,IAAD,EAAO;IACd;IACA;IACA;IACA;IACA;IACA;IACA;IACA,MAAMA,IAAN;IACA,KAAKC,KAAL,GAAaD,IAAI,CAACC,KAAlB;EACH;;EACDC,KAAK,CAACC,UAAD,EAAa;IACd,KAAKC,KAAL,GAAa,IAAb;EACH,CAd8B,CAe/B;;;EACa,IAATC,SAAS,GAAG;IACZ;IACA;IACA;IACA,IAAI,KAAKJ,KAAL,IAAc,IAAlB,EAAwB;MACpB,OAAO,KAAKA,KAAL,CAAWI,SAAlB;IACH,CAFD,MAGK;MACD,OAAO,KAAP;IACH;EACJ;;EACY,IAATA,SAAS,CAACC,KAAD,EAAQ;IACjB;IACA;IACA;IACA,IAAI,KAAKL,KAAL,IAAc,IAAlB,EAAwB;MACpB,KAAKA,KAAL,CAAWI,SAAX,GAAuBC,KAAvB;IACH;EACJ;;EACmB,IAAhBC,gBAAgB,GAAG;IACnB,OAAO,KAAKN,KAAL,CAAWM,gBAAlB;EACH,CArC8B,CAsC/B;;;EACuB,IAAnBC,mBAAmB,GAAG;IACtB,OAAO,KAAKP,KAAL,CAAWO,mBAAlB;EACH,CAzC8B,CA0C/B;;;EACW,IAAPC,OAAO,GAAG;IACV;IACA,OAAO,KAAKR,KAAL,CAAWS,QAAlB;EACH,CA9C8B,CA+C/B;;;EACU,IAANC,MAAM,GAAG;IACT,OAAO,KAAKV,KAAL,CAAWU,MAAlB;EACH,CAlD8B,CAmD/B;;;EACAC,UAAU,GAAG;IACT,OAAO,KAAKX,KAAL,CAAWW,UAAX,EAAP;EACH;;EACDC,UAAU,CAACC,OAAD,EAAU;IAChB,KAAKb,KAAL,CAAWY,UAAX,CAAsBC,OAAtB;EACH;;EACDC,SAAS,GAAG;IACR,MAAMC,MAAM,GAAG;MACX,SAAS;QACL,aAAa,KAAKf,KAAL,CAAWgB,YAAX,EADR;QAEL,UAAU,KAAKhB,KAAL,CAAWc,SAAX;MAFL;IADE,CAAf;IAMA,MAAMG,UAAU,GAAG,MAAMH,SAAN,EAAnB;IACAI,MAAM,CAACC,MAAP,CAAcJ,MAAd,EAAsBE,UAAtB;IACA,OAAOF,MAAP;EACH;;EACDK,4BAA4B,CAACf,KAAD,EAAQ;IAChC,MAAMe,4BAAN,CAAmCf,KAAnC;;IACA,IAAI,KAAKL,KAAL,IAAc,IAAlB,EAAwB;MACpB,KAAKA,KAAL,CAAWoB,4BAAX,CAAwCf,KAAxC;IACH;EACJ;EACD;;;EACiB,OAAVgB,UAAU,CAACC,GAAD,EAAMP,MAAN,EAAcQ,aAAa,GAAG,EAA9B,EAAkC;IAC/C,MAAMC,WAAW,GAAGT,MAAM,CAAC,OAAD,CAA1B;IACA,MAAMf,KAAK,GAAGJ,WAAW,CAAC4B,WAAD,EAAcD,aAAd,CAAzB;IACA,OAAOR,MAAM,CAAC,OAAD,CAAb;IACA,MAAMU,SAAS,GAAG;MAAEzB;IAAF,CAAlB;IACAkB,MAAM,CAACC,MAAP,CAAcM,SAAd,EAAyBV,MAAzB;IACA,OAAO,IAAIO,GAAJ,CAAQG,SAAR,CAAP;EACH;;AAnF8B;AAqFnC,OAAO,MAAMC,eAAN,SAA8B7B,OAA9B,CAAsC;EACzCC,WAAW,CAACC,IAAD,EAAO;IACd,MAAMA,IAAN;IACA,KAAK4B,eAAL,GAAuB,IAAvB;EACH;;EACD1B,KAAK,CAACC,UAAD,EAAa;IACdA,UAAU,GAAGV,kBAAkB,CAACU,UAAD,CAA/B;;IACA,IAAIA,UAAU,CAAC0B,MAAX,GAAoB,CAAxB,EAA2B;MACvB,MAAM,IAAIvC,UAAJ,CAAgB,mEAAD,GAChB,eAAcwC,IAAI,CAACC,SAAL,CAAe5B,UAAf,CAA2B,EADxC,CAAN;IAEH;;IACD,KAAK6B,SAAL,GAAiB,CAAC;MAAEC,KAAK,EAAE9B;IAAT,CAAD,CAAjB;IACA,MAAM+B,eAAe,GAAG,CAAC/B,UAAU,CAAC,CAAD,CAAX,EAAgBgC,MAAhB,CAAuBhC,UAAU,CAACiC,KAAX,CAAiB,CAAjB,CAAvB,CAAxB;;IACA,IAAI,CAAC,KAAKnC,KAAL,CAAWG,KAAhB,EAAuB;MACnB,KAAKH,KAAL,CAAWC,KAAX,CAAiBgC,eAAjB;MACA,KAAKjC,KAAL,CAAWG,KAAX,GAAmB,IAAnB;IACH;;IACD,MAAMF,KAAN,CAAYC,UAAZ;EACH;;EACDkC,kBAAkB,CAAClC,UAAD,EAAa;IAC3BA,UAAU,GAAGV,kBAAkB,CAACU,UAAD,CAA/B;IACA,MAAM+B,eAAe,GAAG,CAAC/B,UAAU,CAAC,CAAD,CAAX,EAAgBgC,MAAhB,CAAuBhC,UAAU,CAACiC,KAAX,CAAiB,CAAjB,CAAvB,CAAxB;IACA,MAAME,gBAAgB,GAAG,KAAKrC,KAAL,CAAWoC,kBAAX,CAA8BH,eAA9B,CAAzB;IACA,MAAMK,SAAS,GAAGpC,UAAU,CAAC,CAAD,CAA5B;IACA,OAAO,CAACmC,gBAAgB,CAAC,CAAD,CAAjB,EAAsBC,SAAtB,EAAiCJ,MAAjC,CAAwCG,gBAAgB,CAACF,KAAjB,CAAuB,CAAvB,CAAxC,CAAP;EACH;;EACDI,IAAI,CAACC,MAAD,EAASC,MAAT,EAAiB;IACjB,OAAO3D,IAAI,CAAC,MAAM;MACd;MACA0D,MAAM,GAAG/C,mBAAmB,CAAC+C,MAAD,CAA5B,CAFc,CAGd;MACA;MACA;;MACA,MAAME,IAAI,GAAG,CAACF,MAAD,EAASG,MAAT,KAAoB;QAC7B;QACA;QACA;QACA;QACA,MAAMC,MAAM,GAAGnD,mBAAmB,CAAC,KAAKO,KAAL,CAAWuC,IAAX,CAAgBC,MAAhB,EAAwBC,MAAxB,CAAD,CAAlC;QACA,OAAO,CAACG,MAAD,EAAS,EAAT,CAAP;MACH,CAPD;;MAQA,MAAMC,UAAU,GAAGnD,GAAG,CAACgD,IAAD,EAAOF,MAAP,EAAe,EAAf,EAAmB;MAAM;MAAzB,EAA4C;MAAK;MAAjD,EAA6D;MAAK;MAAlE,EAAmF;MAAM;MAAzF,EAAuG;MAAK;MAA5G,CAAtB;MACA,MAAMM,CAAC,GAAGD,UAAU,CAAC,CAAD,CAApB,CAfc,CAgBd;MACA;;MACA,OAAOC,CAAP;IACH,CAnBU,CAAX;EAoBH;;AA/CwC;AAiD7C;;AACApB,eAAe,CAACqB,SAAhB,GAA4B,iBAA5B;AACAlE,aAAa,CAACmE,aAAd,CAA4BtB,eAA5B;AACA,OAAO,SAASuB,2BAAT,CAAqC5C,KAArC,EAA4C;EAC/Cd,aAAa,CAAC2D,yBAAd,CAAwC5D,+BAAxC,EAAyE,wBAAzE,EAAmGe,KAAnG;AACH;AACD,MAAM8C,gCAAgC,GAAG,QAAzC;AACA,OAAO,MAAMC,aAAN,SAA4BvD,OAA5B,CAAoC;EACvCC,WAAW,CAACC,IAAD,EAAO;IACd,MAAMA,IAAN,EADc,CAEd;IACA;IACA;IACA;IACA;IACA;IACA;IACA;;IACA,MAAMyB,WAAW,GAAGzB,IAAI,CAACC,KAAL,CAAWc,SAAX,EAApB;IACA,MAAMuC,QAAQ,GAAG,EAAjB;IACAA,QAAQ,CAAC,WAAD,CAAR,GAAwBtD,IAAI,CAACC,KAAL,CAAWgB,YAAX,EAAxB;IACAqC,QAAQ,CAAC,QAAD,CAAR,GAAqB7B,WAArB;IACA,KAAK8B,YAAL,GAAoB1D,WAAW,CAACyD,QAAD,CAA/B;IACA7B,WAAW,CAAC,aAAD,CAAX,GACIA,WAAW,CAAC,aAAD,CAAX,KAA+B,IAA/B,GAAsC,KAAtC,GAA8C,IADlD;IAEA,MAAM+B,QAAQ,GAAG,EAAjB;IACAA,QAAQ,CAAC,WAAD,CAAR,GAAwBxD,IAAI,CAACC,KAAL,CAAWgB,YAAX,EAAxB;IACAuC,QAAQ,CAAC,QAAD,CAAR,GAAqB/B,WAArB;IACA,KAAKgC,aAAL,GAAqB5D,WAAW,CAAC2D,QAAD,CAAhC;IACA,KAAKD,YAAL,CAAkBG,IAAlB,GAAyB,aAAa,KAAKH,YAAL,CAAkBG,IAAxD;IACA,KAAKD,aAAL,CAAmBC,IAAnB,GAA0B,cAAc,KAAKD,aAAL,CAAmBC,IAA3D;IACA,KAAKC,SAAL,GAAiB3D,IAAI,CAAC2D,SAAL,KAAmBC,SAAnB,GACbR,gCADa,GAEbpD,IAAI,CAAC2D,SAFT;IAGAT,2BAA2B,CAAC,KAAKS,SAAN,CAA3B;;IACA,IAAI3D,IAAI,CAACc,OAAT,EAAkB;MACd,MAAM,IAAIzB,mBAAJ,CAAwB,iEAAxB,CAAN;IACH;;IACD,KAAKwE,SAAL,GAAiB7D,IAAI,CAACC,KAAL,CAAW6D,QAA5B;IACA,KAAKC,eAAL,GAAuB/D,IAAI,CAACC,KAAL,CAAW8D,eAAlC;IACA,KAAKC,WAAL,GAAmBhE,IAAI,CAACC,KAAL,CAAW+D,WAA9B;IACA,KAAKpC,eAAL,GAAuB,IAAvB;IACA,KAAKqC,UAAL,GAAkB,IAAlB;IACA,KAAKjC,SAAL,GAAiBhC,IAAI,CAACC,KAAL,CAAW+B,SAA5B;IACA,KAAKkC,YAAL,GAAoB,IAApB;EACH;;EACY,IAAT7D,SAAS,GAAG;IACZ,OAAO,KAAK4D,UAAZ;EACH;;EACY,IAAT5D,SAAS,CAACC,KAAD,EAAQ;IACjB;IACA;IACA;IACA,KAAK2D,UAAL,GAAkB3D,KAAlB;;IACA,IAAI,KAAKiD,YAAL,IAAqB,IAAzB,EAA+B;MAC3B,KAAKA,YAAL,CAAkBlD,SAAlB,GAA8BC,KAA9B;IACH;;IACD,IAAI,KAAKmD,aAAL,IAAsB,IAA1B,EAAgC;MAC5B,KAAKA,aAAL,CAAmBpD,SAAnB,GAA+BC,KAA/B;IACH;EACJ;;EACDM,UAAU,GAAG;IACT,OAAO,KAAK2C,YAAL,CAAkB3C,UAAlB,GAA+BuB,MAA/B,CAAsC,KAAKsB,aAAL,CAAmB7C,UAAnB,EAAtC,CAAP;EACH;;EACDC,UAAU,CAACC,OAAD,EAAU;IAChB,MAAMqD,UAAU,GAAGrD,OAAO,CAACe,MAA3B;IACA,MAAMuC,cAAc,GAAGC,IAAI,CAACC,KAAL,CAAWH,UAAU,GAAG,CAAxB,CAAvB;IACA,KAAKZ,YAAL,CAAkB1C,UAAlB,CAA6BC,OAAO,CAACsB,KAAR,CAAc,CAAd,EAAiBgC,cAAjB,CAA7B;IACA,KAAKX,aAAL,CAAmB5C,UAAnB,CAA8BC,OAAO,CAACsB,KAAR,CAAcgC,cAAd,CAA9B;EACH;;EACD/B,kBAAkB,CAAClC,UAAD,EAAa;IAC3B,IAAIoE,WAAW,GAAG,KAAKhB,YAAL,CAAkBlB,kBAAlB,CAAqClC,UAArC,CAAlB;;IACA,IAAI,EAAEqE,KAAK,CAACC,OAAN,CAAcF,WAAd,KAA8BC,KAAK,CAACC,OAAN,CAAcF,WAAW,CAAC,CAAD,CAAzB,CAAhC,CAAJ,EAAoE;MAChEA,WAAW,GAAG,CAACA,WAAD,CAAd;IACH;;IACDA,WAAW,GAAGA,WAAd;IACA,IAAIG,WAAJ;IACA,IAAIC,YAAJ;IACA,IAAIC,UAAJ;;IACA,IAAI,KAAKZ,WAAT,EAAsB;MAClBY,UAAU,GAAGL,WAAW,CAACnC,KAAZ,CAAkB,CAAlB,CAAb;MACAsC,WAAW,GAAGH,WAAW,CAAC,CAAD,CAAzB;IACH,CAHD,MAIK;MACDG,WAAW,GAAGH,WAAW,CAAC,CAAD,CAAzB;IACH;;IACDG,WAAW,GAAGA,WAAd;;IACA,IAAI,KAAKf,SAAL,KAAmB,QAAvB,EAAiC;MAC7Be,WAAW,CAACA,WAAW,CAAC7C,MAAZ,GAAqB,CAAtB,CAAX,IAAuC,CAAvC;MACA8C,YAAY,GAAG,CAACD,WAAD,CAAf;IACH,CAHD,MAIK,IAAI,KAAKf,SAAL,IAAkB,IAAtB,EAA4B;MAC7BgB,YAAY,GAAG,CAACD,WAAD,EAAcA,WAAW,CAACtC,KAAZ,EAAd,CAAf;IACH,CAFI,MAGA;MACDuC,YAAY,GAAG,CAACD,WAAD,CAAf;IACH;;IACD,IAAI,KAAKV,WAAT,EAAsB;MAClB,IAAI,KAAKL,SAAL,IAAkB,IAAtB,EAA4B;QACxB,OAAOgB,YAAY,CAACxC,MAAb,CAAoByC,UAApB,EAAgCzC,MAAhC,CAAuCyC,UAAU,CAACxC,KAAX,EAAvC,CAAP;MACH;;MACD,OAAO,CAACsC,WAAD,EAAcvC,MAAd,CAAqByC,UAArB,EAAiCzC,MAAjC,CAAwCyC,UAAU,CAACxC,KAAX,EAAxC,CAAP;IACH;;IACD,OAAO5C,aAAa,CAACqF,gBAAd,CAA+BF,YAA/B,CAAP;EACH;;EACDG,KAAK,CAACrC,MAAD,EAASC,MAAT,EAAiB;IAClB,IAAIqC,YAAY,GAAGrC,MAAM,IAAI,IAAV,GAAiB,IAAjB,GAAwBA,MAAM,CAAC,cAAD,CAAjD;IACA,IAAIsC,SAAS,GAAGtC,MAAM,IAAI,IAAV,GAAiB,IAAjB,GAAwBA,MAAM,CAAC,WAAD,CAA9C;;IACA,IAAIA,MAAM,IAAI,IAAd,EAAoB;MAChBA,MAAM,GAAG,EAAT;IACH;;IACD,MAAMuC,YAAY,GAAGrF,eAAe,CAAC6C,MAAD,EAASsC,YAAT,EAAuBC,SAAvB,EAAkC,KAAKd,YAAvC,CAApC;IACAzB,MAAM,GAAGwC,YAAY,CAACxC,MAAtB;IACAsC,YAAY,GAAGE,YAAY,CAACF,YAA5B;IACAC,SAAS,GAAGC,YAAY,CAACD,SAAzB;;IACA,IAAIR,KAAK,CAACC,OAAN,CAAchC,MAAd,CAAJ,EAA2B;MACvBsC,YAAY,GAAGtC,MAAM,CAACL,KAAP,CAAa,CAAb,CAAf;MACAK,MAAM,GAAGA,MAAM,CAAC,CAAD,CAAf;IACH;;IACD,IAAI,CAACsC,YAAY,IAAI,IAAhB,IAAwBA,YAAY,CAAClD,MAAb,KAAwB,CAAjD,KACAmD,SAAS,IAAI,IADjB,EACuB;MACnB,OAAO,MAAMF,KAAN,CAAYrC,MAAZ,EAAoBC,MAApB,CAAP;IACH;;IACD,MAAMwC,gBAAgB,GAAG,EAAzB;IACA,MAAMC,eAAe,GAAG,EAAxB;;IACA,IAAIJ,YAAY,IAAI,IAApB,EAA0B;MACtB,MAAMK,SAAS,GAAGL,YAAY,CAAClD,MAA/B;;MACA,IAAIuD,SAAS,GAAG,CAAZ,GAAgB,CAApB,EAAuB;QACnB,MAAM,IAAI9F,UAAJ,CAAe,wDACjB,wDADiB,GAEjB,sBAFE,CAAN;MAGH;;MACDoD,MAAM,CAAC,cAAD,CAAN,GAAyBqC,YAAzB;MACAG,gBAAgB,CAACG,IAAjB,CAAsB,GAAGN,YAAzB;MACA,MAAMO,UAAU,GAAGP,YAAY,CAC1BQ,GADc,CACVC,KAAK,IAAI,IAAItG,SAAJ,CAAc;QAAE+C,KAAK,EAAEuD,KAAK,CAACvD;MAAf,CAAd,CADC,CAAnB;MAEA,KAAKsB,YAAL,CAAkBkC,SAAlB,GAA8BH,UAAU,CAAClD,KAAX,CAAiB,CAAjB,EAAoBgD,SAAS,GAAG,CAAhC,CAA9B;MACA,KAAK3B,aAAL,CAAmBgC,SAAnB,GAA+BH,UAAU,CAAClD,KAAX,CAAiBgD,SAAS,GAAG,CAA7B,CAA/B;MACAD,eAAe,CAACE,IAAhB,CAAqB,GAAGC,UAAxB;IACH;;IACD,IAAIN,SAAS,IAAI,IAAjB,EAAuB;MACnB,MAAM,IAAI3F,mBAAJ,CAAwB,0DAC1B,kBADE,CAAN;IAEH;;IACD,MAAMqG,gBAAgB,GAAGR,gBAAgB,CAAC,CAAD,CAAhB,YAA+B9F,cAAxD;;IACA,KAAK,MAAMuG,MAAX,IAAqBT,gBAArB,EAAuC;MACnC,IAAIS,MAAM,YAAYvG,cAAlB,KAAqCsG,gBAAzC,EAA2D;QACvD,MAAM,IAAIpG,UAAJ,CAAe,0DACjB,yDADE,CAAN;MAEH;IACJ;;IACD,IAAIoG,gBAAJ,EAAsB;MAClB;MACA,MAAME,SAAS,GAAG,CAACnD,MAAD,EAASN,MAAT,CAAgB+C,gBAAhB,CAAlB;MACA,MAAMW,aAAa,GAAG,KAAK7D,SAAL,CAAeG,MAAf,CAAsBgD,eAAtB,CAAtB,CAHkB,CAIlB;MACA;MACA;MACA;MACA;MACA;MACA;MACA;MACA;;MACA,MAAMW,iBAAiB,GAAG,KAAK9D,SAA/B;MACA,KAAKA,SAAL,GAAiB6D,aAAjB;MACA,MAAMhD,MAAM,GAAG,MAAMiC,KAAN,CAAYc,SAAZ,EAAuBlD,MAAvB,CAAf;MACA,KAAKV,SAAL,GAAiB8D,iBAAjB;MACA,OAAOjD,MAAP;IACH,CAlBD,MAmBK;MACD,OAAO,MAAMiC,KAAN,CAAYrC,MAAZ,EAAoBC,MAApB,CAAP;IACH;EACJ;;EACDF,IAAI,CAACC,MAAD,EAASC,MAAT,EAAiB;IACjB,OAAO3D,IAAI,CAAC,MAAM;MACd,MAAMgG,YAAY,GAAGrC,MAAM,CAAC,cAAD,CAA3B;MACA,IAAIK,CAAJ;MACA,IAAIgD,IAAJ;;MACA,IAAIhB,YAAY,IAAI,IAApB,EAA0B;QACtBhC,CAAC,GAAG,KAAKQ,YAAL,CAAkBf,IAAlB,CAAuBC,MAAvB,EAA+BC,MAA/B,CAAJ;QACAqD,IAAI,GAAG,KAAKtC,aAAL,CAAmBjB,IAAnB,CAAwBC,MAAxB,EAAgCC,MAAhC,CAAP;MACH,CAHD,MAIK;QACD,MAAMsD,YAAY,GAAGjB,YAAY,CAAC3C,KAAb,CAAmB,CAAnB,EAAsB2C,YAAY,CAAClD,MAAb,GAAsB,CAA5C,CAArB;QACA,MAAMoE,aAAa,GAAGlB,YAAY,CAAC3C,KAAb,CAAmB2C,YAAY,CAAClD,MAAb,GAAsB,CAAzC,CAAtB;QACAkB,CAAC,GAAG,KAAKQ,YAAL,CAAkBf,IAAlB,CAAuBC,MAAvB,EAA+BtB,MAAM,CAACC,MAAP,CAAcsB,MAAd,EAAsB;UAAEqC,YAAY,EAAEiB;QAAhB,CAAtB,CAA/B,CAAJ;QACAD,IAAI,GAAG,KAAKtC,aAAL,CAAmBjB,IAAnB,CAAwBC,MAAxB,EAAgCtB,MAAM,CAACC,MAAP,CAAcsB,MAAd,EAAsB;UAAEqC,YAAY,EAAEkB;QAAhB,CAAtB,CAAhC,CAAP;MACH;;MACD,IAAIrD,MAAJ;;MACA,IAAI,KAAKoB,WAAT,EAAsB;QAClB,IAAIQ,KAAK,CAACC,OAAN,CAAc1B,CAAd,CAAJ,EAAsB;UAClBH,MAAM,GAAGG,CAAC,CAACX,KAAF,CAAQ,CAAR,EAAWD,MAAX,CAAkB4D,IAAI,CAAC3D,KAAL,CAAW,CAAX,CAAlB,CAAT;QACH,CAFD,MAGK,CACJ;;QACDW,CAAC,GAAGA,CAAC,CAAC,CAAD,CAAL;QACAgD,IAAI,GAAGA,IAAI,CAAC,CAAD,CAAX;MACH;;MACD,IAAI,KAAKhC,eAAT,EAA0B;QACtBgC,IAAI,GAAGlH,GAAG,CAACqH,OAAJ,CAAYH,IAAZ,EAAkB,CAAlB,CAAP;MACH;;MACD,IAAIlD,MAAJ;;MACA,IAAI,KAAKc,SAAL,KAAmB,QAAvB,EAAiC;QAC7Bd,MAAM,GAAG7D,CAAC,CAACmH,WAAF,CAAc,CAACpD,CAAD,EAAIgD,IAAJ,CAAd,CAAT;MACH,CAFD,MAGK,IAAI,KAAKpC,SAAL,KAAmB,KAAvB,EAA8B;QAC/Bd,MAAM,GAAGhE,GAAG,CAACuH,GAAJ,CAAQrD,CAAR,EAAWgD,IAAX,CAAT;MACH,CAFI,MAGA,IAAI,KAAKpC,SAAL,KAAmB,KAAvB,EAA8B;QAC/Bd,MAAM,GAAGhE,GAAG,CAACwH,GAAJ,CAAQ,EAAR,EAAYxH,GAAG,CAACuH,GAAJ,CAAQrD,CAAR,EAAWgD,IAAX,CAAZ,CAAT;MACH,CAFI,MAGA,IAAI,KAAKpC,SAAL,KAAmB,KAAvB,EAA8B;QAC/Bd,MAAM,GAAGhE,GAAG,CAACwH,GAAJ,CAAQtD,CAAR,EAAWgD,IAAX,CAAT;MACH,CAFI,MAGA,IAAI,KAAKpC,SAAL,IAAkB,IAAtB,EAA4B;QAC7Bd,MAAM,GAAG,CAACE,CAAD,EAAIgD,IAAJ,CAAT;MACH,CA1Ca,CA2Cd;;;MACA,IAAI,KAAK/B,WAAT,EAAsB;QAClB,IAAI,KAAKL,SAAL,IAAkB,IAAtB,EAA4B;UACxB,OAAOd,MAAM,CAACV,MAAP,CAAcS,MAAd,CAAP;QACH;;QACD,OAAO,CAACC,MAAD,EAASV,MAAT,CAAgBS,MAAhB,CAAP;MACH;;MACD,OAAOC,MAAP;IACH,CAnDU,CAAX;EAoDH;;EACDyD,WAAW,CAAC1D,MAAD,EAAS;IAChB,KAAKW,YAAL,CAAkB+C,WAAlB;IACA,KAAK7C,aAAL,CAAmB6C,WAAnB;EACH;;EACDpG,KAAK,CAACC,UAAD,EAAa;IACdlB,SAAS,CAAC,KAAKsE,YAAL,CAAkBG,IAAnB,EAAyB,MAAM;MACpC,KAAKH,YAAL,CAAkBrD,KAAlB,CAAwBC,UAAxB;IACH,CAFQ,CAAT;IAGAlB,SAAS,CAAC,KAAKwE,aAAL,CAAmBC,IAApB,EAA0B,MAAM;MACrC,KAAKD,aAAL,CAAmBvD,KAAnB,CAAyBC,UAAzB;IACH,CAFQ,CAAT;IAGA,KAAKC,KAAL,GAAa,IAAb;EACH;;EACDmG,WAAW,CAAC9D,MAAD,EAAS+D,IAAT,EAAe;IACtB,IAAIhC,KAAK,CAACC,OAAN,CAAc+B,IAAd,CAAJ,EAAyB;MACrBA,IAAI,GAAGA,IAAI,CAAC,CAAD,CAAX;IACH;;IACD,IAAIC,UAAJ;;IACA,IAAI,KAAK1C,eAAT,EAA0B;MACtB,IAAI,KAAKJ,SAAL,IAAkB,IAAtB,EAA4B;QACxB8C,UAAU,GAAG,CAACD,IAAD,EAAOA,IAAP,CAAb;MACH,CAFD,MAGK;QACDC,UAAU,GAAGD,IAAb;MACH;IACJ,CAPD,MAQK;MACD,IAAI,KAAK7C,SAAL,IAAkB,IAAtB,EAA4B;QACxB8C,UAAU,GAAG,CAAC,IAAD,EAAO,IAAP,CAAb;MACH,CAFD,MAGK;QACDA,UAAU,GAAG,IAAb;MACH;IACJ;;IACD,IAAI,KAAKzC,WAAT,EAAsB;MAClB,MAAMpB,MAAM,GAAG,KAAKW,YAAL,CAAkBX,MAAjC;MACA,MAAM8D,SAAS,GAAG9D,MAAM,CAAC2C,GAAP,CAAWC,KAAK,IAAI,IAApB,CAAlB;;MACA,IAAIhB,KAAK,CAACC,OAAN,CAAcgC,UAAd,CAAJ,EAA+B;QAC3B,OAAOA,UAAU,CAACtE,MAAX,CAAkBuE,SAAlB,EAA6BvE,MAA7B,CAAoCuE,SAApC,CAAP;MACH,CAFD,MAGK;QACD,OAAO,CAACD,UAAD,EAAatE,MAAb,CAAoBuE,SAApB,EAA+BvE,MAA/B,CAAsCuE,SAAtC,CAAP;MACH;IACJ,CATD,MAUK;MACD,OAAOD,UAAP;IACH;EACJ;;EACmB,IAAhBlG,gBAAgB,GAAG;IACnB,OAAO,KAAKgD,YAAL,CAAkBhD,gBAAlB,CAAmC4B,MAAnC,CAA0C,KAAKsB,aAAL,CAAmBlD,gBAA7D,CAAP;EACH;;EACsB,IAAnBC,mBAAmB,GAAG;IACtB,OAAO,KAAK+C,YAAL,CAAkB/C,mBAAlB,CAAsC2B,MAAtC,CAA6C,KAAKsB,aAAL,CAAmBjD,mBAAhE,CAAP;EACH,CAlRsC,CAmRvC;;;EACAa,4BAA4B,CAACf,KAAD,EAAQ;IAChC,MAAMe,4BAAN,CAAmCf,KAAnC;;IACA,IAAI,KAAKiD,YAAL,IAAqB,IAAzB,EAA+B;MAC3B,KAAKA,YAAL,CAAkBlC,4BAAlB,CAA+Cf,KAA/C;IACH;;IACD,IAAI,KAAKmD,aAAL,IAAsB,IAA1B,EAAgC;MAC5B,KAAKA,aAAL,CAAmBpC,4BAAnB,CAAgDf,KAAhD;IACH;EACJ;;EACDS,SAAS,GAAG;IACR,MAAMC,MAAM,GAAG;MACX,aAAa,KAAK2C;IADP,CAAf,CADQ,CAIR;;IACA,MAAMzC,UAAU,GAAG,MAAMH,SAAN,EAAnB;IACAI,MAAM,CAACC,MAAP,CAAcJ,MAAd,EAAsBE,UAAtB;IACA,OAAOF,MAAP;EACH;EACD;;;EACiB,OAAVM,UAAU,CAACC,GAAD,EAAMP,MAAN,EAAc;IAC3B,MAAM2F,QAAQ,GAAG9G,WAAW,CAACmB,MAAM,CAAC,OAAD,CAAP,CAA5B;IACA,OAAOA,MAAM,CAAC,OAAD,CAAb,CAF2B,CAG3B;;IACA,IAAIA,MAAM,CAAC,cAAD,CAAN,IAA0B,IAA9B,EAAoC;MAChC,MAAM,IAAI3B,mBAAJ,CAAyB,6DAAD,GACzB,+BADC,CAAN;IAEH,CAP0B,CAQ3B;;;IACA,MAAMqC,SAAS,GAAGV,MAAlB;IACAU,SAAS,CAAC,OAAD,CAAT,GAAqBiF,QAArB;IACA,OAAO,IAAIpF,GAAJ,CAAQG,SAAR,CAAP;EACH;;AAnTsC;AAqT3C;;AACA2B,aAAa,CAACL,SAAd,GAA0B,eAA1B;AACAlE,aAAa,CAACmE,aAAd,CAA4BI,aAA5B"},"metadata":{},"sourceType":"module"}