{"ast":null,"code":"import _asyncToGenerator from \"C:/Users/Anke/Documents/Feelix documents/Feelix2.0-dev/Feelix v2/node_modules/@babel/runtime/helpers/esm/asyncToGenerator.js\";\n\nvar _loadGraphModel;\n\n/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { io, Tensor, util } from '@tensorflow/tfjs-core';\nimport { OperationMapper } from '../operations/operation_mapper';\nimport { GraphExecutor } from './graph_executor';\nimport { ResourceManager } from './resource_manager';\nexport const TFHUB_SEARCH_PARAM = '?tfjs-format=file';\nexport const DEFAULT_MODEL_NAME = 'model.json';\n/**\n * A `tf.GraphModel` is a directed, acyclic graph built from a\n * SavedModel GraphDef and allows inference execution.\n *\n * A `tf.GraphModel` can only be created by loading from a model converted from\n * a [TensorFlow SavedModel](https://www.tensorflow.org/guide/saved_model) using\n * the command line converter tool and loaded via `tf.loadGraphModel`.\n *\n * @doc {heading: 'Models', subheading: 'Classes'}\n */\n\nexport class GraphModel {\n  /**\n   * @param modelUrl url for the model, or an `io.IOHandler`.\n   * @param weightManifestUrl url for the weight file generated by\n   * scripts/convert.py script.\n   * @param requestOption options for Request, which allows to send credentials\n   * and custom headers.\n   * @param onProgress Optional, progress callback function, fired periodically\n   * before the load is completed.\n   */\n  constructor(modelUrl, loadOptions = {}, tfio = io) {\n    this.modelUrl = modelUrl;\n    this.loadOptions = loadOptions;\n    this.version = 'n/a';\n    this.io = tfio;\n\n    if (loadOptions == null) {\n      this.loadOptions = {};\n    }\n\n    this.resourceManager = new ResourceManager();\n  } // Returns the version information for the tensorflow model GraphDef.\n\n\n  get modelVersion() {\n    return this.version;\n  }\n\n  get inputNodes() {\n    return this.executor.inputNodes;\n  }\n\n  get outputNodes() {\n    return this.executor.outputNodes;\n  }\n\n  get inputs() {\n    return this.executor.inputs;\n  }\n\n  get outputs() {\n    return this.executor.outputs;\n  }\n\n  get weights() {\n    return this.executor.weightMap;\n  }\n\n  get metadata() {\n    return this.artifacts.userDefinedMetadata;\n  }\n\n  get modelSignature() {\n    return this.signature;\n  }\n\n  get modelStructuredOutputKeys() {\n    return this.structuredOutputKeys;\n  }\n\n  findIOHandler() {\n    const path = this.modelUrl;\n\n    if (path.load != null) {\n      // Path is an IO Handler.\n      this.handler = path;\n    } else if (this.loadOptions.requestInit != null) {\n      this.handler = this.io.browserHTTPRequest(path, this.loadOptions);\n    } else {\n      const handlers = this.io.getLoadHandlers(path, this.loadOptions);\n\n      if (handlers.length === 0) {\n        // For backward compatibility: if no load handler can be found,\n        // assume it is a relative http path.\n        handlers.push(this.io.browserHTTPRequest(path, this.loadOptions));\n      } else if (handlers.length > 1) {\n        throw new Error(`Found more than one (${handlers.length}) load handlers for ` + `URL '${[path]}'`);\n      }\n\n      this.handler = handlers[0];\n    }\n  }\n  /**\n   * Loads the model and weight files, construct the in memory weight map and\n   * compile the inference graph.\n   */\n\n\n  load() {\n    this.findIOHandler();\n\n    if (this.handler.load == null) {\n      throw new Error('Cannot proceed with model loading because the IOHandler provided ' + 'does not have the `load` method implemented.');\n    }\n\n    const loadResult = this.handler.load();\n\n    if (util.isPromise(loadResult)) {\n      return loadResult.then(artifacts => this.loadSync(artifacts));\n    }\n\n    return this.loadSync(loadResult);\n  }\n  /**\n   * Synchronously construct the in memory weight map and\n   * compile the inference graph. Also initialize hashtable if any.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes', ignoreCI: true}\n   */\n\n\n  loadSync(artifacts) {\n    this.artifacts = artifacts;\n    const graph = this.artifacts.modelTopology;\n    let signature = this.artifacts.signature;\n\n    if (this.artifacts.userDefinedMetadata != null) {\n      const metadata = this.artifacts.userDefinedMetadata;\n\n      if (metadata.signature != null) {\n        signature = metadata.signature;\n      }\n\n      if (metadata.structuredOutputKeys != null) {\n        this.structuredOutputKeys = metadata.structuredOutputKeys;\n      }\n    }\n\n    this.signature = signature;\n    this.version = `${graph.versions.producer}.${graph.versions.minConsumer}`;\n    const weightMap = this.io.decodeWeights(this.artifacts.weightData, this.artifacts.weightSpecs);\n    this.executor = new GraphExecutor(OperationMapper.Instance.transformGraph(graph, this.signature));\n    this.executor.weightMap = this.convertTensorMapToTensorsMap(weightMap); // Attach a model-level resourceManager to each executor to share resources,\n    // such as `HashTable`.\n\n    this.executor.resourceManager = this.resourceManager;\n\n    if (artifacts.modelInitializer != null && artifacts.modelInitializer.node != null) {\n      const initializer = OperationMapper.Instance.transformGraph(artifacts.modelInitializer);\n      this.initializer = new GraphExecutor(initializer);\n      this.initializer.weightMap = this.executor.weightMap; // Attach a model-level resourceManager to the initializer, the\n      // hashTables created from when executing the initializer will be stored\n      // in the resourceManager.\n\n      this.initializer.resourceManager = this.resourceManager;\n      this.initializer.executeAsync({}, []);\n    }\n\n    return true;\n  }\n  /**\n   * Save the configuration and/or weights of the GraphModel.\n   *\n   * An `IOHandler` is an object that has a `save` method of the proper\n   * signature defined. The `save` method manages the storing or\n   * transmission of serialized data (\"artifacts\") that represent the\n   * model's topology and weights onto or via a specific medium, such as\n   * file downloads, local storage, IndexedDB in the web browser and HTTP\n   * requests to a server. TensorFlow.js provides `IOHandler`\n   * implementations for a number of frequently used saving mediums, such as\n   * `tf.io.browserDownloads` and `tf.io.browserLocalStorage`. See `tf.io`\n   * for more details.\n   *\n   * This method also allows you to refer to certain types of `IOHandler`s\n   * as URL-like string shortcuts, such as 'localstorage://' and\n   * 'indexeddb://'.\n   *\n   * Example 1: Save `model`'s topology and weights to browser [local\n   * storage](https://developer.mozilla.org/en-US/docs/Web/API/Window/localStorage);\n   * then load it back.\n   *\n   * ```js\n   * const modelUrl =\n   *    'https://storage.googleapis.com/tfjs-models/savedmodel/mobilenet_v2_1.0_224/model.json';\n   * const model = await tf.loadGraphModel(modelUrl);\n   * const zeros = tf.zeros([1, 224, 224, 3]);\n   * model.predict(zeros).print();\n   *\n   * const saveResults = await model.save('localstorage://my-model-1');\n   *\n   * const loadedModel = await tf.loadGraphModel('localstorage://my-model-1');\n   * console.log('Prediction from loaded model:');\n   * model.predict(zeros).print();\n   * ```\n   *\n   * @param handlerOrURL An instance of `IOHandler` or a URL-like,\n   * scheme-based string shortcut for `IOHandler`.\n   * @param config Options for saving the model.\n   * @returns A `Promise` of `SaveResult`, which summarizes the result of\n   * the saving, such as byte sizes of the saved artifacts for the model's\n   *   topology and weight values.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes', ignoreCI: true}\n   */\n\n\n  save(handlerOrURL, config) {\n    var _this = this;\n\n    return _asyncToGenerator(function* () {\n      if (typeof handlerOrURL === 'string') {\n        const handlers = _this.io.getSaveHandlers(handlerOrURL);\n\n        if (handlers.length === 0) {\n          throw new Error(`Cannot find any save handlers for URL '${handlerOrURL}'`);\n        } else if (handlers.length > 1) {\n          throw new Error(`Found more than one (${handlers.length}) save handlers for ` + `URL '${handlerOrURL}'`);\n        }\n\n        handlerOrURL = handlers[0];\n      }\n\n      if (handlerOrURL.save == null) {\n        throw new Error('GraphModel.save() cannot proceed because the IOHandler ' + 'provided does not have the `save` attribute defined.');\n      }\n\n      return handlerOrURL.save(_this.artifacts);\n    })();\n  }\n  /**\n   * Execute the inference for the input tensors.\n   *\n   * @param input The input tensors, when there is single input for the model,\n   * inputs param should be a `tf.Tensor`. For models with mutliple inputs,\n   * inputs params should be in either `tf.Tensor`[] if the input order is\n   * fixed, or otherwise NamedTensorMap format.\n   *\n   * For model with multiple inputs, we recommend you use NamedTensorMap as the\n   * input type, if you use `tf.Tensor`[], the order of the array needs to\n   * follow the\n   * order of inputNodes array. @see {@link GraphModel.inputNodes}\n   *\n   * You can also feed any intermediate nodes using the NamedTensorMap as the\n   * input type. For example, given the graph\n   *    InputNode => Intermediate => OutputNode,\n   * you can execute the subgraph Intermediate => OutputNode by calling\n   *    model.execute('IntermediateNode' : tf.tensor(...));\n   *\n   * This is useful for models that uses tf.dynamic_rnn, where the intermediate\n   * state needs to be fed manually.\n   *\n   * For batch inference execution, the tensors for each input need to be\n   * concatenated together. For example with mobilenet, the required input shape\n   * is [1, 244, 244, 3], which represents the [batch, height, width, channel].\n   * If we are provide a batched data of 100 images, the input tensor should be\n   * in the shape of [100, 244, 244, 3].\n   *\n   * @param config Prediction configuration for specifying the batch size.\n   * Currently the batch size option is ignored for graph model.\n   *\n   * @returns Inference result tensors. If the model is converted and it\n   * originally had structured_outputs in tensorflow, then a NamedTensorMap\n   * will be returned matching the structured_outputs. If no structured_outputs\n   * are present, the output will be single `tf.Tensor` if the model has single\n   * output node, otherwise Tensor[].\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n\n\n  predict(inputs, config) {\n    const outputTensors = this.execute(inputs, this.outputNodes);\n\n    if (this.structuredOutputKeys) {\n      const outputTensorsArray = outputTensors instanceof Tensor ? [outputTensors] : outputTensors;\n      const outputTensorMap = {};\n      outputTensorsArray.forEach((outputTensor, i) => outputTensorMap[this.structuredOutputKeys[i]] = outputTensor);\n      return outputTensorMap;\n    }\n\n    return outputTensors;\n  }\n\n  normalizeInputs(inputs) {\n    if (!(inputs instanceof Tensor) && !Array.isArray(inputs)) {\n      // The input is already a NamedTensorMap.\n      return inputs;\n    }\n\n    inputs = Array.isArray(inputs) ? inputs : [inputs];\n\n    if (inputs.length !== this.inputNodes.length) {\n      throw new Error('Input tensor count mismatch,' + `the graph model has ${this.inputNodes.length} placeholders, ` + `while there are ${inputs.length} input tensors.`);\n    }\n\n    return this.inputNodes.reduce((map, inputName, i) => {\n      map[inputName] = inputs[i];\n      return map;\n    }, {});\n  }\n\n  normalizeOutputs(outputs) {\n    outputs = outputs || this.outputNodes;\n    return !Array.isArray(outputs) ? [outputs] : outputs;\n  }\n  /**\n   * Executes inference for the model for given input tensors.\n   * @param inputs tensor, tensor array or tensor map of the inputs for the\n   * model, keyed by the input node names.\n   * @param outputs output node name from the TensorFlow model, if no\n   * outputs are specified, the default outputs of the model would be used.\n   * You can inspect intermediate nodes of the model by adding them to the\n   * outputs array.\n   *\n   * @returns A single tensor if provided with a single output or no outputs\n   * are provided and there is only one default output, otherwise return a\n   * tensor array. The order of the tensor array is the same as the outputs\n   * if provided, otherwise the order of outputNodes attribute of the model.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n\n\n  execute(inputs, outputs) {\n    inputs = this.normalizeInputs(inputs);\n    outputs = this.normalizeOutputs(outputs);\n    const result = this.executor.execute(inputs, outputs);\n    return result.length > 1 ? result : result[0];\n  }\n  /**\n   * Executes inference for the model for given input tensors in async\n   * fashion, use this method when your model contains control flow ops.\n   * @param inputs tensor, tensor array or tensor map of the inputs for the\n   * model, keyed by the input node names.\n   * @param outputs output node name from the TensorFlow model, if no outputs\n   * are specified, the default outputs of the model would be used. You can\n   * inspect intermediate nodes of the model by adding them to the outputs\n   * array.\n   *\n   * @returns A Promise of single tensor if provided with a single output or\n   * no outputs are provided and there is only one default output, otherwise\n   * return a tensor map.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n\n\n  executeAsync(inputs, outputs) {\n    var _this2 = this;\n\n    return _asyncToGenerator(function* () {\n      inputs = _this2.normalizeInputs(inputs);\n      outputs = _this2.normalizeOutputs(outputs);\n      const result = yield _this2.executor.executeAsync(inputs, outputs);\n      return result.length > 1 ? result : result[0];\n    })();\n  }\n  /**\n   * Get intermediate tensors for model debugging mode (flag\n   * KEEP_INTERMEDIATE_TENSORS is true).\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n\n\n  getIntermediateTensors() {\n    return this.executor.getIntermediateTensors();\n  }\n  /**\n   * Dispose intermediate tensors for model debugging mode (flag\n   * KEEP_INTERMEDIATE_TENSORS is true).\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n\n\n  disposeIntermediateTensors() {\n    this.executor.disposeIntermediateTensors();\n  }\n\n  convertTensorMapToTensorsMap(map) {\n    return Object.keys(map).reduce((newMap, key) => {\n      newMap[key] = [map[key]];\n      return newMap;\n    }, {});\n  }\n  /**\n   * Releases the memory used by the weight tensors and resourceManager.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n\n\n  dispose() {\n    this.executor.dispose();\n\n    if (this.initializer) {\n      this.initializer.dispose();\n    }\n\n    this.resourceManager.dispose();\n  }\n\n}\n/**\n * Load a graph model given a URL to the model definition.\n *\n * Example of loading MobileNetV2 from a URL and making a prediction with a\n * zeros input:\n *\n * ```js\n * const modelUrl =\n *    'https://storage.googleapis.com/tfjs-models/savedmodel/mobilenet_v2_1.0_224/model.json';\n * const model = await tf.loadGraphModel(modelUrl);\n * const zeros = tf.zeros([1, 224, 224, 3]);\n * model.predict(zeros).print();\n * ```\n *\n * Example of loading MobileNetV2 from a TF Hub URL and making a prediction\n * with a zeros input:\n *\n * ```js\n * const modelUrl =\n *    'https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/classification/2';\n * const model = await tf.loadGraphModel(modelUrl, {fromTFHub: true});\n * const zeros = tf.zeros([1, 224, 224, 3]);\n * model.predict(zeros).print();\n * ```\n * @param modelUrl The url or an `io.IOHandler` that loads the model.\n * @param options Options for the HTTP request, which allows to send\n *     credentials\n *    and custom headers.\n *\n * @doc {heading: 'Models', subheading: 'Loading'}\n */\n\nexport function loadGraphModel(_x) {\n  return (_loadGraphModel = _loadGraphModel || _asyncToGenerator(function* (modelUrl, options = {}, tfio = io) {\n    if (modelUrl == null) {\n      throw new Error('modelUrl in loadGraphModel() cannot be null. Please provide a url ' + 'or an IOHandler that loads the model');\n    }\n\n    if (options == null) {\n      options = {};\n    }\n\n    if (options.fromTFHub && typeof modelUrl === 'string') {\n      modelUrl = getTFHubUrl(modelUrl);\n    }\n\n    const model = new GraphModel(modelUrl, options, tfio);\n    yield model.load();\n    return model;\n  })).apply(this, arguments);\n}\n/**\n * Load a graph model given a synchronous IO handler with a 'load' method.\n *\n * @param modelSource The `io.IOHandlerSync` that loads the model, or the\n *     `io.ModelArtifacts` that encode the model, or a tuple of\n *     `[io.ModelJSON, ArrayBuffer]` of which the first element encodes the\n *      model and the second contains the weights.\n *\n * @doc {heading: 'Models', subheading: 'Loading'}\n */\n\nexport function loadGraphModelSync(modelSource) {\n  if (modelSource == null) {\n    throw new Error('modelUrl in loadGraphModelSync() cannot be null. Please provide ' + 'model artifacts or an IOHandler that loads the model');\n  }\n\n  let ioHandler;\n\n  if (modelSource instanceof Array) {\n    const [modelJSON, weights] = modelSource;\n\n    if (!modelJSON) {\n      throw new Error('modelJSON must be the first element of the array');\n    }\n\n    if (!weights || !(weights instanceof ArrayBuffer)) {\n      throw new Error('An ArrayBuffer of weights must be the second element of' + ' the array');\n    }\n\n    if (!('modelTopology' in modelJSON)) {\n      throw new Error('Model JSON is missing \\'modelTopology\\'');\n    }\n\n    if (!('weightsManifest' in modelJSON)) {\n      throw new Error('Model JSON is missing \\'weightsManifest\\'');\n    }\n\n    const weightSpecs = io.getWeightSpecs(modelJSON.weightsManifest);\n    const modelArtifacts = io.getModelArtifactsForJSONSync(modelJSON, weightSpecs, weights);\n    ioHandler = io.fromMemorySync(modelArtifacts);\n  } else if ('load' in modelSource) {\n    // Then modelSource is already an IOHandlerSync.\n    ioHandler = modelSource;\n  } else if ('modelTopology' in modelSource && 'weightSpecs' in modelSource && 'weightData' in modelSource) {\n    // modelSource is of type ModelArtifacts.\n    ioHandler = io.fromMemorySync(modelSource);\n  } else {\n    throw new Error('Unknown model format');\n  }\n\n  const model = new GraphModel(ioHandler);\n  model.load();\n  return model;\n}\n\nfunction getTFHubUrl(modelUrl) {\n  if (!modelUrl.endsWith('/')) {\n    modelUrl = modelUrl + '/';\n  }\n\n  return `${modelUrl}${DEFAULT_MODEL_NAME}${TFHUB_SEARCH_PARAM}`;\n}","map":{"version":3,"names":["io","Tensor","util","OperationMapper","GraphExecutor","ResourceManager","TFHUB_SEARCH_PARAM","DEFAULT_MODEL_NAME","GraphModel","constructor","modelUrl","loadOptions","tfio","version","resourceManager","modelVersion","inputNodes","executor","outputNodes","inputs","outputs","weights","weightMap","metadata","artifacts","userDefinedMetadata","modelSignature","signature","modelStructuredOutputKeys","structuredOutputKeys","findIOHandler","path","load","handler","requestInit","browserHTTPRequest","handlers","getLoadHandlers","length","push","Error","loadResult","isPromise","then","loadSync","graph","modelTopology","versions","producer","minConsumer","decodeWeights","weightData","weightSpecs","Instance","transformGraph","convertTensorMapToTensorsMap","modelInitializer","node","initializer","executeAsync","save","handlerOrURL","config","getSaveHandlers","predict","outputTensors","execute","outputTensorsArray","outputTensorMap","forEach","outputTensor","i","normalizeInputs","Array","isArray","reduce","map","inputName","normalizeOutputs","result","getIntermediateTensors","disposeIntermediateTensors","Object","keys","newMap","key","dispose","loadGraphModel","options","fromTFHub","getTFHubUrl","model","loadGraphModelSync","modelSource","ioHandler","modelJSON","ArrayBuffer","getWeightSpecs","weightsManifest","modelArtifacts","getModelArtifactsForJSONSync","fromMemorySync","endsWith"],"sources":["C:/Users/Anke/Documents/Feelix documents/Feelix2.0-dev/Feelix v2/node_modules/@tensorflow/tfjs-converter/dist/executor/graph_model.js"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { io, Tensor, util } from '@tensorflow/tfjs-core';\nimport { OperationMapper } from '../operations/operation_mapper';\nimport { GraphExecutor } from './graph_executor';\nimport { ResourceManager } from './resource_manager';\nexport const TFHUB_SEARCH_PARAM = '?tfjs-format=file';\nexport const DEFAULT_MODEL_NAME = 'model.json';\n/**\n * A `tf.GraphModel` is a directed, acyclic graph built from a\n * SavedModel GraphDef and allows inference execution.\n *\n * A `tf.GraphModel` can only be created by loading from a model converted from\n * a [TensorFlow SavedModel](https://www.tensorflow.org/guide/saved_model) using\n * the command line converter tool and loaded via `tf.loadGraphModel`.\n *\n * @doc {heading: 'Models', subheading: 'Classes'}\n */\nexport class GraphModel {\n    /**\n     * @param modelUrl url for the model, or an `io.IOHandler`.\n     * @param weightManifestUrl url for the weight file generated by\n     * scripts/convert.py script.\n     * @param requestOption options for Request, which allows to send credentials\n     * and custom headers.\n     * @param onProgress Optional, progress callback function, fired periodically\n     * before the load is completed.\n     */\n    constructor(modelUrl, loadOptions = {}, tfio = io) {\n        this.modelUrl = modelUrl;\n        this.loadOptions = loadOptions;\n        this.version = 'n/a';\n        this.io = tfio;\n        if (loadOptions == null) {\n            this.loadOptions = {};\n        }\n        this.resourceManager = new ResourceManager();\n    }\n    // Returns the version information for the tensorflow model GraphDef.\n    get modelVersion() {\n        return this.version;\n    }\n    get inputNodes() {\n        return this.executor.inputNodes;\n    }\n    get outputNodes() {\n        return this.executor.outputNodes;\n    }\n    get inputs() {\n        return this.executor.inputs;\n    }\n    get outputs() {\n        return this.executor.outputs;\n    }\n    get weights() {\n        return this.executor.weightMap;\n    }\n    get metadata() {\n        return this.artifacts.userDefinedMetadata;\n    }\n    get modelSignature() {\n        return this.signature;\n    }\n    get modelStructuredOutputKeys() {\n        return this.structuredOutputKeys;\n    }\n    findIOHandler() {\n        const path = this.modelUrl;\n        if (path.load != null) {\n            // Path is an IO Handler.\n            this.handler = path;\n        }\n        else if (this.loadOptions.requestInit != null) {\n            this.handler = this.io.browserHTTPRequest(path, this.loadOptions);\n        }\n        else {\n            const handlers = this.io.getLoadHandlers(path, this.loadOptions);\n            if (handlers.length === 0) {\n                // For backward compatibility: if no load handler can be found,\n                // assume it is a relative http path.\n                handlers.push(this.io.browserHTTPRequest(path, this.loadOptions));\n            }\n            else if (handlers.length > 1) {\n                throw new Error(`Found more than one (${handlers.length}) load handlers for ` +\n                    `URL '${[path]}'`);\n            }\n            this.handler = handlers[0];\n        }\n    }\n    /**\n     * Loads the model and weight files, construct the in memory weight map and\n     * compile the inference graph.\n     */\n    load() {\n        this.findIOHandler();\n        if (this.handler.load == null) {\n            throw new Error('Cannot proceed with model loading because the IOHandler provided ' +\n                'does not have the `load` method implemented.');\n        }\n        const loadResult = this.handler.load();\n        if (util.isPromise(loadResult)) {\n            return loadResult.then(artifacts => this.loadSync(artifacts));\n        }\n        return this.loadSync(loadResult);\n    }\n    /**\n     * Synchronously construct the in memory weight map and\n     * compile the inference graph. Also initialize hashtable if any.\n     *\n     * @doc {heading: 'Models', subheading: 'Classes', ignoreCI: true}\n     */\n    loadSync(artifacts) {\n        this.artifacts = artifacts;\n        const graph = this.artifacts.modelTopology;\n        let signature = this.artifacts.signature;\n        if (this.artifacts.userDefinedMetadata != null) {\n            const metadata = this.artifacts.userDefinedMetadata;\n            if (metadata.signature != null) {\n                signature = metadata.signature;\n            }\n            if (metadata.structuredOutputKeys != null) {\n                this.structuredOutputKeys = metadata.structuredOutputKeys;\n            }\n        }\n        this.signature = signature;\n        this.version = `${graph.versions.producer}.${graph.versions.minConsumer}`;\n        const weightMap = this.io.decodeWeights(this.artifacts.weightData, this.artifacts.weightSpecs);\n        this.executor = new GraphExecutor(OperationMapper.Instance.transformGraph(graph, this.signature));\n        this.executor.weightMap = this.convertTensorMapToTensorsMap(weightMap);\n        // Attach a model-level resourceManager to each executor to share resources,\n        // such as `HashTable`.\n        this.executor.resourceManager = this.resourceManager;\n        if (artifacts.modelInitializer != null &&\n            artifacts.modelInitializer.node != null) {\n            const initializer = OperationMapper.Instance.transformGraph(artifacts.modelInitializer);\n            this.initializer = new GraphExecutor(initializer);\n            this.initializer.weightMap = this.executor.weightMap;\n            // Attach a model-level resourceManager to the initializer, the\n            // hashTables created from when executing the initializer will be stored\n            // in the resourceManager.\n            this.initializer.resourceManager = this.resourceManager;\n            this.initializer.executeAsync({}, []);\n        }\n        return true;\n    }\n    /**\n     * Save the configuration and/or weights of the GraphModel.\n     *\n     * An `IOHandler` is an object that has a `save` method of the proper\n     * signature defined. The `save` method manages the storing or\n     * transmission of serialized data (\"artifacts\") that represent the\n     * model's topology and weights onto or via a specific medium, such as\n     * file downloads, local storage, IndexedDB in the web browser and HTTP\n     * requests to a server. TensorFlow.js provides `IOHandler`\n     * implementations for a number of frequently used saving mediums, such as\n     * `tf.io.browserDownloads` and `tf.io.browserLocalStorage`. See `tf.io`\n     * for more details.\n     *\n     * This method also allows you to refer to certain types of `IOHandler`s\n     * as URL-like string shortcuts, such as 'localstorage://' and\n     * 'indexeddb://'.\n     *\n     * Example 1: Save `model`'s topology and weights to browser [local\n     * storage](https://developer.mozilla.org/en-US/docs/Web/API/Window/localStorage);\n     * then load it back.\n     *\n     * ```js\n     * const modelUrl =\n     *    'https://storage.googleapis.com/tfjs-models/savedmodel/mobilenet_v2_1.0_224/model.json';\n     * const model = await tf.loadGraphModel(modelUrl);\n     * const zeros = tf.zeros([1, 224, 224, 3]);\n     * model.predict(zeros).print();\n     *\n     * const saveResults = await model.save('localstorage://my-model-1');\n     *\n     * const loadedModel = await tf.loadGraphModel('localstorage://my-model-1');\n     * console.log('Prediction from loaded model:');\n     * model.predict(zeros).print();\n     * ```\n     *\n     * @param handlerOrURL An instance of `IOHandler` or a URL-like,\n     * scheme-based string shortcut for `IOHandler`.\n     * @param config Options for saving the model.\n     * @returns A `Promise` of `SaveResult`, which summarizes the result of\n     * the saving, such as byte sizes of the saved artifacts for the model's\n     *   topology and weight values.\n     *\n     * @doc {heading: 'Models', subheading: 'Classes', ignoreCI: true}\n     */\n    async save(handlerOrURL, config) {\n        if (typeof handlerOrURL === 'string') {\n            const handlers = this.io.getSaveHandlers(handlerOrURL);\n            if (handlers.length === 0) {\n                throw new Error(`Cannot find any save handlers for URL '${handlerOrURL}'`);\n            }\n            else if (handlers.length > 1) {\n                throw new Error(`Found more than one (${handlers.length}) save handlers for ` +\n                    `URL '${handlerOrURL}'`);\n            }\n            handlerOrURL = handlers[0];\n        }\n        if (handlerOrURL.save == null) {\n            throw new Error('GraphModel.save() cannot proceed because the IOHandler ' +\n                'provided does not have the `save` attribute defined.');\n        }\n        return handlerOrURL.save(this.artifacts);\n    }\n    /**\n     * Execute the inference for the input tensors.\n     *\n     * @param input The input tensors, when there is single input for the model,\n     * inputs param should be a `tf.Tensor`. For models with mutliple inputs,\n     * inputs params should be in either `tf.Tensor`[] if the input order is\n     * fixed, or otherwise NamedTensorMap format.\n     *\n     * For model with multiple inputs, we recommend you use NamedTensorMap as the\n     * input type, if you use `tf.Tensor`[], the order of the array needs to\n     * follow the\n     * order of inputNodes array. @see {@link GraphModel.inputNodes}\n     *\n     * You can also feed any intermediate nodes using the NamedTensorMap as the\n     * input type. For example, given the graph\n     *    InputNode => Intermediate => OutputNode,\n     * you can execute the subgraph Intermediate => OutputNode by calling\n     *    model.execute('IntermediateNode' : tf.tensor(...));\n     *\n     * This is useful for models that uses tf.dynamic_rnn, where the intermediate\n     * state needs to be fed manually.\n     *\n     * For batch inference execution, the tensors for each input need to be\n     * concatenated together. For example with mobilenet, the required input shape\n     * is [1, 244, 244, 3], which represents the [batch, height, width, channel].\n     * If we are provide a batched data of 100 images, the input tensor should be\n     * in the shape of [100, 244, 244, 3].\n     *\n     * @param config Prediction configuration for specifying the batch size.\n     * Currently the batch size option is ignored for graph model.\n     *\n     * @returns Inference result tensors. If the model is converted and it\n     * originally had structured_outputs in tensorflow, then a NamedTensorMap\n     * will be returned matching the structured_outputs. If no structured_outputs\n     * are present, the output will be single `tf.Tensor` if the model has single\n     * output node, otherwise Tensor[].\n     *\n     * @doc {heading: 'Models', subheading: 'Classes'}\n     */\n    predict(inputs, config) {\n        const outputTensors = this.execute(inputs, this.outputNodes);\n        if (this.structuredOutputKeys) {\n            const outputTensorsArray = outputTensors instanceof Tensor ? [outputTensors] : outputTensors;\n            const outputTensorMap = {};\n            outputTensorsArray.forEach((outputTensor, i) => outputTensorMap[this.structuredOutputKeys[i]] =\n                outputTensor);\n            return outputTensorMap;\n        }\n        return outputTensors;\n    }\n    normalizeInputs(inputs) {\n        if (!(inputs instanceof Tensor) && !Array.isArray(inputs)) {\n            // The input is already a NamedTensorMap.\n            return inputs;\n        }\n        inputs = Array.isArray(inputs) ? inputs : [inputs];\n        if (inputs.length !== this.inputNodes.length) {\n            throw new Error('Input tensor count mismatch,' +\n                `the graph model has ${this.inputNodes.length} placeholders, ` +\n                `while there are ${inputs.length} input tensors.`);\n        }\n        return this.inputNodes.reduce((map, inputName, i) => {\n            map[inputName] = inputs[i];\n            return map;\n        }, {});\n    }\n    normalizeOutputs(outputs) {\n        outputs = outputs || this.outputNodes;\n        return !Array.isArray(outputs) ? [outputs] : outputs;\n    }\n    /**\n     * Executes inference for the model for given input tensors.\n     * @param inputs tensor, tensor array or tensor map of the inputs for the\n     * model, keyed by the input node names.\n     * @param outputs output node name from the TensorFlow model, if no\n     * outputs are specified, the default outputs of the model would be used.\n     * You can inspect intermediate nodes of the model by adding them to the\n     * outputs array.\n     *\n     * @returns A single tensor if provided with a single output or no outputs\n     * are provided and there is only one default output, otherwise return a\n     * tensor array. The order of the tensor array is the same as the outputs\n     * if provided, otherwise the order of outputNodes attribute of the model.\n     *\n     * @doc {heading: 'Models', subheading: 'Classes'}\n     */\n    execute(inputs, outputs) {\n        inputs = this.normalizeInputs(inputs);\n        outputs = this.normalizeOutputs(outputs);\n        const result = this.executor.execute(inputs, outputs);\n        return result.length > 1 ? result : result[0];\n    }\n    /**\n     * Executes inference for the model for given input tensors in async\n     * fashion, use this method when your model contains control flow ops.\n     * @param inputs tensor, tensor array or tensor map of the inputs for the\n     * model, keyed by the input node names.\n     * @param outputs output node name from the TensorFlow model, if no outputs\n     * are specified, the default outputs of the model would be used. You can\n     * inspect intermediate nodes of the model by adding them to the outputs\n     * array.\n     *\n     * @returns A Promise of single tensor if provided with a single output or\n     * no outputs are provided and there is only one default output, otherwise\n     * return a tensor map.\n     *\n     * @doc {heading: 'Models', subheading: 'Classes'}\n     */\n    async executeAsync(inputs, outputs) {\n        inputs = this.normalizeInputs(inputs);\n        outputs = this.normalizeOutputs(outputs);\n        const result = await this.executor.executeAsync(inputs, outputs);\n        return result.length > 1 ? result : result[0];\n    }\n    /**\n     * Get intermediate tensors for model debugging mode (flag\n     * KEEP_INTERMEDIATE_TENSORS is true).\n     *\n     * @doc {heading: 'Models', subheading: 'Classes'}\n     */\n    getIntermediateTensors() {\n        return this.executor.getIntermediateTensors();\n    }\n    /**\n     * Dispose intermediate tensors for model debugging mode (flag\n     * KEEP_INTERMEDIATE_TENSORS is true).\n     *\n     * @doc {heading: 'Models', subheading: 'Classes'}\n     */\n    disposeIntermediateTensors() {\n        this.executor.disposeIntermediateTensors();\n    }\n    convertTensorMapToTensorsMap(map) {\n        return Object.keys(map).reduce((newMap, key) => {\n            newMap[key] = [map[key]];\n            return newMap;\n        }, {});\n    }\n    /**\n     * Releases the memory used by the weight tensors and resourceManager.\n     *\n     * @doc {heading: 'Models', subheading: 'Classes'}\n     */\n    dispose() {\n        this.executor.dispose();\n        if (this.initializer) {\n            this.initializer.dispose();\n        }\n        this.resourceManager.dispose();\n    }\n}\n/**\n * Load a graph model given a URL to the model definition.\n *\n * Example of loading MobileNetV2 from a URL and making a prediction with a\n * zeros input:\n *\n * ```js\n * const modelUrl =\n *    'https://storage.googleapis.com/tfjs-models/savedmodel/mobilenet_v2_1.0_224/model.json';\n * const model = await tf.loadGraphModel(modelUrl);\n * const zeros = tf.zeros([1, 224, 224, 3]);\n * model.predict(zeros).print();\n * ```\n *\n * Example of loading MobileNetV2 from a TF Hub URL and making a prediction\n * with a zeros input:\n *\n * ```js\n * const modelUrl =\n *    'https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/classification/2';\n * const model = await tf.loadGraphModel(modelUrl, {fromTFHub: true});\n * const zeros = tf.zeros([1, 224, 224, 3]);\n * model.predict(zeros).print();\n * ```\n * @param modelUrl The url or an `io.IOHandler` that loads the model.\n * @param options Options for the HTTP request, which allows to send\n *     credentials\n *    and custom headers.\n *\n * @doc {heading: 'Models', subheading: 'Loading'}\n */\nexport async function loadGraphModel(modelUrl, options = {}, tfio = io) {\n    if (modelUrl == null) {\n        throw new Error('modelUrl in loadGraphModel() cannot be null. Please provide a url ' +\n            'or an IOHandler that loads the model');\n    }\n    if (options == null) {\n        options = {};\n    }\n    if (options.fromTFHub && typeof modelUrl === 'string') {\n        modelUrl = getTFHubUrl(modelUrl);\n    }\n    const model = new GraphModel(modelUrl, options, tfio);\n    await model.load();\n    return model;\n}\n/**\n * Load a graph model given a synchronous IO handler with a 'load' method.\n *\n * @param modelSource The `io.IOHandlerSync` that loads the model, or the\n *     `io.ModelArtifacts` that encode the model, or a tuple of\n *     `[io.ModelJSON, ArrayBuffer]` of which the first element encodes the\n *      model and the second contains the weights.\n *\n * @doc {heading: 'Models', subheading: 'Loading'}\n */\nexport function loadGraphModelSync(modelSource) {\n    if (modelSource == null) {\n        throw new Error('modelUrl in loadGraphModelSync() cannot be null. Please provide ' +\n            'model artifacts or an IOHandler that loads the model');\n    }\n    let ioHandler;\n    if (modelSource instanceof Array) {\n        const [modelJSON, weights] = modelSource;\n        if (!modelJSON) {\n            throw new Error('modelJSON must be the first element of the array');\n        }\n        if (!weights || !(weights instanceof ArrayBuffer)) {\n            throw new Error('An ArrayBuffer of weights must be the second element of'\n                + ' the array');\n        }\n        if (!('modelTopology' in modelJSON)) {\n            throw new Error('Model JSON is missing \\'modelTopology\\'');\n        }\n        if (!('weightsManifest' in modelJSON)) {\n            throw new Error('Model JSON is missing \\'weightsManifest\\'');\n        }\n        const weightSpecs = io.getWeightSpecs(modelJSON.weightsManifest);\n        const modelArtifacts = io.getModelArtifactsForJSONSync(modelJSON, weightSpecs, weights);\n        ioHandler = io.fromMemorySync(modelArtifacts);\n    }\n    else if ('load' in modelSource) {\n        // Then modelSource is already an IOHandlerSync.\n        ioHandler = modelSource;\n    }\n    else if ('modelTopology' in modelSource && 'weightSpecs' in modelSource\n        && 'weightData' in modelSource) {\n        // modelSource is of type ModelArtifacts.\n        ioHandler = io.fromMemorySync(modelSource);\n    }\n    else {\n        throw new Error('Unknown model format');\n    }\n    const model = new GraphModel(ioHandler);\n    model.load();\n    return model;\n}\nfunction getTFHubUrl(modelUrl) {\n    if (!modelUrl.endsWith('/')) {\n        modelUrl = (modelUrl) + '/';\n    }\n    return `${modelUrl}${DEFAULT_MODEL_NAME}${TFHUB_SEARCH_PARAM}`;\n}\n"],"mappings":";;;;AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAASA,EAAT,EAAaC,MAAb,EAAqBC,IAArB,QAAiC,uBAAjC;AACA,SAASC,eAAT,QAAgC,gCAAhC;AACA,SAASC,aAAT,QAA8B,kBAA9B;AACA,SAASC,eAAT,QAAgC,oBAAhC;AACA,OAAO,MAAMC,kBAAkB,GAAG,mBAA3B;AACP,OAAO,MAAMC,kBAAkB,GAAG,YAA3B;AACP;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AACA,OAAO,MAAMC,UAAN,CAAiB;EACpB;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACIC,WAAW,CAACC,QAAD,EAAWC,WAAW,GAAG,EAAzB,EAA6BC,IAAI,GAAGZ,EAApC,EAAwC;IAC/C,KAAKU,QAAL,GAAgBA,QAAhB;IACA,KAAKC,WAAL,GAAmBA,WAAnB;IACA,KAAKE,OAAL,GAAe,KAAf;IACA,KAAKb,EAAL,GAAUY,IAAV;;IACA,IAAID,WAAW,IAAI,IAAnB,EAAyB;MACrB,KAAKA,WAAL,GAAmB,EAAnB;IACH;;IACD,KAAKG,eAAL,GAAuB,IAAIT,eAAJ,EAAvB;EACH,CAnBmB,CAoBpB;;;EACgB,IAAZU,YAAY,GAAG;IACf,OAAO,KAAKF,OAAZ;EACH;;EACa,IAAVG,UAAU,GAAG;IACb,OAAO,KAAKC,QAAL,CAAcD,UAArB;EACH;;EACc,IAAXE,WAAW,GAAG;IACd,OAAO,KAAKD,QAAL,CAAcC,WAArB;EACH;;EACS,IAANC,MAAM,GAAG;IACT,OAAO,KAAKF,QAAL,CAAcE,MAArB;EACH;;EACU,IAAPC,OAAO,GAAG;IACV,OAAO,KAAKH,QAAL,CAAcG,OAArB;EACH;;EACU,IAAPC,OAAO,GAAG;IACV,OAAO,KAAKJ,QAAL,CAAcK,SAArB;EACH;;EACW,IAARC,QAAQ,GAAG;IACX,OAAO,KAAKC,SAAL,CAAeC,mBAAtB;EACH;;EACiB,IAAdC,cAAc,GAAG;IACjB,OAAO,KAAKC,SAAZ;EACH;;EAC4B,IAAzBC,yBAAyB,GAAG;IAC5B,OAAO,KAAKC,oBAAZ;EACH;;EACDC,aAAa,GAAG;IACZ,MAAMC,IAAI,GAAG,KAAKrB,QAAlB;;IACA,IAAIqB,IAAI,CAACC,IAAL,IAAa,IAAjB,EAAuB;MACnB;MACA,KAAKC,OAAL,GAAeF,IAAf;IACH,CAHD,MAIK,IAAI,KAAKpB,WAAL,CAAiBuB,WAAjB,IAAgC,IAApC,EAA0C;MAC3C,KAAKD,OAAL,GAAe,KAAKjC,EAAL,CAAQmC,kBAAR,CAA2BJ,IAA3B,EAAiC,KAAKpB,WAAtC,CAAf;IACH,CAFI,MAGA;MACD,MAAMyB,QAAQ,GAAG,KAAKpC,EAAL,CAAQqC,eAAR,CAAwBN,IAAxB,EAA8B,KAAKpB,WAAnC,CAAjB;;MACA,IAAIyB,QAAQ,CAACE,MAAT,KAAoB,CAAxB,EAA2B;QACvB;QACA;QACAF,QAAQ,CAACG,IAAT,CAAc,KAAKvC,EAAL,CAAQmC,kBAAR,CAA2BJ,IAA3B,EAAiC,KAAKpB,WAAtC,CAAd;MACH,CAJD,MAKK,IAAIyB,QAAQ,CAACE,MAAT,GAAkB,CAAtB,EAAyB;QAC1B,MAAM,IAAIE,KAAJ,CAAW,wBAAuBJ,QAAQ,CAACE,MAAO,sBAAxC,GACX,QAAO,CAACP,IAAD,CAAO,GADb,CAAN;MAEH;;MACD,KAAKE,OAAL,GAAeG,QAAQ,CAAC,CAAD,CAAvB;IACH;EACJ;EACD;AACJ;AACA;AACA;;;EACIJ,IAAI,GAAG;IACH,KAAKF,aAAL;;IACA,IAAI,KAAKG,OAAL,CAAaD,IAAb,IAAqB,IAAzB,EAA+B;MAC3B,MAAM,IAAIQ,KAAJ,CAAU,sEACZ,8CADE,CAAN;IAEH;;IACD,MAAMC,UAAU,GAAG,KAAKR,OAAL,CAAaD,IAAb,EAAnB;;IACA,IAAI9B,IAAI,CAACwC,SAAL,CAAeD,UAAf,CAAJ,EAAgC;MAC5B,OAAOA,UAAU,CAACE,IAAX,CAAgBnB,SAAS,IAAI,KAAKoB,QAAL,CAAcpB,SAAd,CAA7B,CAAP;IACH;;IACD,OAAO,KAAKoB,QAAL,CAAcH,UAAd,CAAP;EACH;EACD;AACJ;AACA;AACA;AACA;AACA;;;EACIG,QAAQ,CAACpB,SAAD,EAAY;IAChB,KAAKA,SAAL,GAAiBA,SAAjB;IACA,MAAMqB,KAAK,GAAG,KAAKrB,SAAL,CAAesB,aAA7B;IACA,IAAInB,SAAS,GAAG,KAAKH,SAAL,CAAeG,SAA/B;;IACA,IAAI,KAAKH,SAAL,CAAeC,mBAAf,IAAsC,IAA1C,EAAgD;MAC5C,MAAMF,QAAQ,GAAG,KAAKC,SAAL,CAAeC,mBAAhC;;MACA,IAAIF,QAAQ,CAACI,SAAT,IAAsB,IAA1B,EAAgC;QAC5BA,SAAS,GAAGJ,QAAQ,CAACI,SAArB;MACH;;MACD,IAAIJ,QAAQ,CAACM,oBAAT,IAAiC,IAArC,EAA2C;QACvC,KAAKA,oBAAL,GAA4BN,QAAQ,CAACM,oBAArC;MACH;IACJ;;IACD,KAAKF,SAAL,GAAiBA,SAAjB;IACA,KAAKd,OAAL,GAAgB,GAAEgC,KAAK,CAACE,QAAN,CAAeC,QAAS,IAAGH,KAAK,CAACE,QAAN,CAAeE,WAAY,EAAxE;IACA,MAAM3B,SAAS,GAAG,KAAKtB,EAAL,CAAQkD,aAAR,CAAsB,KAAK1B,SAAL,CAAe2B,UAArC,EAAiD,KAAK3B,SAAL,CAAe4B,WAAhE,CAAlB;IACA,KAAKnC,QAAL,GAAgB,IAAIb,aAAJ,CAAkBD,eAAe,CAACkD,QAAhB,CAAyBC,cAAzB,CAAwCT,KAAxC,EAA+C,KAAKlB,SAApD,CAAlB,CAAhB;IACA,KAAKV,QAAL,CAAcK,SAAd,GAA0B,KAAKiC,4BAAL,CAAkCjC,SAAlC,CAA1B,CAjBgB,CAkBhB;IACA;;IACA,KAAKL,QAAL,CAAcH,eAAd,GAAgC,KAAKA,eAArC;;IACA,IAAIU,SAAS,CAACgC,gBAAV,IAA8B,IAA9B,IACAhC,SAAS,CAACgC,gBAAV,CAA2BC,IAA3B,IAAmC,IADvC,EAC6C;MACzC,MAAMC,WAAW,GAAGvD,eAAe,CAACkD,QAAhB,CAAyBC,cAAzB,CAAwC9B,SAAS,CAACgC,gBAAlD,CAApB;MACA,KAAKE,WAAL,GAAmB,IAAItD,aAAJ,CAAkBsD,WAAlB,CAAnB;MACA,KAAKA,WAAL,CAAiBpC,SAAjB,GAA6B,KAAKL,QAAL,CAAcK,SAA3C,CAHyC,CAIzC;MACA;MACA;;MACA,KAAKoC,WAAL,CAAiB5C,eAAjB,GAAmC,KAAKA,eAAxC;MACA,KAAK4C,WAAL,CAAiBC,YAAjB,CAA8B,EAA9B,EAAkC,EAAlC;IACH;;IACD,OAAO,IAAP;EACH;EACD;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;EACUC,IAAI,CAACC,YAAD,EAAeC,MAAf,EAAuB;IAAA;;IAAA;MAC7B,IAAI,OAAOD,YAAP,KAAwB,QAA5B,EAAsC;QAClC,MAAMzB,QAAQ,GAAG,KAAI,CAACpC,EAAL,CAAQ+D,eAAR,CAAwBF,YAAxB,CAAjB;;QACA,IAAIzB,QAAQ,CAACE,MAAT,KAAoB,CAAxB,EAA2B;UACvB,MAAM,IAAIE,KAAJ,CAAW,0CAAyCqB,YAAa,GAAjE,CAAN;QACH,CAFD,MAGK,IAAIzB,QAAQ,CAACE,MAAT,GAAkB,CAAtB,EAAyB;UAC1B,MAAM,IAAIE,KAAJ,CAAW,wBAAuBJ,QAAQ,CAACE,MAAO,sBAAxC,GACX,QAAOuB,YAAa,GADnB,CAAN;QAEH;;QACDA,YAAY,GAAGzB,QAAQ,CAAC,CAAD,CAAvB;MACH;;MACD,IAAIyB,YAAY,CAACD,IAAb,IAAqB,IAAzB,EAA+B;QAC3B,MAAM,IAAIpB,KAAJ,CAAU,4DACZ,sDADE,CAAN;MAEH;;MACD,OAAOqB,YAAY,CAACD,IAAb,CAAkB,KAAI,CAACpC,SAAvB,CAAP;IAhB6B;EAiBhC;EACD;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;EACIwC,OAAO,CAAC7C,MAAD,EAAS2C,MAAT,EAAiB;IACpB,MAAMG,aAAa,GAAG,KAAKC,OAAL,CAAa/C,MAAb,EAAqB,KAAKD,WAA1B,CAAtB;;IACA,IAAI,KAAKW,oBAAT,EAA+B;MAC3B,MAAMsC,kBAAkB,GAAGF,aAAa,YAAYhE,MAAzB,GAAkC,CAACgE,aAAD,CAAlC,GAAoDA,aAA/E;MACA,MAAMG,eAAe,GAAG,EAAxB;MACAD,kBAAkB,CAACE,OAAnB,CAA2B,CAACC,YAAD,EAAeC,CAAf,KAAqBH,eAAe,CAAC,KAAKvC,oBAAL,CAA0B0C,CAA1B,CAAD,CAAf,GAC5CD,YADJ;MAEA,OAAOF,eAAP;IACH;;IACD,OAAOH,aAAP;EACH;;EACDO,eAAe,CAACrD,MAAD,EAAS;IACpB,IAAI,EAAEA,MAAM,YAAYlB,MAApB,KAA+B,CAACwE,KAAK,CAACC,OAAN,CAAcvD,MAAd,CAApC,EAA2D;MACvD;MACA,OAAOA,MAAP;IACH;;IACDA,MAAM,GAAGsD,KAAK,CAACC,OAAN,CAAcvD,MAAd,IAAwBA,MAAxB,GAAiC,CAACA,MAAD,CAA1C;;IACA,IAAIA,MAAM,CAACmB,MAAP,KAAkB,KAAKtB,UAAL,CAAgBsB,MAAtC,EAA8C;MAC1C,MAAM,IAAIE,KAAJ,CAAU,iCACX,uBAAsB,KAAKxB,UAAL,CAAgBsB,MAAO,iBADlC,GAEX,mBAAkBnB,MAAM,CAACmB,MAAO,iBAF/B,CAAN;IAGH;;IACD,OAAO,KAAKtB,UAAL,CAAgB2D,MAAhB,CAAuB,CAACC,GAAD,EAAMC,SAAN,EAAiBN,CAAjB,KAAuB;MACjDK,GAAG,CAACC,SAAD,CAAH,GAAiB1D,MAAM,CAACoD,CAAD,CAAvB;MACA,OAAOK,GAAP;IACH,CAHM,EAGJ,EAHI,CAAP;EAIH;;EACDE,gBAAgB,CAAC1D,OAAD,EAAU;IACtBA,OAAO,GAAGA,OAAO,IAAI,KAAKF,WAA1B;IACA,OAAO,CAACuD,KAAK,CAACC,OAAN,CAActD,OAAd,CAAD,GAA0B,CAACA,OAAD,CAA1B,GAAsCA,OAA7C;EACH;EACD;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;EACI8C,OAAO,CAAC/C,MAAD,EAASC,OAAT,EAAkB;IACrBD,MAAM,GAAG,KAAKqD,eAAL,CAAqBrD,MAArB,CAAT;IACAC,OAAO,GAAG,KAAK0D,gBAAL,CAAsB1D,OAAtB,CAAV;IACA,MAAM2D,MAAM,GAAG,KAAK9D,QAAL,CAAciD,OAAd,CAAsB/C,MAAtB,EAA8BC,OAA9B,CAAf;IACA,OAAO2D,MAAM,CAACzC,MAAP,GAAgB,CAAhB,GAAoByC,MAApB,GAA6BA,MAAM,CAAC,CAAD,CAA1C;EACH;EACD;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;EACUpB,YAAY,CAACxC,MAAD,EAASC,OAAT,EAAkB;IAAA;;IAAA;MAChCD,MAAM,GAAG,MAAI,CAACqD,eAAL,CAAqBrD,MAArB,CAAT;MACAC,OAAO,GAAG,MAAI,CAAC0D,gBAAL,CAAsB1D,OAAtB,CAAV;MACA,MAAM2D,MAAM,SAAS,MAAI,CAAC9D,QAAL,CAAc0C,YAAd,CAA2BxC,MAA3B,EAAmCC,OAAnC,CAArB;MACA,OAAO2D,MAAM,CAACzC,MAAP,GAAgB,CAAhB,GAAoByC,MAApB,GAA6BA,MAAM,CAAC,CAAD,CAA1C;IAJgC;EAKnC;EACD;AACJ;AACA;AACA;AACA;AACA;;;EACIC,sBAAsB,GAAG;IACrB,OAAO,KAAK/D,QAAL,CAAc+D,sBAAd,EAAP;EACH;EACD;AACJ;AACA;AACA;AACA;AACA;;;EACIC,0BAA0B,GAAG;IACzB,KAAKhE,QAAL,CAAcgE,0BAAd;EACH;;EACD1B,4BAA4B,CAACqB,GAAD,EAAM;IAC9B,OAAOM,MAAM,CAACC,IAAP,CAAYP,GAAZ,EAAiBD,MAAjB,CAAwB,CAACS,MAAD,EAASC,GAAT,KAAiB;MAC5CD,MAAM,CAACC,GAAD,CAAN,GAAc,CAACT,GAAG,CAACS,GAAD,CAAJ,CAAd;MACA,OAAOD,MAAP;IACH,CAHM,EAGJ,EAHI,CAAP;EAIH;EACD;AACJ;AACA;AACA;AACA;;;EACIE,OAAO,GAAG;IACN,KAAKrE,QAAL,CAAcqE,OAAd;;IACA,IAAI,KAAK5B,WAAT,EAAsB;MAClB,KAAKA,WAAL,CAAiB4B,OAAjB;IACH;;IACD,KAAKxE,eAAL,CAAqBwE,OAArB;EACH;;AAlVmB;AAoVxB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AACA,gBAAsBC,cAAtB;EAAA,+DAAO,WAA8B7E,QAA9B,EAAwC8E,OAAO,GAAG,EAAlD,EAAsD5E,IAAI,GAAGZ,EAA7D,EAAiE;IACpE,IAAIU,QAAQ,IAAI,IAAhB,EAAsB;MAClB,MAAM,IAAI8B,KAAJ,CAAU,uEACZ,sCADE,CAAN;IAEH;;IACD,IAAIgD,OAAO,IAAI,IAAf,EAAqB;MACjBA,OAAO,GAAG,EAAV;IACH;;IACD,IAAIA,OAAO,CAACC,SAAR,IAAqB,OAAO/E,QAAP,KAAoB,QAA7C,EAAuD;MACnDA,QAAQ,GAAGgF,WAAW,CAAChF,QAAD,CAAtB;IACH;;IACD,MAAMiF,KAAK,GAAG,IAAInF,UAAJ,CAAeE,QAAf,EAAyB8E,OAAzB,EAAkC5E,IAAlC,CAAd;IACA,MAAM+E,KAAK,CAAC3D,IAAN,EAAN;IACA,OAAO2D,KAAP;EACH,CAdD;AAAA;AAeA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AACA,OAAO,SAASC,kBAAT,CAA4BC,WAA5B,EAAyC;EAC5C,IAAIA,WAAW,IAAI,IAAnB,EAAyB;IACrB,MAAM,IAAIrD,KAAJ,CAAU,qEACZ,sDADE,CAAN;EAEH;;EACD,IAAIsD,SAAJ;;EACA,IAAID,WAAW,YAAYpB,KAA3B,EAAkC;IAC9B,MAAM,CAACsB,SAAD,EAAY1E,OAAZ,IAAuBwE,WAA7B;;IACA,IAAI,CAACE,SAAL,EAAgB;MACZ,MAAM,IAAIvD,KAAJ,CAAU,kDAAV,CAAN;IACH;;IACD,IAAI,CAACnB,OAAD,IAAY,EAAEA,OAAO,YAAY2E,WAArB,CAAhB,EAAmD;MAC/C,MAAM,IAAIxD,KAAJ,CAAU,4DACV,YADA,CAAN;IAEH;;IACD,IAAI,EAAE,mBAAmBuD,SAArB,CAAJ,EAAqC;MACjC,MAAM,IAAIvD,KAAJ,CAAU,yCAAV,CAAN;IACH;;IACD,IAAI,EAAE,qBAAqBuD,SAAvB,CAAJ,EAAuC;MACnC,MAAM,IAAIvD,KAAJ,CAAU,2CAAV,CAAN;IACH;;IACD,MAAMY,WAAW,GAAGpD,EAAE,CAACiG,cAAH,CAAkBF,SAAS,CAACG,eAA5B,CAApB;IACA,MAAMC,cAAc,GAAGnG,EAAE,CAACoG,4BAAH,CAAgCL,SAAhC,EAA2C3C,WAA3C,EAAwD/B,OAAxD,CAAvB;IACAyE,SAAS,GAAG9F,EAAE,CAACqG,cAAH,CAAkBF,cAAlB,CAAZ;EACH,CAlBD,MAmBK,IAAI,UAAUN,WAAd,EAA2B;IAC5B;IACAC,SAAS,GAAGD,WAAZ;EACH,CAHI,MAIA,IAAI,mBAAmBA,WAAnB,IAAkC,iBAAiBA,WAAnD,IACF,gBAAgBA,WADlB,EAC+B;IAChC;IACAC,SAAS,GAAG9F,EAAE,CAACqG,cAAH,CAAkBR,WAAlB,CAAZ;EACH,CAJI,MAKA;IACD,MAAM,IAAIrD,KAAJ,CAAU,sBAAV,CAAN;EACH;;EACD,MAAMmD,KAAK,GAAG,IAAInF,UAAJ,CAAesF,SAAf,CAAd;EACAH,KAAK,CAAC3D,IAAN;EACA,OAAO2D,KAAP;AACH;;AACD,SAASD,WAAT,CAAqBhF,QAArB,EAA+B;EAC3B,IAAI,CAACA,QAAQ,CAAC4F,QAAT,CAAkB,GAAlB,CAAL,EAA6B;IACzB5F,QAAQ,GAAIA,QAAD,GAAa,GAAxB;EACH;;EACD,OAAQ,GAAEA,QAAS,GAAEH,kBAAmB,GAAED,kBAAmB,EAA7D;AACH"},"metadata":{},"sourceType":"module"}